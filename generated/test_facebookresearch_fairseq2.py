import sys
_module = sys.modules[__name__]
del sys
update_pep503_index = _module
conf = _module
setup = _module
fairseq2n = _module
fairseq2 = _module
assets = _module
_card = _module
_download_manager = _module
_error = _module
_metadata_provider = _module
_store = _module
chatbots = _module
_chatbot = _module
_handler = _module
llama = _module
mistral = _module
checkpoint = _module
_manager = _module
cli = _module
_cli = _module
_logging = _module
_main = _module
commands = _module
chatbot = _module
_convert_checkpoint = _module
_write_hf_config = _module
recipe = _module
utils = _module
argparse = _module
config_registry = _module
context = _module
data = _module
_data_pipeline = _module
_memory = _module
_vocabulary_info = _module
audio = _module
image = _module
parquet = _module
dataloader = _module
tools = _module
text = _module
_converters = _module
_reader = _module
tokenizers = _module
_hub = _module
_ref = _module
_tokenizer = _module
char_tokenizer = _module
nllb = _module
s2t_transformer = _module
sentencepiece = _module
tiktoken = _module
datasets = _module
_config = _module
_data_reader = _module
_utils = _module
asr = _module
instruction = _module
parallel_text = _module
preference = _module
speech = _module
device = _module
error = _module
extensions = _module
gang = _module
generation = _module
_beam_search = _module
_algo = _module
_generator = _module
_generator = _module
_sampling = _module
_generator = _module
_sampler = _module
_step_processor = _module
text = _module
logging = _module
metrics = _module
_aggregation = _module
_bag = _module
_descriptor = _module
recorders = _module
_jsonl = _module
_log = _module
_recorder = _module
_tensorboard = _module
_wandb = _module
_bleu = _module
_chrf = _module
_wer = _module
models = _module
_handler = _module
_hub = _module
_model = _module
conformer = _module
_block = _module
_convolution = _module
decoder = _module
encoder_decoder = _module
feature_extractor = _module
fsdp = _module
jepa = _module
_factory = _module
_handler = _module
_model = _module
classifier = _module
_handler = _module
_model = _module
_factory = _module
_handler = _module
integ = _module
lora = _module
_handler = _module
_factory = _module
_feature_extractor = _module
_frontend = _module
_handler = _module
seq2seq = _module
sequence = _module
transformer = _module
_frontend = _module
_handler = _module
_model = _module
transformer_decoder = _module
_model = _module
_sharder = _module
checkpoint = _module
vit = _module
_feature_extractor = _module
_frontend = _module
w2vbert = _module
_handler = _module
_model = _module
wav2vec2 = _module
_factory = _module
_feature_extractor = _module
_frontend = _module
_handler = _module
_masker = _module
_model = _module
_position_encoder = _module
_vector_quantizer = _module
_handler = _module
_model = _module
nn = _module
_embedding = _module
_incremental_state = _module
_normalization = _module
_position_encoder = _module
_projection = _module
checkpointing = _module
ddp = _module
fsdp = _module
functional = _module
lora = _module
ops = _module
padding = _module
_attention = _module
_attention_mask = _module
_decoder = _module
_decoder_layer = _module
_encoder = _module
_encoder_layer = _module
_ffn = _module
_layer_norm = _module
_multihead_attention = _module
_norm_order = _module
_relative_attention = _module
_residual = _module
_shaw_attention = _module
gradient = _module
mask = _module
module = _module
optim = _module
_adamw = _module
_dynamic_loss_scaler = _module
_handler = _module
_optimizer = _module
lr_scheduler = _module
_cosine_annealing = _module
_handler = _module
_lr_scheduler = _module
_myle = _module
_noam = _module
_polynomial_decay = _module
_tri_stage = _module
recipes = _module
_common = _module
_eval = _module
cluster = _module
common = _module
config = _module
_dataclasses = _module
_handlers = _module
early_stopper = _module
evaluator = _module
generator = _module
lm = _module
_instruction_finetune = _module
_loss_eval = _module
_preference_finetune = _module
_common = _module
_cpo = _module
_dpo = _module
_handler = _module
_orpo = _module
_recipe = _module
_simpo = _module
_text_generate = _module
metrics = _module
mt = _module
_common = _module
_eval = _module
_train = _module
_translate = _module
runner = _module
trainer = _module
log = _module
rich = _module
sweep_tagger = _module
_common = _module
_eval = _module
_train = _module
_train = _module
registry = _module
_assets = _module
_chatbots = _module
_clusters = _module
_datasets = _module
_generation = _module
_metrics = _module
_models = _module
_optim = _module
_recipes = _module
_root = _module
_text_tokenizers = _module
tensor_parallel = _module
typing = _module
dataclass = _module
env = _module
file = _module
lazy = _module
profiler = _module
rng = _module
state = _module
structured = _module
version = _module
yaml = _module
tests = _module
common = _module
conftest = _module
integration = _module
test_incremental_decode = _module
test_sampling = _module
test_step_processor = _module
test_llama = _module
test_llama_lora = _module
test_nllb = _module
test_s2t_transformer = _module
test_parquet_dataloader = _module
unit = _module
test_card = _module
test_audio_decoder = _module
test_waveform_to_fbank_converter = _module
data_pipeline = _module
test_bucket = _module
test_bucket_by_length = _module
test_collate = _module
test_concat = _module
test_constant = _module
test_count = _module
test_data_pipeline = _module
test_dynamic_bucket = _module
test_filter = _module
test_map = _module
test_prefetch = _module
test_read_iterator = _module
test_read_sequence = _module
test_repeat = _module
test_round_robin = _module
test_sample = _module
test_shard = _module
test_shuffle = _module
test_skip = _module
test_take = _module
test_yield_from = _module
test_zip = _module
test_image_decoder = _module
test_collater = _module
test_file_mapper = _module
test_memory = _module
test_read_pickle_wrapped_iterator = _module
test_sentencepiece = _module
test_str_splitter = _module
test_str_to_int_converter = _module
test_str_to_tensor_converter = _module
test_step_processor = _module
test_integ = _module
test_functional = _module
test_lora = _module
test_padding = _module
test_position_encoder = _module
test_attention = _module
test_attention_mask = _module
test_gradient = _module
test_mask = _module
test_module = _module
test_adamw = _module
test_lr_scheduler = _module
test_sweep_tagger = _module
test_config_registry = _module
test_gang = _module
test_metrics = _module
test_dataclass = _module
test_state = _module
test_structured = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import operator as op
from dataclasses import dataclass
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


from typing import Final


import torch


from abc import ABC


from abc import abstractmethod


from collections.abc import Sequence


from typing import Literal


from typing import TypeAlias


from typing import final


from torch import Tensor


import warnings


from collections.abc import Iterable


from collections.abc import Iterator


from collections.abc import Mapping


from collections.abc import Set


from warnings import catch_warnings


from torch.distributed._shard import load_with_process_group


from torch.distributed.fsdp import FullyShardedDataParallel as FSDP


from torch.distributed.fsdp.api import FullStateDictConfig


from torch.distributed.fsdp.api import StateDictType


from torch.nn import Module


from torch.cuda import OutOfMemoryError


from typing import Any


from collections.abc import Callable


from typing import TYPE_CHECKING


from typing import TypedDict


from typing import TypeVar


from collections.abc import Generator


from enum import Enum


from functools import partial


import numpy as np


import pandas as pd


from numpy.typing import NDArray


from typing import cast


from torch.nn.functional import layer_norm


import torch.distributed as dist


from torch.distributed import Backend


from torch.distributed import ProcessGroup


from torch.distributed import ReduceOp


from types import NoneType


from torch.nn.functional import log_softmax


from collections import OrderedDict


from typing import Protocol


from torch.utils.hooks import RemovableHandle


from torch.nn.functional import softmax


from torch.nn.functional import pad


import math


import re


from typing import TextIO


from torch.nn.modules.utils import consume_prefix_in_state_dict_if_present


from typing import Generic


from torch.nn.functional import ctc_loss


from torch.nn import Dropout


from torch.nn import GLU


from torch.nn import BatchNorm1d


from torch.nn import Conv1d


from torch.nn import SiLU


from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy


import torch.nn as nn


from torch.nn import GELU


from torch.nn import Conv2d


from torch.nn import Conv3d


from torch.nn import Parameter


from torch.nn import Sequential


from collections.abc import MutableMapping


from torch.nn.functional import cross_entropy


from torch.nn import GroupNorm


from torch.nn.functional import group_norm


from torch.nn.utils import remove_weight_norm


from torch.nn.utils import weight_norm


from torch.nn.functional import gumbel_softmax


from torch.nn.functional import embedding


from torch.nn.parameter import Parameter


from torch import Size


from torch.nn.functional import interpolate


from torch.nn.functional import linear


from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import CheckpointImpl


from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import apply_activation_checkpointing


from torch.distributed.algorithms._checkpoint.checkpoint_wrapper import checkpoint_wrapper


from torch.distributed import GradBucket


from torch.futures import Future


from torch.nn.parallel import DistributedDataParallel as DDP


from torch.distributed.fsdp.api import BackwardPrefetch


from torch.distributed.fsdp.api import CPUOffload


from torch.distributed.fsdp.api import MixedPrecision


from torch.distributed.fsdp.api import ShardedOptimStateDictConfig


from torch.distributed.fsdp.api import ShardedStateDictConfig


from torch.distributed.fsdp.api import ShardingStrategy


from torch.nn.functional import dropout


from torch.nn.functional import scaled_dot_product_attention


from torch import Generator


from torch.nn import ModuleList


from torch.autograd import Function


from torch.nn import ReLU


from torch.nn import Sigmoid


from collections.abc import MutableSequence


import logging


from torch.nn.utils import clip_grad_norm_


from itertools import chain


from typing import runtime_checkable


from torch.optim import Optimizer


from torch.optim.adamw import adamw


from torch.cuda.amp.grad_scaler import GradScaler


from torch.distributed.fsdp.sharded_grad_scaler import ShardedGradScaler


from torch.optim.lr_scheduler import _LRScheduler


from collections.abc import Collection


from random import Random


from typing import Sequence


from itertools import count


import torch.distributed


from typing import Iterable


from typing import Mapping


from torch.profiler import record_function


from typing import ClassVar


from typing import TypeGuard


from torch import device


from torch import dtype


from typing import BinaryIO


from time import perf_counter


from torch.profiler import ProfilerActivity


from torch.profiler import profile


from torch.profiler import schedule


from torch.profiler import tensorboard_trace_handler


from copy import deepcopy


from types import UnionType


from typing import Union


from typing import get_args


from typing import get_origin


from typing import get_type_hints


from torch.nn.functional import relu


from torch.optim import AdamW as BaseAdamW


from torch.optim import SGD


def to_padding_mask(seq_lens: 'Tensor', batch_seq_len: 'int') ->Tensor:
    """Convert a sequence length array to a boolean padding mask tensor.

    :param seq_lens:
        An array where each element represents the length of a sequence. *Shape:*
        :math:`(N)`, where :math:`N` is the batch size.
    :param batch_seq_len:
        The sequence length of the mask.

    :returns:
        The mask. *Shape:* :math:`(N,S)`, where :math:`N` is the batch size and
        :math:`S` is the sequence length.
    """
    batch_size = seq_lens.size(0)
    indices = torch.arange(batch_seq_len, device=seq_lens.device).expand(batch_size, -1)
    lengths = seq_lens.unsqueeze(1).expand(-1, batch_seq_len)
    return indices < lengths


def get_seq_lens(seqs: 'Tensor', padding_mask: 'PaddingMask | None') ->Tensor:
    """Retrieve the sequence lengths of ``seqs``.

    :param seqs:
        The sequences. *Shape:* :math:`(N,S,*)`, where :math:`N` is the batch
        size, :math:`S` is the sequence length, and :math:`*` is any number of
        sequence-specific dimensions including none.
    :param padding_mask:
        The padding mask. *Shape:* :math:`(N,S)`, where :math:`N` is the batch
        size and :math:`S` is the sequence length.

    :returns:
        An array where each element represents the length of the corresponding
        sequence in ``seqs``. *Shape:* :math:`(N)`, where :math:`N` is the batch
        size.
    """
    if padding_mask is not None:
        return padding_mask.seq_lens
    return torch.full((seqs.size(0),), seqs.size(1), device=seqs.device)


class SequenceData(TypedDict):
    seqs: 'Tensor'
    seq_lens: 'Tensor'
    is_ragged: 'bool'


class InternalError(Exception):
    pass


def apply_padding_mask(seqs: 'Tensor', padding_mask: 'PaddingMask | None', pad_value: 'int | float | Tensor'=0) ->Tensor:
    """Apply the specified padding mask to ``seqs``.

    :param seqs:
        The sequences to mask. *Shape:* :math:`(N,S,*)`, where :math:`N` is the
        batch size, :math:`S` is the sequence length, and :math:`*` is any
        number of sequence-specific dimensions including none.
    :param padding_mask:
        The padding mask to apply. *Shape:* :math:`(N,S)`, where :math:`N` is
        the batch size and :math:`S` is the sequence length.
    :param pad_value:
        The value for padded positions.

    :returns:
        The input sequences with mask applied. *Shape:* Same as ``seqs``.
    """
    if padding_mask is None:
        return seqs
    m = padding_mask.materialize()
    for _ in range(seqs.ndim - m.ndim):
        m = m.unsqueeze(-1)
    return seqs.where(m, pad_value)


@final
class JepaClassifierModel(Module):
    """
    Represents a pretrained Jepa model, with an attentive probing layer for
    classfication tasks. See
        * :cite:t:`https://doi.org/10.48550/arXiv.2301.08243`
        * :cite:t:`https://doi.org/10.48550/arXiv.2404.08471`
    """
    model_dim: 'int'
    encoder_frontend: 'TransformerFrontend'
    encoder: 'TransformerEncoder'
    pooler: 'AttentivePooler'
    head: 'Projection'

    def __init__(self, encoder_frontend: 'TransformerFrontend', encoder: 'TransformerEncoder', pooler: 'AttentivePooler', head: 'Projection') ->None:
        super().__init__()
        self.model_dim = encoder.model_dim
        self.encoder_frontend = encoder_frontend
        self.encoder = encoder
        self.pooler = pooler
        self.head = head

    def forward(self, batch: 'SequenceBatch') ->Tensor:
        seqs, padding_mask = self.encoder_frontend(batch.seqs, batch.padding_mask)
        seqs, _ = self.encoder(seqs, padding_mask)
        seqs = self.pooler(seqs)
        seqs = seqs.squeeze(1)
        return self.head(seqs)

    def extra_repr(self) ->str:
        """:meta private:"""
        return f'model_dim={self.model_dim}'


def nll_loss(lprobs: 'Tensor', targets: 'Tensor', pad_idx: 'int | None', *, label_smoothing: float=0.0, reduction: Literal['none', 'sum']='sum') ->Tensor:
    """Compute the negative log-likelihood loss.

    In contrast to :func:`torch.nn.functional.nll_loss`, this function expects
    ``lprobs`` to be of shape :math:`(N,S,T)`, where :math:`N` is the batch
    size, :math:`S` is the sequence length, and :math:`T` is the size of the
    target vocabulary. The loss is computed over the last dimension which avoids
    strided access and improves runtime performance, in particular for large
    vocabularies.

    :param lprobs:
        The log probabilities. *Shape:* See the function description.
    :param targets:
        The target indices. *Shape:* :math:`(N,S)`, where :math:`N` is the batch
        size and :math:`S` is the sequence length.
    :param pad_idx:
        The index of the PAD symbol in the target vocabulary.
    :param label_smoothing:
        The amount of label smoothing to apply while computing the loss.
    :param reduction:
        The reduction to apply to the output.
    """
    targets = targets.unsqueeze(-1)
    loss = -lprobs.gather(dim=-1, index=targets)
    if label_smoothing > 0.0:
        smooth_loss = -lprobs.sum(dim=-1, keepdim=True)
    else:
        smooth_loss = None
    if pad_idx is not None:
        padding_mask = targets.eq(pad_idx)
        loss.masked_fill_(padding_mask, 0.0)
        if smooth_loss is not None:
            smooth_loss.masked_fill_(padding_mask, 0.0)
    if reduction == 'sum':
        loss = loss.sum()
        if smooth_loss is not None:
            smooth_loss = smooth_loss.sum()
    if smooth_loss is not None:
        eps = label_smoothing / (lprobs.size(-1) - 1)
        loss = (1.0 - label_smoothing - eps) * loss + eps * smooth_loss
    if reduction == 'none':
        loss = loss.squeeze(-1)
    return loss


T = TypeVar('T')


@final
class IncrementalStateBag:
    """Holds the module states during incremental decoding."""
    _step_nr: 'int'
    _max_num_steps: 'int'
    _capacity_increment: 'int | None'
    _module_states: 'dict[Module, IncrementalState]'

    def __init__(self, max_num_steps: 'int', *, capacity_increment: (int | None)=16) ->None:
        """
        :param max_num_steps:
            The maximum number of steps to take.
        :param capacity_increment:
            The sequence length capacity of state tensors will be incremented by
            multiples of this value. If ``None``, state tensors will be
            preallocated with a capacity of ``max_num_steps``.
        """
        if capacity_increment is not None and capacity_increment < 1:
            raise ValueError(f'`capacity_increment` must be greater than or equal to 1, but is {capacity_increment} instead.')
        self._step_nr = 0
        self._max_num_steps = max_num_steps
        self._capacity_increment = capacity_increment
        self._module_states = {}

    def increment_step_nr(self, value: 'int'=1) ->None:
        """Increment the step number.

        This method should be called after every decoding step. It is used by
        modules to keep track of the position in the sequence.

        :param value:
            The value by which to increment the step number.
        """
        step_nr = self._step_nr + value
        if step_nr >= self._max_num_steps:
            raise ValueError(f'The current step number ({self._step_nr}) with `value` increment ({value}) must be less than or equal to the maximum number of steps ({self.max_num_steps}), but is {self._step_nr + value} instead.')
        self._step_nr = step_nr

    def get_state(self, m: 'Module', kls: 'type[T]') ->(T | None):
        """Get the state of ``m`` if present in the bag.

        :param m:
            The module.
        :param kls:
            The expected ``type`` of the state. If the type of the state in the
            bag does not match ``kls``, ``None`` will be returned.

        :returns:
            The state of the module.
        """
        state = self._module_states.get(m, None)
        if isinstance(state, kls):
            return state
        else:
            return None

    def set_state(self, m: 'Module', state: 'IncrementalState') ->None:
        """Set the state of ``m``.

        :param m:
            The module.
        :param state:
            The state to store.
        """
        self._module_states[m] = state

    def reorder(self, new_order: 'Tensor') ->None:
        """Reorder the module states.

        See :meth:`IncrementalState.reorder` for more information.
        """
        for state in self._module_states.values():
            state.reorder(new_order)

    @property
    def step_nr(self) ->int:
        """The current step number."""
        return self._step_nr

    @property
    def max_num_steps(self) ->int:
        """The maximum number of steps."""
        return self._max_num_steps

    @property
    def capacity_increment(self) ->(int | None):
        """The sequence length capacity of state tensors will be incremented by
        multiples of this value."""
        return self._capacity_increment

    def size_bytes(self) ->int:
        """Return the size of the state bag in bytes."""
        return sum(s.size_bytes() for s in self._module_states.values())

    def capacity_bytes(self) ->int:
        """Return the reserved capacity of the state bag in bytes."""
        return sum(s.capacity_bytes() for s in self._module_states.values())


class PatchFeatureExtractor(Module, ABC):
    """
    Extracts patch features from N-dimensional inputs and embeds them in a
    latent space.
    """
    feature_dim: 'int'

    def __init__(self, feature_dim: 'int') ->None:
        """
        :param feature_dim:
            The dimensionality of extracted patch features.
        """
        super().__init__()
        self.feature_dim = feature_dim

    @abstractmethod
    def forward(self, x: 'Tensor') ->Tensor:
        """
        :param x: The inputs from which to extract patch features. *Shape:*
            :math:`(N,C,*)`, where :math:`N` is the batch size, :math:`C` is the
            number of channels, and :math:`*` is any number of input-specific
            dimensions.

        :returns: The extracted patch features. *Shape:* :math:`(N,*,E)`, where
              :math:`N` is the batch size, :math:`*` is the same number of
              dimensions as in input, but potentially with different
              dimensionality, and :math:`E` is the dimensionality of the patch
              features.
        """


class Projection(Module, ABC):
    """Applies a linear transformation to incoming data."""
    input_dim: 'int'
    output_dim: 'int'

    def __init__(self, input_dim: 'int', output_dim: 'int') ->None:
        """
        :param input_dim:
            The dimensionality of inputs.
        :param output_dim:
            The dimensionality of projected outputs.
        """
        super().__init__()
        self.input_dim, self.output_dim = input_dim, output_dim

    @abstractmethod
    def forward(self, x: 'Tensor') ->Tensor:
        """
        :param x:
            The input to project. *Shape:* :math:`(*,H_{inp})`, where
            :math:`H_{inp}` is the input dimensionality.

        :returns:
            The projected output. *Shape:* :math:`(*,H_{out})`, where all but
            the last dimension are the same shape as the input and
            :math:`H_{out}` is the output dimensionality.
        """

    def extra_repr(self) ->str:
        """:meta private:"""
        return f'input_dim={self.input_dim}, output_dim={self.output_dim}'


def _init_uniform(weight: 'Tensor', bias: 'Tensor | None') ->None:
    nn.init.kaiming_uniform_(weight, a=math.sqrt(5))
    if bias is not None:
        fan_in = weight.size(1)
        m = 1
        if weight.ndim > 2:
            for s in weight.shape[2:]:
                m *= s
        fan_in *= m
        bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
        nn.init.uniform_(bias, -bound, bound)


def extract_masked_elements(seqs: 'Tensor', temporal_mask: 'Tensor') ->Tensor:
    """Extract masked elements from ``seqs``.

    :param seqs:
        The sequences. *Shape:* :math:`(N,S,M)`, where :math:`N` is the batch
        size, :math:`S` is the sequence length, and :math:`M` is the
        dimensionality of the model.
    :param temporal_mask:
        The temporal mask. *Shape:* :math:`(N,S)`, where :math:`N` is the batch
        size and :math`S` is the sequence length.
    """
    batch_size = seqs.size(0)
    seqs = seqs[temporal_mask]
    return seqs.unflatten(0, (batch_size, -1))


class _GradientScaleFunction(Function):

    @staticmethod
    def forward(ctx: 'Any', x: 'Tensor', scale: 'float') ->Tensor:
        if not x.dtype.is_floating_point:
            raise TypeError(f'`x` must be a float tensor, but is a `{x.dtype}` tensor instead.')
        ctx.scale = scale
        return x.detach().clone()

    @staticmethod
    def backward(ctx: 'Any', grad_output: 'Tensor') ->tuple[Tensor, None]:
        return grad_output * ctx.scale, None


def scale_gradient(x: 'Tensor', scale: 'float') ->Tensor:
    """Scale the gradient of ``x`` during backpropagation.

    This is typically used to allow one part of a model to learn at a lower rate
    than the rest.

    :param x:
        The input tensor.
    :param scale:
        The scale factor of the gradient.
    """
    return _GradientScaleFunction.apply(x, scale)


class NotSupportedError(Exception):
    pass


class Wav2Vec2Masker(Module, ABC):
    """Masks extracted wav2vec 2.0 features."""

    @abstractmethod
    def forward(self, seqs: 'Tensor', padding_mask: 'PaddingMask | None') ->tuple[Tensor, Tensor]:
        """
        :param seqs:
            The sequences to mask. *Shape:* :math:`(N,S,M)`, where :math:`N` is
            the batch size, :math:`S` is the sequence length, and :math:`M` is
            the dimensionality of the model.
        :param seq_lens:
            An array where each element represents the length of the sequence at
            the same index in ``seqs``. *Shape:* :math:`(N)`, where :math:`N` is
            the batch size.

        :returns:
            - The input sequences with mask applied. *Shape:* Same as ``seqs``.
            - The temporal mask that has been applied to ``seqs``. *Shape:*
              :math:`(N,S)`, where :math:`N` is the batch size and :math`S` is
              the sequence length.
        """


class RowMaskFactory(Protocol):

    def __call__(self, shape: 'tuple[int, int]', span_len: 'int', max_mask_prob: 'float', row_lens: 'Tensor | None'=None, min_num_spans: 'int'=0, device: 'Device | None'=None) ->(Tensor | None):
        """Compute a random row mask of the specified shape.

        :param shape:
            The shape of the mask.
        :param span_len:
            The length of each mask span.
        :param max_mask_prob:
            The maximum probability of masking an element in a row.
        :param row_lens:
            The length of each row. *Shape:* :math:`(R)`, where :math:`R` is the
            number of rows.
        :param min_num_spans:
            The minimum number of mask spans per row.
        :param device:
            The device on which to initialize the mask.

        :returns:
            The boolean row mask. *:Shape:* ``shape``.
        """


def repeat_interleave(x: 'Tensor', dim: 'int', repeat: 'int') ->Tensor:
    """Repeat elements of a tensor.

    :param x:
        The input tensor.
    :param dim:
        The dimension along which to repeat values.
    :param repeat:
        The number of repetitions.

    :returns:
        The repeated tensor which has the same shape as input, except along the
        given axis.

    .. note::
        This is a lightweight version of :func:`torch.repeat_interleave` that
        is faster for repetitions along a single dimension.
    """
    if repeat == 1:
        return x
    shape = [-1] * (x.ndim + 1)
    if dim < 0:
        dim += x.ndim
    shape[dim + 1] = repeat
    return x.unsqueeze(dim + 1).expand(shape).flatten(dim, dim + 1)


def _compute_mask_spans(row_lens: 'Tensor', span_len: 'int', max_mask_prob: 'float', min_num_spans: 'int') ->(Tensor | None):
    """Compute random mask spans of the specified shape."""
    device, dtype = row_lens.device, row_lens.dtype
    num_rows = len(row_lens)
    if num_rows == 0:
        return None
    num_spans_per_row = max_mask_prob / span_len * (row_lens - 1)
    num_spans = int(num_spans_per_row.min())
    if min_num_spans > num_spans:
        raise ValueError(f'`min_num_spans` is {min_num_spans}, but with the given `span_len` and `max_mask_prob` only {num_spans} mask span(s) can be generated.')
    if num_spans == 0:
        return None
    span_start_range = row_lens - span_len + 1
    span_start_range = repeat_interleave(span_start_range, dim=0, repeat=num_spans)
    rand_scales = torch.rand(num_rows * num_spans, device=device)
    span_offsets = span_start_range * rand_scales
    span_offsets = span_offsets.view(num_rows, -1)
    span_offsets = repeat_interleave(span_offsets, dim=-1, repeat=span_len)
    indices = torch.arange(span_len, device=device, dtype=dtype)
    indices = indices.repeat(num_spans).unsqueeze(0).expand(num_rows, -1)
    return span_offsets + indices


def _generate_mask(indices: 'Tensor', max_row_len: 'int') ->Tensor:
    """Generate a boolean mask by setting ``indices`` to ``True``."""
    float_mask = torch.zeros((indices.size(0), max_row_len), device=indices.device)
    float_mask.scatter_(1, indices, 1.0)
    min_num_masked = int(torch.count_nonzero(float_mask, dim=-1).min())
    random_values = torch.rand_like(float_mask) + 0.001
    random_values = random_values * float_mask
    _, indices = torch.topk(random_values, k=min_num_masked, dim=1, sorted=False)
    bool_mask = torch.full_like(float_mask, False, dtype=torch.bool)
    return bool_mask.scatter_(1, indices, True)


def compute_row_mask(shape: 'tuple[int, int]', span_len: 'int', max_mask_prob: 'float', row_lens: 'Tensor | None'=None, min_num_spans: 'int'=0, device: 'Device | None'=None) ->(Tensor | None):
    """Implements the :class:`RowMaskFactory` protocol.

    Note that, due to mask span overlap, the effective mask probability will be
    lower than ``max_mask_prob``. The implementation also guarantees that there
    will be always at least one unmasked element in each row.
    """
    num_rows, max_row_len = shape
    if row_lens is None:
        if span_len >= max_row_len:
            raise ValueError(f'The size of the second dimension of `shape` must be greater than `span_len` ({span_len}), but is {max_row_len} instead.')
        row_lens = torch.full((num_rows,), max_row_len, device=device, dtype=torch.int64)
    else:
        row_lens = row_lens.view(num_rows)
        if (span_len >= row_lens).any():
            raise ValueError(f'All lengths in `row_lens` must be greater than `span_len` ({span_len}), but at least one length is smaller. row_lens: {row_lens}')
    indices = _compute_mask_spans(row_lens, span_len, max_mask_prob, min_num_spans)
    if indices is None:
        return row_lens.new_empty((0, 0))
    return _generate_mask(indices, max_row_len)


class VectorQuantizer(Module, ABC):
    """Quantizes incoming data in a differentiable way."""
    input_dim: 'int'
    output_dim: 'int'
    num_codebooks: 'int'
    num_codebook_entries: 'int'

    def __init__(self, input_dim: 'int', output_dim: 'int') ->None:
        """
        :param input_dim:
            The dimensionality of inputs.
        :param output_dim:
            The dimensionality of quantized outputs.
        """
        super().__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim

    @abstractmethod
    def forward(self, x: 'Tensor') ->'VectorQuantizerOutput':
        pass


def init_entry_projection(proj: 'Linear') ->None:
    nn.init.normal_(proj.weight, mean=0.0, std=1.0)
    if proj.bias is None:
        raise ValueError('`proj.bias` must not be `None`.')
    nn.init.zeros_(proj.bias)


def init_final_projection(proj: 'Linear') ->None:
    """Initialize ``proj`` as the final projection of a wav2vec 2.0 ASR model."""
    nn.init.xavier_uniform_(proj.weight)
    if proj.bias is not None:
        nn.init.zeros_(proj.bias)


class Embedding(Module, ABC):
    """Stores embeddings of a fixed dictionary and size."""
    num_embeddings: 'int'
    embedding_dim: 'int'
    pad_idx: 'int | None'
    padding_idx: 'int | None'

    def __init__(self, num_embeddings: 'int', embedding_dim: 'int', pad_idx: 'int | None'=None) ->None:
        """
        :param num_embeddings:
            The size of the embedding table.
        :param embedding_dim:
            The dimensionality of returned embeddings.
        :param pad_idx:
            If not ``None``, entries at ``pad_idx`` do not contribute to the
            gradient; therefore, the embedding at ``pad_idx`` is not updated
            during training.
        """
        super().__init__()
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        self.pad_idx = pad_idx
        self.padding_idx = pad_idx

    @abstractmethod
    def forward(self, x: 'Tensor') ->Tensor:
        """
        :param x:
            The embedding indices. *Shape:* Any.

        :returns:
            The embeddings corresponding to the specified indices. *Shape:*
            :math:`(*,E)`, where :math:`*` is the input shape and :math:`E` is
            the dimensionality of the embeddings.
        """

    def extra_repr(self) ->str:
        """:meta private:"""
        s = f'num_embeddings={self.num_embeddings}, embedding_dim={self.embedding_dim}'
        if self.pad_idx is not None:
            s = f'{s}, pad_idx={self.pad_idx}'
        return s


def _do_gather(x: 'Tensor', gang: 'Gang', dim: 'int') ->Tensor:
    if gang.size == 1:
        return x
    splits = [torch.empty_like(x) for _ in range(gang.size)]
    gang.all_gather_to_list(splits, x)
    return torch.cat(splits, dim=dim)


def _do_scatter(x: 'Tensor', gang: 'Gang', dim: 'int') ->Tensor:
    if gang.size == 1:
        return x
    dim_size = x.size(dim)
    if dim_size % gang.size != 0:
        raise ValueError(f'The size of the dimension {dim} of `x` must be a multiple of `gang.size` ({gang.size}), but is {dim_size} instead.')
    splits = x.split(dim_size // gang.size, dim=dim)
    return splits[gang.rank].contiguous()


class _GatherFunction(Function):

    @staticmethod
    def forward(ctx: 'Any', x: 'Tensor', gang: 'Gang', dim: 'int') ->Tensor:
        ctx.dim = dim
        ctx.gang = gang
        return _do_gather(x, gang, dim)

    @staticmethod
    def backward(ctx: 'Any', grad_output: 'Tensor') ->tuple[Tensor, None, None]:
        x = _do_scatter(grad_output, ctx.gang, ctx.dim)
        return x, None, None


def gather(x: 'Tensor', gang: 'Gang', dim: 'int'=-1) ->Tensor:
    """Gather ``x`` across all processes in ``gang`` over ``dim``.

    This is an autograd-aware operation and the backward pass will return the
    scattered gradient of ``x``.
    """
    return _GatherFunction.apply(x, gang, dim)


class ReduceOperation(Enum):
    """Specifies a reduce operation."""
    SUM = 1
    MEAN = 2
    PRODUCT = 3
    MIN = 4
    MAX = 5


class _ReduceFunction(Function):

    @staticmethod
    def forward(ctx: 'Any', x: 'Tensor', gang: 'Gang') ->Tensor:
        x = x.detach().clone()
        gang.all_reduce(x, ReduceOperation.SUM)
        return x

    @staticmethod
    def backward(ctx: 'Any', grad_output: 'Tensor') ->tuple[Tensor, None, None]:
        return grad_output, None, None


def reduce(x: 'Tensor', gang: 'Gang') ->Tensor:
    """Reduce ``x`` across all processes in ``gang``.

    This is an autograd-aware operation and the backward pass will return the
    all-reduced gradient of ``x``.
    """
    return _ReduceFunction.apply(x, gang)


def apply_to_parameters(module: 'Module', fn: 'Callable[[Tensor], Tensor]', *, recurse: bool=True, memo: (dict[Tensor, Tensor] | None)=None, no_memo: bool=False) ->None:
    """Apply ``fn`` to the parameters and buffers of ``module``.

    :param module:
        The module to process.
    :param fn:
        The function to apply.
    :param recurse:
        If ``True``, applies ``fn`` to the parameters and buffers of descendant
        modules.
    :param memo:
        The memoization dictionary to detect shared parameters and buffers. If
        ``None`` and ``no_memo`` is ``False``, constructs an internal one.
    :param no_memo:
        If ``True``, skips memoization.
    """
    if no_memo:
        memo = None
    elif memo is None and recurse:
        memo = {}
    if recurse:
        for child in module.children():
            if child is not None:
                apply_to_parameters(child, fn, recurse=recurse, memo=memo, no_memo=no_memo)

    def call_fn(source: 'Tensor', is_param: 'bool'=False, requires_grad: 'bool'=False) ->Tensor:
        if memo is not None and source in memo:
            return memo[source]
        target = fn(source)
        if is_param:
            target = Parameter(target, requires_grad)
        elif requires_grad:
            target.requires_grad_(requires_grad)
        if memo is not None:
            memo[source] = target
        return target
    for param_name, param in module.named_parameters(recurse=False):
        if param is None:
            continue
        with torch.no_grad():
            new_param = call_fn(param, is_param=True, requires_grad=param.requires_grad)
        setattr(module, param_name, new_param)
        if (grad := param.grad) is not None:
            with torch.no_grad():
                new_grad = call_fn(grad, requires_grad=grad.requires_grad)
            new_param.grad = new_grad
    for buffer_name, buffer in module.named_buffers(recurse=False):
        if buffer is None:
            continue
        setattr(module, buffer_name, call_fn(buffer))


def to_empty(module: 'Module', device: 'Device', *, recurse: bool=True, memo: (dict[Tensor, Tensor] | None)=None) ->None:
    """Move the parameters and buffers of ``module`` to ``device`` without
    copying storage.

    :param module:
        The module to move.
    :param device:
        The target device.
    :param recurse:
        If ``True``, moves the parameters and buffers of descendant modules.
    :param memo:
        The memoization dictionary to detect shared parameters and buffers. If
        ``None``, constructs an internal one.
    """

    def convert(source: 'Tensor') ->Tensor:
        return torch.empty_like(source, device=device)
    apply_to_parameters(module, convert, recurse=recurse, memo=memo)


class _ReduceOnBackwardFunction(Function):

    @staticmethod
    def forward(ctx: 'Any', x: 'Tensor', gang: 'Gang') ->Tensor:
        ctx.gang = gang
        return x

    @staticmethod
    def backward(ctx: 'Any', grad_output: 'Tensor') ->tuple[Tensor, None]:
        ctx.gang.all_reduce(grad_output, ReduceOperation.SUM)
        return grad_output, None


def reduce_on_backward(x: 'Tensor', gang: 'Gang') ->Tensor:
    """Reduce the gradient of ``x`` across all processes in ``gang``."""
    return _ReduceOnBackwardFunction.apply(x, gang)


class PositionEncoder(Module, ABC):
    """Encodes sequences with positional information."""
    encoding_dim: 'int'
    max_seq_len: 'int | None'

    def __init__(self, encoding_dim: 'int', max_seq_len: 'int | None') ->None:
        """
        :param encoding_dim: The dimensionality of positional encodings. The
            last dimension of input sequences is expected to have the same
            dimensionality.
        :param max_seq_len: The maximum allowed length for input sequences.
            Sequences longer than ``max_seq_len`` will cause a :class:`ValueError`.
            Typically it is set to the context length of the underlying model.
            If ``None``, sequences can have arbitrary length.
        """
        super().__init__()
        self.encoding_dim = encoding_dim
        self.max_seq_len = max_seq_len

    def forward(self, seqs: 'Tensor', padding_mask: 'PaddingMask | None', *, state_bag: (IncrementalStateBag | None)=None) ->Tensor:
        """
        Returns a copy of ``seqs`` with positional information encoded.

        :param seqs: The input sequences to encode. *Shape:* :math:`(*,S,E)`,
            where :math:`*` is any number of batch dimensions including none,
            :math:`S` is the sequence length, and :math:`E` is the dimensionality
            of the positional encodings.
        :param padding_mask: The padding mask of ``seqs``. *Shape:* :math:`(*,S)`,
            where :math:`*` is any number of batch dimensions including none and
            :math:`S` is the sequence length.
        :param state_bag: If not ``None``, the encoder will operate in
            incremental decoding mode. This means that the first step in ``seqs``
            will be considered to be at position :attr:`state_bag.step_nr
            <fairseq2.nn.IncrementalStateBag.step_nr>` instead of 0.

        :raises ValueError: when the sequence length of ``seqs`` exceeds
            :attr:`max_seq_len`.

        :returns: The input sequences with positional information encoded.
            *Shape:* Same as ``seqs``.
        """
        if self.max_seq_len is not None:
            if self.training or state_bag is None:
                start_step = 0
            else:
                start_step = state_bag.step_nr
            if (seq_len := start_step + seqs.size(-2)) > self.max_seq_len:
                raise ValueError(f'The input sequence length must be less than or equal to the maximum sequence length ({self.max_seq_len}), but is {seq_len} instead.')
        return self._do_forward(seqs, padding_mask, state_bag)

    @abstractmethod
    def _do_forward(self, seqs: 'Tensor', padding_mask: 'PaddingMask | None', state_bag: 'IncrementalStateBag | None') ->Tensor:
        """
        When overriden in a subclass, returns a copy of ``seqs`` with positional
        information encoded. See :meth:`forward` for parameter descriptions.

        :meta public:
        """

    def extra_repr(self) ->str:
        """:meta private:"""
        s = f'encoding_dim={self.encoding_dim}'
        if self.max_seq_len is not None:
            s = f'{s}, max_seq_len={self.max_seq_len}'
        return s


def _fill_sin_freq_table(freqs: 'Tensor', encoding_dim: 'int', steps: 'Tensor', correction: 'int'=1) ->None:
    freqs = freqs.flatten(0, -2)
    num_sin = encoding_dim // 2
    l_half = freqs[:, :num_sin]
    r_half = freqs[:, num_sin:]
    indices = torch.arange(num_sin, device=steps.device, dtype=steps.dtype)
    freqs = torch.exp(indices * -math.log(10000.0) / (num_sin - correction))
    torch.outer(steps, freqs, out=l_half)
    r_dim = r_half.size(1)
    r_half.copy_(l_half[:, :r_dim])
    l_half.sin_()
    r_half.cos_()


class InterpolatedPositionEncoder(Module, ABC):
    """Encodes N-dimensional inputs with interpolated positional information."""
    encoding_dim: 'int'

    def __init__(self, encoding_dim: 'int') ->None:
        """
        :param encoding_dim: The dimensionality of positional encodings. The
            last dimension of inputs is expected to have the same dimensionality.
        """
        super().__init__()
        self.encoding_dim = encoding_dim

    @abstractmethod
    def forward(self, x: 'Tensor') ->Tensor:
        """
        Returns a copy of ``x`` with positional information encoded.

        :params x: The inputs to encode. *Shape:* :math:`(N,*,E)`, where
            :math:`N` is the batch size, :math:`*` is any number of
            implementation-specific dimensions, and :math:`E` is the
            dimensionality of the positional encodings.

        :returns: The inputs with positional information encoded.  *Shape:* Same
            as ``x``.
        """

    def extra_repr(self) ->str:
        """:meta private:"""
        return f'encoding_dim={self.encoding_dim}'


class _ScatterFunction(Function):

    @staticmethod
    def forward(ctx: 'Any', x: 'Tensor', gang: 'Gang', dim: 'int') ->Tensor:
        ctx.dim = dim
        ctx.gang = gang
        return _do_scatter(x, gang, dim)

    @staticmethod
    def backward(ctx: 'Any', grad_output: 'Tensor') ->tuple[Tensor, None, None]:
        x = _do_gather(grad_output, ctx.gang, ctx.dim)
        return x, None, None


def scatter(x: 'Tensor', gang: 'Gang', dim: 'int'=-1) ->Tensor:
    """Scatter ``x`` across all processes in ``gang`` over ``dim``.

    This is an autograd-aware operation and the backward pass will return the
    all-gathered gradient of ``x``.
    """
    return _ScatterFunction.apply(x, gang, dim)


class LoRALayer(ABC):

    def __init__(self, config: 'LoRAConfig'):
        self.r = config.r
        self.alpha = config.alpha
        self.scaling = self.alpha / self.r
        self.dropout_p = config.dropout_p

    @property
    @abstractmethod
    def wrapped_module(self) ->nn.Module:
        ...

    @abstractmethod
    def merge(self) ->None:
        ...

    @abstractmethod
    def unmerge(self) ->None:
        ...


@final
class LoRAEmbedding(Embedding, LoRALayer):
    wrapped: 'Embedding'
    weight: 'Parameter'
    lora_A: 'Parameter'
    lora_B: 'Parameter'
    merged: 'bool'

    def __init__(self, wrapped: 'Embedding', config: 'LoRAConfig', device: 'Device | None'=None, dtype: 'DataType | None'=None) ->None:
        """
        :param wrapped:
            The Embedding module to be wrapped.
        :param config:
            The LoRA config.
        """
        Embedding.__init__(self, wrapped.num_embeddings, wrapped.embedding_dim, wrapped.pad_idx)
        LoRALayer.__init__(self, config)
        self.wrapped = wrapped
        self.wrapped.weight.requires_grad_(False)
        self.weight = self.wrapped.weight
        self.lora_A = Parameter(torch.empty((self.r, self.num_embeddings), device=device, dtype=dtype))
        self.lora_B = Parameter(torch.empty((self.embedding_dim, self.r), device=device, dtype=dtype))
        self.merged = False
        self.reset_lora_parameters()

    def forward(self, x: 'Tensor') ->Tensor:
        if self.merged:
            return embedding(x, self.weight, self.pad_idx)
        else:
            return embedding(x, self.weight + (self.lora_B @ self.lora_A).T * self.scaling, self.pad_idx)

    def reset_lora_parameters(self) ->None:
        """Reset the parameters and buffers of the module."""
        nn.init.zeros_(self.lora_A)
        nn.init.normal_(self.lora_B)

    @property
    def wrapped_module(self) ->Embedding:
        return self.wrapped

    def merge(self) ->None:
        if self.merged:
            return
        with torch.no_grad():
            self.weight += (self.lora_B @ self.lora_A).T * self.scaling
        self.merged = True

    def unmerge(self) ->None:
        if not self.merged:
            return
        with torch.no_grad():
            self.weight -= (self.lora_B @ self.lora_A).T * self.scaling
        self.merged = False


@final
class LoRALinear(Projection, LoRALayer):
    wrapped: 'Projection'
    weight: 'Parameter'
    bias: 'Parameter | None'
    lora_A: 'Parameter'
    lora_B: 'Parameter'
    dropout: 'Dropout | None'
    skip_init: 'bool'
    merged: 'bool'

    def __init__(self, wrapped: 'Projection', config: 'LoRAConfig', skip_init: 'bool'=False, device: 'Device | None'=None, dtype: 'DataType | None'=None) ->None:
        """
        :param wrapped:
            The Linear module to be wrapped.
        :param config:
            The LoRA config.
        :param skip_init:
            If ``True``, the weights and bias will be left uninitialized, and
            :meth:`reset_lora_parameters` will become noop.
        """
        Projection.__init__(self, wrapped.input_dim, wrapped.output_dim)
        LoRALayer.__init__(self, config)
        self.wrapped = wrapped
        self.wrapped.weight.requires_grad_(False)
        self.weight = self.wrapped.weight
        if self.wrapped.bias is not None:
            self.wrapped.bias.requires_grad_(False)
            self.bias = self.wrapped.bias
        else:
            self.register_parameter('bias', None)
        self.lora_A = Parameter(torch.empty((self.r, self.input_dim), device=device, dtype=dtype))
        self.lora_B = Parameter(torch.empty((self.output_dim, self.r), device=device, dtype=dtype))
        if self.dropout_p > 0.0:
            self.dropout = Dropout(self.dropout_p)
        else:
            self.register_module('dropout', None)
        self.merged = False
        self.skip_init = skip_init
        self.reset_lora_parameters()

    def forward(self, x: 'Tensor') ->Tensor:
        if self.merged:
            return linear(x, self.weight, self.bias)
        else:
            h1 = linear(x, self.weight, self.bias)
            if self.dropout is not None:
                x = self.dropout(x)
            h2 = linear(x, self.lora_B @ self.lora_A * self.scaling)
            return h1 + h2

    def reset_lora_parameters(self) ->None:
        """Reset the parameters and buffers of the module."""
        if not self.skip_init:
            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
            nn.init.zeros_(self.lora_B)

    @property
    def wrapped_module(self) ->Projection:
        return self.wrapped

    def merge(self) ->None:
        if self.merged:
            return
        with torch.no_grad():
            self.weight += self.lora_B @ self.lora_A * self.scaling
        self.merged = True

    def unmerge(self) ->None:
        if not self.merged:
            return
        with torch.no_grad():
            self.weight -= self.lora_B @ self.lora_A * self.scaling
        self.merged = False


class AttentionMask(ABC):
    """Represents an attention mask."""

    @abstractmethod
    def materialize(self) ->Tensor:
        """Materialize the attention mask tensor."""


class SDPA(Module, ABC):
    """Computes scaled dot-product attention."""

    @abstractmethod
    def forward(self, seqs: 'Tensor', keys: 'Tensor', key_padding_mask: 'PaddingMask | None', values: 'Tensor', *, attn_mask: (AttentionMask | None)=None, needs_weights: bool=False) ->tuple[Tensor, Tensor | None]:
        """
        :param seqs:
            The sequences to query. *Shape:* :math:`(N,H,S,K)`, where :math:`N`
            is the batch size, :math:`H` is the number of heads, :math:`S` is
            the sequence length, and :math:`K` is the key size.
        :param keys:
            The keys. *Shape:* :math:`(N,H,S_{kv},K)`, where :math:`N` is the
            batch size, :math:`H` is the number of heads, :math:`S_{kv}` is the
            key/value sequence length, and :math:`K` is the key size.
        :param key_padding_mask:
            The padding mask indicating which key positions to ignore for the
            purpose of attention. *Shape:* :math:`(N,S_{kv})`, where :math:`N`
            is the batch size and :math:`S_{kv}` is the key/value sequence
            length.
        :param values:
            The values. *Shape:* :math:`(N,H,S_{kv},V)`, where :math:`N` is the
            batch size, :math:`H` is the number of heads, :math:`S_{kv}` is the
            key/value sequence length, and :math:`V` is the value size.
        :param attn_mask:
            The mask that will be added to attention weights before computing
            the attention. *Shape:* :math:`([H],S,S_{kv})`, where :math:`H` is
            the number of heads, :math:`S` is the sequence length, and
            :math:`S_{kv}` is the key/value sequence length.
        :param needs_weights:
            If ``True``, returns the attention weights.

        :returns:
            - The attention values. *Shape:* :math:`(N,H,S,V)`, where :math:`N`
              is the batch size, :math:`H` is the number of heads, :math:`S` is
              the sequence length, and :math:`V` is the value size.
            - The attention weights. *Shape:* :math:`(N,H,S,S_{kv})`, where
              :math:`N` is the batch size, :math:`H` is the number of heads,
              :math:`S` is the sequence length, and :math:`S_{kv}` is the
              key/value sequence length.
        """


def _create_causal_attention_mask(seq_len: 'int', key_len: 'int', attn_len: 'int | None', attn_window_len: 'int | None', device: 'Device | None', dtype: 'DataType | None') ->Tensor:
    if dtype is None:
        dtype = torch.get_default_dtype()
    dt = torch.float32 if dtype == torch.bfloat16 else dtype
    mask = torch.ones((seq_len, key_len), device=device, dtype=dt)
    mask.tril_(diagonal=0)
    if attn_window_len is not None:
        mask.triu_(diagonal=1 - attn_window_len)
    if attn_len is not None and attn_len != seq_len:
        mask = mask[-attn_len:]
    mask.log_()
    return mask


def _naive_scaled_dot_product_attention(seqs: 'Tensor', keys: 'Tensor', key_padding_mask: 'PaddingMask | None', values: 'Tensor', attn_mask: 'AttentionMask | None', dropout_p: 'float', needs_weights: 'bool', training: 'bool') ->tuple[Tensor, Tensor | None]:
    attn_weights = torch.matmul(seqs, keys.transpose(-1, -2))
    attn_weights = attn_weights * seqs.size(-1) ** -0.5
    if attn_mask is not None:
        m = attn_mask.materialize()
        attn_weights = attn_weights + m
    if key_padding_mask is not None:
        m = key_padding_mask.materialize()
        m = m[:, None, None, :]
        attn_weights = torch.where(m, attn_weights, -torch.inf)
    attn_weights = softmax(attn_weights, dim=-1, dtype=torch.float32)
    attn_weights = attn_weights.type_as(seqs)
    if training and dropout_p > 0.0:
        attn_weights = dropout(attn_weights, dropout_p)
    attn = torch.matmul(attn_weights, values)
    return attn, attn_weights if needs_weights else None


@final
class LogWriter:
    """Writes log messages using ``format()`` strings."""
    _NO_HIGHLIGHT: 'Final' = {'highlighter': None}
    _logger: 'Logger'

    def __init__(self, logger: 'Logger') ->None:
        """
        :param logger:
            The logger to write to.
        """
        self._logger = logger

    def debug(self, message: 'str', *args: Any, highlight: bool=False, **kwargs: Any) ->None:
        """Log a message with level ``DEBUG``."""
        self._write(logging.DEBUG, message, args, kwargs, highlight)

    def info(self, message: 'str', *args: Any, highlight: bool=False, **kwargs: Any) ->None:
        """Log a message with level ``INFO``."""
        self._write(logging.INFO, message, args, kwargs, highlight)

    def warning(self, message: 'str', *args: Any, highlight: bool=False, **kwargs: Any) ->None:
        """Log a message with level ``WARNING``."""
        self._write(logging.WARNING, message, args, kwargs, highlight)

    def error(self, message: 'str', *args: Any, highlight: bool=False, **kwargs: Any) ->None:
        """Log a message with level ``ERROR``."""
        self._write(logging.ERROR, message, args, kwargs, highlight)

    def exception(self, message: 'str', *args: Any, highlight: bool=False, **kwargs: Any) ->None:
        """Log a message with level ``ERROR``."""
        self._write(logging.ERROR, message, args, kwargs, highlight, exc_info=True)

    def _write(self, level: 'int', message: 'str', args: 'tuple[Any, ...]', kwargs: 'dict[str, Any]', highlight: 'bool', exc_info: 'bool'=False) ->None:
        if args or kwargs:
            if not self._logger.isEnabledFor(level):
                return
            message = str(message).format(*args, **kwargs)
        extra = None if highlight else self._NO_HIGHLIGHT
        self._logger.log(level, message, exc_info=exc_info, extra=extra)

    def is_enabled_for(self, level: 'int') ->bool:
        """Return ``True`` if the writer is enabled for ``level``."""
        return self._logger.isEnabledFor(level)

    def is_enabled_for_debug(self) ->bool:
        """Return ``True`` if the writer is enabled for ``logging.DEBUG``."""
        return self._logger.isEnabledFor(logging.DEBUG)

    def is_enabled_for_info(self) ->bool:
        """Return ``True`` if the writer is enabled for ``loggig.INFO``."""
        return self._logger.isEnabledFor(logging.INFO)


def get_log_writer(name: 'str | None'=None) ->LogWriter:
    """Return a :class:`LogWriter` for the logger with the specified name."""
    return LogWriter(getLogger(name))


class AttentionMaskFactory(Protocol):
    """Constructs instances of :class:`AttentionMask`."""

    def __call__(self, seqs: 'Tensor', keys: 'Tensor', *, training: bool=True, state_bag: (IncrementalStateBag | None)=None) ->(AttentionMask | None):
        """
        :param seqs:
            The sequences for which to make a mask. *Shape:* :math:`(N,S,M)`,
            where :math:`N` is the batch size, :math:`S` is the sequence length,
            and :math:`M` is the dimensionality of the model.
        :param keys:
            The keys. *Shape:* :math:`(N,S_{kv},K)`, where :math:`N` is the
            batch size, :math:`S_{kv}` is the key/value sequence length, and
            :math:`K` is the key size.
        :param training:
            If ``True``, the calling module is in training mode.
        :param state_bag:
            The state bag to use for incremental decoding.

        :returns:
            An implementation-defined mask for ``seqs``.
        """


class InvalidOperationError(Exception):
    pass


class TransformerNormOrder(Enum):
    """Specifies the Layer Normalization order."""
    POST = 0
    """Apply Layer Normalization after each layer's residual connection as
    described in :cite:t:`https://doi.org/10.48550/arxiv.1706.03762`."""
    PRE = 1
    """Apply Layer Normalization at the beginning of each layer as described in
    :cite:t:`https://doi.org/10.48550/arxiv.2002.04745`."""
    PRE_WITH_NORMFORMER = 2
    """Apply Layer Normalization as described in
    :cite:t:`https://doi.org/10.48550/arxiv.2110.09456`."""


class _RecordDropForBackwardFunction(Function):

    @staticmethod
    def forward(ctx: 'Any', x: 'Tensor', dropped_output: 'Tensor') ->Tensor:
        return x

    @staticmethod
    def backward(ctx: 'Any', grad_output: 'Tensor') ->tuple[Tensor, Tensor]:
        return grad_output, torch.zeros_like(grad_output)


def _record_drop_for_backward(x: 'Tensor', dropped_output: 'Tensor') ->Tensor:
    return _RecordDropForBackwardFunction.apply(x, dropped_output)


class ResidualConnect(Module, ABC):
    """Represents a residual connection within a Transformer layer."""

    @abstractmethod
    def forward(self, seqs: 'Tensor', residual: 'Tensor') ->Tensor:
        """
        :param seqs: The sequences output by a module such as a multi-head
            attention layer or a feed-forward network. *Shape:* :math:`(N,S,M)`,
            where :math:`N` is the batch size, :math:`S` is the sequence length,
            and :math:`M` is the dimensionality of the model.
        :param residual: The input sequences to the module. *Shape:* Same as
            ``seqs``.

        :returns: The output sequences with residuals applied. *Shape:* Same as
            ``seqs``.
        """


class FeedForwardNetwork(Module, ABC):
    """Represents a Transformer feed-forward network."""
    model_dim: 'int'

    def __init__(self, model_dim: 'int') ->None:
        """
        :param model_dim:
            The dimensionality of the model.
        """
        super().__init__()
        self.model_dim = model_dim

    @abstractmethod
    def forward(self, seqs: 'Tensor') ->Tensor:
        """
        :param seqs:
            The sequences to project. *Shape:* :math:`(N,S,M)`, where :math:`N`
            is the batch size, :math:`S` is the sequence length, and :math:`M`
            is the dimensionality of the model.

        :returns:
            The projected sequences. *Shape:* Same as ``seqs``.
        """

    def extra_repr(self) ->str:
        """:meta private:"""
        return f'model_dim={self.model_dim}'


class MultiheadAttention(Module, ABC):
    """Represents a Transformer multi-head attention layer."""
    num_heads: 'int'
    model_dim: 'int'
    _attn_weight_hooks: 'dict[int, AttentionWeightHook]'

    def __init__(self, model_dim: 'int', num_heads: 'int') ->None:
        """
        :param model_dim:
            The dimensionality of the model.
        :param num_heads:
            The number of attention heads.
        """
        super().__init__()
        self.model_dim = model_dim
        self.num_heads = num_heads
        self._attn_weight_hooks = OrderedDict()

    @abstractmethod
    def forward(self, seqs: 'Tensor', padding_mask: 'PaddingMask | None', keys: 'Tensor', key_padding_mask: 'PaddingMask | None', values: 'Tensor', *, attn_mask: (AttentionMask | None)=None, state_bag: (IncrementalStateBag | None)=None) ->Tensor:
        """
        :param seqs:
            The sequences to query. *Shape:* :math:`(N,S,M)`, where :math:`N` is
            the batch size, :math:`S` is the sequence length, and :math:`M` is
            the dimensionality of the model.
        :param padding_mask:
            The padding mask of ``seqs``. *Shape:* :math:`(N,S)`, where :math:`N`
            is the batch size and :math:`S` is the sequence length.
        :param keys:
            The keys. *Shape:* :math:`(N,S_{kv},K)`, where :math:`N` is the
            batch size, :math:`S_{kv}` is the key/value sequence length, and
            :math:`K` is the key size.
        :param key_padding_mask:
            The padding mask indicating which key positions to ignore for the
            purpose of attention. *Shape:* :math:`(N,S_{kv})`, where :math:`N`
            is the batch size and :math:`S_{kv}` is the key/value sequence
            length.
        :param values:
            The values. *Shape:* :math:`(N,S_{kv},V)`, where :math:`N` is the
            batch size, :math:`S_{kv}` is the key/value sequence length, and
            :math:`V` is the value size.
        :param attn_mask:
            The mask that will be added to attention weights before computing
            the attention. *Shape:* :math:`([H],S,S_{kv})`, where :math:`H` is
            the number of attention heads, :math:`S` is the sequence length, and
            :math:`S_{kv}` is the key/value sequence length.
        :param state_bag:
            The state bag to use for incremental decoding.

        :returns:
            The attention values for ``seqs``. *Shape:* :math:`(N,S,M)`, where
            :math:`N` is the batch size, :math:`S` is the sequence length, and
            :math:`M` is the dimensionality of the model.
        """

    def register_attn_weight_hook(self, hook: 'AttentionWeightHook') ->RemovableHandle:
        """Register an attention weight hook on the module.

        The hook will be called every time after the module has computed
        attention weights.

        :param hook:
            The hook to register.

        :returns:
            A handle that can be used to remove the added hook by calling
            ``handle.remove()``.
        """
        handle = RemovableHandle(self._attn_weight_hooks)
        self._attn_weight_hooks[handle.id] = hook
        return handle

    def extra_repr(self) ->str:
        """:meta private:"""
        return f'num_heads={self.num_heads}, model_dim={self.model_dim}'


class IncrementalState(ABC):
    """Holds the state of a module during incremental decoding.

    Incremental decoding is a special mode at inference time where the module
    only receives an input corresponding to the previous output and must produce
    the next output incrementally. Thus the module must cache any long-term
    state that is needed about the sequence.
    """

    @abstractmethod
    def reorder(self, new_order: 'Tensor') ->None:
        """Rearrange the state according to a new batch order.

        This will be called when the order of the batch has changed. A typical
        use case is beam search, where the batch order changes between steps
        based on the selection of beams.

        :param new_order:
            The new order of the batch. It is frequently used with
            :func:`torch.index_select` to rearrange the state tensors. *Shape:*
            :math:`(N)`, where :math:`N` is the batch size.
        """

    @abstractmethod
    def size_bytes(self) ->int:
        """Return the size of the state in bytes."""

    @abstractmethod
    def capacity_bytes(self) ->int:
        """Return the reserved capacity of the state in bytes."""


class AttentionState(IncrementalState):
    """Holds the projected keys and values of a :class:`MultiheadAttention`
    module during incremental decoding."""

    @abstractmethod
    def append(self, k: 'Tensor', v: 'Tensor') ->None:
        """Update the state with ``k`` and ``v``.

        :param k:
            The projected keys of the current step. *Shape:*
            :math:`(N,H,1,K_{proj})`, where :math:`N` is the batch size,
            :math:`H` is the number of heads, :math:`1` is the step length, and
            :math:`K_{proj}` is the projected key size.
        :param v:
            The projected values of the current step. *Shape:*
            :math:`(N,H,1,V_{proj})`, where :math:`N` is the batch size,
            :math:`H` is the number of heads, :math:`1` is the step length, and
            :math:`V_{proj}` is the projected value size.
        """

    @abstractmethod
    def get(self) ->tuple[Tensor, Tensor]:
        """Return the state that should be used to compute the attention.

        :returns:
            - The projected keys.
            - The projected values.
        """


class AttentionStateFactory(Protocol):
    """Constructs instances of :class:`AttentionState`."""

    def __call__(self, k: 'Tensor', v: 'Tensor', max_seq_len: 'int', capacity_increment: 'int | None') ->AttentionState:
        """
        :param k:
            The initial projected keys.
        :param v:
            The initial projected values.
        :param max_seq_len:
            The maximum sequence length.
        :param capacity_increment:
            The sequence length capacity of state tensors will be incremented by
            multiples of this value. If ``None``, state tensors will be
            preallocated with a capacity of ``max_seq_len``.
        """


def create_default_sdpa(*, attn_dropout_p: float=0.0) ->SDPA:
    """Make an instance of the default :class:`SDPA`.

    :param attn_dropout_p:
        The dropout probability on attention weights.
    """
    return _sdpa_factory(attn_dropout_p=attn_dropout_p)


def init_output_projection(proj: 'Linear') ->None:
    """Initialize ``proj`` as a multi-head attention output projection."""
    nn.init.xavier_uniform_(proj.weight)
    if proj.bias is not None:
        nn.init.zeros_(proj.bias)


def init_qkv_projection(proj: 'Linear') ->None:
    """Initialize ``proj`` as a multi-head attention input projection."""
    nn.init.xavier_uniform_(proj.weight, gain=2 ** -0.5)
    if proj.bias is not None:
        nn.init.zeros_(proj.bias)


def init_shaw_embedding(embed: 'StandardEmbedding') ->None:
    """Initialize ``embed`` for use in :class:`ShawRelativePositionSDPA`."""
    nn.init.xavier_uniform_(embed.weight)


class AdamWTestNet(Module):

    def __init__(self, dtype: 'DataType') ->None:
        super().__init__()
        self.conv1 = Conv2d(8, 4, 1, device=device, dtype=dtype)
        self.conv2 = Conv2d(4, 2, 1, device=device, dtype=dtype)

    def forward(self, x: 'Tensor') ->Tensor:
        return self.conv2(relu(self.conv1(x)))


class LRSchedulerTestNet(Module):

    def __init__(self) ->None:
        super().__init__()
        self.conv1 = Conv2d(1, 1, 1)
        self.conv2 = Conv2d(1, 1, 1)

    def forward(self, x: 'Tensor') ->Tensor:
        return self.conv2(relu(self.conv1(x)))


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (LRSchedulerTestNet,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
]

class Test_facebookresearch_fairseq2(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

