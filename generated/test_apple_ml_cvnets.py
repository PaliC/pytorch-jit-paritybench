import sys
_module = sys.modules[__name__]
del sys
common = _module
conftest = _module
cvnets = _module
anchor_generator = _module
base_anchor_generator = _module
ssd_anchor_generator = _module
image_projection_layers = _module
attention_pool_2d = _module
base_image_projection = _module
global_pool_2d = _module
simple_projection_head = _module
layers = _module
activation = _module
gelu = _module
hard_sigmoid = _module
hard_swish = _module
leaky_relu = _module
prelu = _module
relu = _module
relu6 = _module
sigmoid = _module
swish = _module
tanh = _module
adaptive_pool = _module
base_layer = _module
conv_layer = _module
dropout = _module
embedding = _module
flatten = _module
global_pool = _module
identity = _module
linear_attention = _module
linear_layer = _module
multi_head_attention = _module
normalization = _module
batch_norm = _module
group_norm = _module
instance_norm = _module
layer_norm = _module
sync_batch_norm = _module
normalization_layers = _module
pixel_shuffle = _module
pooling = _module
positional_embedding = _module
positional_encoding = _module
random_layers = _module
single_head_attention = _module
softmax = _module
stochastic_depth = _module
token_merging = _module
upsample = _module
matcher_det = _module
base_matcher = _module
ssd_matcher = _module
misc = _module
averaging_utils = _module
box_utils = _module
common = _module
init_utils = _module
third_party = _module
ssd_utils = _module
models = _module
audio_classification = _module
audio_byteformer = _module
base_audio_classification = _module
base_model = _module
classification = _module
base_image_encoder = _module
byteformer = _module
config = _module
efficientnet = _module
fastvit = _module
mobilenetv1 = _module
mobilenetv2 = _module
mobilenetv3 = _module
mobileone = _module
mobilevit = _module
mobilevit_v2 = _module
regnet = _module
resnet = _module
swin_transformer = _module
vit = _module
efficientnet = _module
fastvit = _module
mobilenetv1 = _module
mobilenetv2 = _module
mobilenetv3 = _module
mobileone = _module
mobilevit = _module
mobilevit_v2 = _module
regnet = _module
resnet = _module
swin_transformer = _module
vit = _module
detection = _module
base_detection = _module
mask_rcnn = _module
ssd = _module
utils = _module
rcnn_utils = _module
multi_modal_img_text = _module
base_multi_modal_img_text = _module
clip = _module
segmentation = _module
base_seg = _module
enc_dec = _module
heads = _module
base_seg_head = _module
deeplabv3 = _module
pspnet = _module
simple_seg_head = _module
modules = _module
aspp_block = _module
base_module = _module
efficientnet = _module
fastvit = _module
feature_pyramid = _module
mobilenetv2 = _module
mobileone_block = _module
mobilevit_block = _module
pspnet_module = _module
regnet_modules = _module
resnet_modules = _module
squeeze_excitation = _module
ssd_heads = _module
swin_transformer_block = _module
transformer = _module
windowed_transformer = _module
neural_augmentor = _module
neural_aug = _module
neural_aug_utils = _module
text_encoders = _module
base_text_encoder = _module
transformer = _module
data = _module
collate_fns = _module
byteformer_collate_functions = _module
collate_functions = _module
data_loaders = _module
datasets = _module
speech_commands_v2 = _module
base_image_classification_dataset = _module
base_imagenet_shift_dataset = _module
imagenet = _module
imagenet_a = _module
imagenet_r = _module
imagenet_sketch = _module
imagenet_synsets = _module
imagenet_v2 = _module
places365 = _module
dataset_base = _module
coco_base = _module
coco_mask_rcnn = _module
coco_ssd = _module
base_multi_modal_img_text = _module
flickr = _module
img_text_tar_dataset = _module
zero_shot = _module
base_zero_shot = _module
imagenet_class_names = _module
templates = _module
ade20k = _module
base_segmentation = _module
coco_segmentation = _module
pascal_voc = _module
text = _module
video = _module
loader = _module
dataloader = _module
sampler = _module
base_sampler = _module
batch_sampler = _module
chain_sampler = _module
multi_scale_sampler = _module
variable_batch_sampler = _module
text_tokenizer = _module
base_tokenizer = _module
clip_tokenizer = _module
transforms = _module
audio = _module
audio_aux = _module
mfccs = _module
audio_bytes = _module
base_transforms = _module
image_bytes = _module
image_pil = _module
image_torch = _module
video = _module
video_reader = _module
base_av_reader = _module
decord_reader = _module
pyav_reader = _module
docs = _module
conf = _module
engine = _module
detection_utils = _module
coco_map = _module
eval_detection = _module
eval_segmentation = _module
evaluation_engine = _module
segmentation_utils = _module
cityscapes_iou = _module
training_engine = _module
utils = _module
loss_fn = _module
base_criteria = _module
base_classification_criteria = _module
binary_cross_entropy = _module
cross_entropy = _module
composite_loss = _module
base_detection_criteria = _module
mask_rcnn_loss = _module
ssd_multibox_loss = _module
distillation = _module
base_distillation = _module
hard_distillation = _module
soft_kl_distillation = _module
base_multi_modal_img_text_criteria = _module
contrastive_loss_clip = _module
neural_augmentation = _module
base_segmentation_criteria = _module
cross_entropy = _module
build_helper = _module
class_weighting = _module
loss_landscape = _module
landscape_utils = _module
main_benchmark = _module
main_conversion = _module
main_eval = _module
main_loss_landscape = _module
main_train = _module
metrics = _module
average_precision = _module
coco_map = _module
confusion_mat = _module
image_text_retrieval = _module
intersection_over_union = _module
metric_base = _module
metric_base_test = _module
misc = _module
probability_histograms = _module
psnr = _module
retrieval_cmc = _module
stats = _module
topk_accuracy = _module
optim = _module
adam = _module
adamw = _module
base_optim = _module
scheduler = _module
base_scheduler = _module
cosine = _module
cyclic = _module
fixed = _module
multi_step = _module
polynomial = _module
sgd = _module
options = _module
errors = _module
opts = _module
parse_args = _module
setup = _module
tests = _module
configs = _module
test_collate_functions = _module
test_speech_commands_v2 = _module
mock_imagenet = _module
test_base_image_classification_dataset = _module
test_mock_imagenet = _module
mock_ade20k = _module
test_mock_ade20k = _module
test_dataset_base = _module
test_common = _module
test_video = _module
samplers = _module
test_chain_sampler = _module
test_data_samplers = _module
test_av_reader = _module
dummy_datasets = _module
classification = _module
multi_modal_img_text = _module
segmentation = _module
ssd_detection = _module
dummy_loader = _module
test_token_merging = _module
loss_fns = _module
test_class_weighting = _module
test_classification_loss = _module
test_composite_loss = _module
test_contrastive_loss = _module
test_detection_loss = _module
test_distillation_loss = _module
test_neural_aug = _module
test_neural_aug_compatibility = _module
test_segmentation_loss = _module
base = _module
test_coco_map = _module
test_image_text_retrieval_metrics = _module
test_iou = _module
test_misc = _module
test_probability_histogram = _module
test_psnr = _module
test_retrieval_cmc_metrics = _module
test_topk_accuracy = _module
test_common = _module
test_base_audio_classification = _module
test_byteformer = _module
test_byteformer = _module
test_neural_aug_utils = _module
test_transformer = _module
test_windowed_transformer = _module
test_parse_args = _module
test_utils = _module
test_byteformer_collate_fn = _module
test_conventions = _module
test_image_pil = _module
test_model = _module
test_multi_head_attn = _module
test_pos_embeddings = _module
test_scheduler = _module
test_tokenizer = _module
test_training_engine = _module
test_audio = _module
test_audio_bytes = _module
test_image = _module
test_image_bytes = _module
test_video = _module
test_common_utils = _module
test_dict_utils = _module
test_import_utils = _module
checkpoint_utils = _module
color_map = _module
common_utils = _module
ddp_utils = _module
dict_utils = _module
download_utils = _module
download_utils_base = _module
import_utils = _module
logger = _module
math_utils = _module
object_utils = _module
object_utils_test = _module
pytorch_to_coreml = _module
registry = _module
registry_test = _module
resources = _module
tensor_utils = _module
ddp_functional_utils = _module
visualization_utils = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import operator as op
from dataclasses import dataclass
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


from typing import Optional


from typing import Tuple


from typing import Union


import torch


from torch import Tensor


from itertools import product


from typing import List


import numpy as np


from torch import nn


from torch.nn import functional as F


from typing import Dict


import torch.nn


from typing import Any


from typing import Type


from torch import Size


import math


import random


from torchvision.ops import StochasticDepth as StochasticDepthTorch


from copy import deepcopy


import re


from types import MethodType


from typing import Mapping


from torch.utils.checkpoint import checkpoint as gradient_checkpoint_fn


from torch.nn import init


import copy


import torch.nn as nn


from functools import partial


from torch.utils.checkpoint import checkpoint_sequential as gradient_checkpoint_fn


from torchvision.models.detection.anchor_utils import AnchorGenerator


from torchvision.models.detection.mask_rcnn import MaskRCNN


from torchvision.ops import MultiScaleRoIAlign


from torchvision.ops import batched_nms


import torch.nn.functional as F


from typing import Sequence


from torchvision.ops.roi_align import RoIAlign


from torch.nn import functional


from torch.utils.data import default_collate


from torch.utils.data.sampler import Sampler


import torchaudio


from torchvision.datasets import ImageFolder


from abc import ABC


from typing import TypedDict


from torch.utils import data


from typing import AnyStr


from torch.utils.data import DataLoader


from typing import Callable


from typing import Iterator


import torch.distributed as dist


import itertools


from torchvision import transforms as T


from torchvision.transforms import functional as F


from torch.nn import functional as F_torch


from torchvision.io import write_video


from torchvision.transforms import functional as FV


import numpy


from torchvision.transforms import functional as F_vision


import time


from torch.cuda.amp import autocast


import abc


import matplotlib.pyplot as plt


from matplotlib import animation


from matplotlib import cm


from torch.cuda.amp import GradScaler


from torch.distributed.elastic.multiprocessing import errors


from numbers import Number


from sklearn.metrics import average_precision_score


from typing import Iterable


from torch.optim import Adam


from torch.optim import AdamW


from torch.optim import SGD


import torch.utils.data as data


from collections import OrderedDict


import scipy.io.wavfile as wav


from torch.utils.mobile_optimizer import optimize_for_mobile


from torch import distributed as dist


from torch.autograd import Function


from matplotlib.colors import hsv_to_rgb


class BaseAnchorGenerator(torch.nn.Module):
    """
    Base class for anchor generators for the task of object detection.
    """

    def __init__(self, *args, **kwargs) ->None:
        super().__init__()
        self.anchors_dict = dict()

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser') ->argparse.ArgumentParser:
        """
        Add anchor generator-specific arguments to the parser
        """
        return parser

    def num_anchors_per_os(self):
        """Returns anchors per output stride. Child classes must implement this function."""
        raise NotImplementedError

    @torch.no_grad()
    def _generate_anchors(self, height: 'int', width: 'int', output_stride: 'int', device: 'Optional[str]'='cpu', *args, **kwargs) ->Union[Tensor, Tuple[Tensor, ...]]:
        raise NotImplementedError

    @torch.no_grad()
    def _get_anchors(self, fm_height: 'int', fm_width: 'int', fm_output_stride: 'int', device: 'Optional[str]'='cpu', *args, **kwargs) ->Union[Tensor, Tuple[Tensor, ...]]:
        key = 'h_{}_w_{}_os_{}'.format(fm_height, fm_width, fm_output_stride)
        if key not in self.anchors_dict:
            default_anchors_ctr = self._generate_anchors(*args, height=fm_height, width=fm_width, output_stride=fm_output_stride, device=device, **kwargs)
            self.anchors_dict[key] = default_anchors_ctr
            return default_anchors_ctr
        else:
            return self.anchors_dict[key]

    @torch.no_grad()
    def forward(self, fm_height: 'int', fm_width: 'int', fm_output_stride: 'int', device: 'Optional[str]'='cpu', *args, **kwargs) ->Union[Tensor, Tuple[Tensor, ...]]:
        """
        Returns anchors for the feature map

        Args:
            fm_height (int): Height of the feature map
            fm_width (int): Width of the feature map
            fm_output_stride (int): Output stride of the feature map
            device (Optional, str): Device (cpu or cuda). Defaults to cpu

        Returns:
            Tensor or Tuple of Tensors
        """
        return self._get_anchors(*args, fm_height=fm_height, fm_width=fm_width, fm_output_stride=fm_output_stride, device=device, **kwargs)


class SSDAnchorGenerator(BaseAnchorGenerator):
    """
    This class generates anchors (or priors) ``on-the-fly`` for the
    `single shot object detector (SSD) <https://arxiv.org/abs/1512.02325>`_.
    """

    def __init__(self, opts, *args, **kwargs) ->None:
        output_strides = getattr(opts, 'anchor_generator.ssd.output_strides', [32, 64, 128, 256, -1])
        aspect_ratios = getattr(opts, 'anchor_generator.ssd.aspect_ratios', [[2, 3]] * len(output_strides))
        min_ratio = getattr(opts, 'anchor_generator.ssd.min_scale_ratio', 0.1)
        max_ratio = getattr(opts, 'anchor_generator.ssd.max_scale_ratio', 1.05)
        no_clipping = getattr(opts, 'anchor_generator.ssd.no_clipping', False)
        step = getattr(opts, 'anchor_generator.ssd.step', [1])
        if isinstance(step, int):
            step = [step] * len(output_strides)
        elif isinstance(step, List) and len(step) <= len(output_strides):
            step = step + [1] * (len(output_strides) - len(step))
        else:
            logger.error('--anchor-generator.ssd.step should be either a list of ints with the same length as the output strides OR an integer')
        super().__init__()
        aspect_ratios = [list(set(ar)) for ar in aspect_ratios]
        output_strides_aspect_ratio = dict()
        for k, v in zip(output_strides, aspect_ratios):
            output_strides_aspect_ratio[k] = v
        self.output_strides_aspect_ratio = output_strides_aspect_ratio
        self.output_strides = output_strides
        self.anchors_dict = dict()
        self.num_output_strides = len(output_strides)
        self.num_aspect_ratios = len(aspect_ratios)
        scales = np.linspace(min_ratio, max_ratio, len(output_strides) + 1)
        self.sizes = dict()
        for i, s in enumerate(output_strides):
            self.sizes[s] = {'min': scales[i], 'max': (scales[i] * scales[i + 1]) ** 0.5, 'step': step[i]}
        self.clip = not no_clipping
        self.min_scale_ratio = min_ratio
        self.max_scale_ratio = max_ratio
        self.step = step

    def __repr__(self):
        return '{}(min_scale_ratio={}, max_scale_ratio={}, n_output_strides={}, n_aspect_ratios={}, clipping={})'.format(self.__class__.__name__, self.min_scale_ratio, self.max_scale_ratio, self.num_output_strides, self.num_aspect_ratios, self.clip)

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser') ->argparse.ArgumentParser:
        """
        Adds SSD anchor generator-specific arguments to the parser
        """
        group = parser.add_argument_group(title=cls.__name__)
        group.add_argument('--anchor-generator.ssd.output-strides', nargs='+', type=int, help='Output strides of the feature maps for which we want to generate anchors')
        group.add_argument('--anchor-generator.ssd.aspect-ratios', nargs='+', type=float, action='append', help='Aspect ratios at each output stride')
        group.add_argument('--anchor-generator.ssd.min-scale-ratio', type=float, help='Min. scale ratio')
        group.add_argument('--anchor-generator.ssd.max-scale-ratio', type=float, help='Max. scale ratio')
        group.add_argument('--anchor-generator.ssd.no-clipping', action='store_true', help="Don't clip the anchors")
        group.add_argument('--anchor-generator.ssd.step', type=int, default=[1], nargs='+', help='Step between pixels')
        return parser

    def num_anchors_per_os(self) ->List:
        """
        Returns anchors per output stride for SSD
        """
        return [(2 + 2 * len(ar)) for os, ar in self.output_strides_aspect_ratio.items()]

    @torch.no_grad()
    def _generate_anchors(self, height: 'int', width: 'int', output_stride: 'int', device: 'Optional[str]'='cpu', *args, **kwargs) ->Tensor:
        min_size_h = self.sizes[output_stride]['min']
        min_size_w = self.sizes[output_stride]['min']
        max_size_h = self.sizes[output_stride]['max']
        max_size_w = self.sizes[output_stride]['max']
        aspect_ratio = self.output_strides_aspect_ratio[output_stride]
        step = max(1, self.sizes[output_stride]['step'])
        default_anchors_ctr = []
        start_step = max(0, step // 2)
        for y, x in product(range(start_step, height, step), range(start_step, width, step)):
            cx = (x + 0.5) / width
            cy = (y + 0.5) / height
            default_anchors_ctr.append([cx, cy, min_size_w, min_size_h])
            default_anchors_ctr.append([cx, cy, max_size_w, max_size_h])
            for ratio in aspect_ratio:
                ratio = ratio ** 0.5
                default_anchors_ctr.extend([[cx, cy, min_size_w * ratio, min_size_h / ratio], [cx, cy, min_size_w / ratio, min_size_h * ratio]])
        default_anchors_ctr = torch.tensor(default_anchors_ctr, dtype=torch.float, device=device)
        if self.clip:
            default_anchors_ctr = torch.clamp(default_anchors_ctr, min=0.0, max=1.0)
        return default_anchors_ctr


def parameter_list(named_parameters, weight_decay: 'Optional[float]'=0.0, no_decay_bn_filter_bias: 'Optional[bool]'=False, *args, **kwargs) ->List[Dict]:
    module_name = kwargs.get('module_name', '')
    with_decay = []
    without_decay = []
    with_decay_param_names = []
    without_decay_param_names = []
    if isinstance(named_parameters, list):
        for n_parameter in named_parameters:
            for p_name, param in n_parameter():
                if param.requires_grad and len(param.shape) == 1 and no_decay_bn_filter_bias:
                    without_decay.append(param)
                    without_decay_param_names.append(module_name + p_name)
                elif param.requires_grad:
                    with_decay.append(param)
                    with_decay_param_names.append(module_name + p_name)
    else:
        for p_name, param in named_parameters():
            if param.requires_grad and len(param.shape) == 1 and no_decay_bn_filter_bias:
                without_decay.append(param)
                without_decay_param_names.append(module_name + p_name)
            elif param.requires_grad:
                with_decay.append(param)
                with_decay_param_names.append(module_name + p_name)
    param_list = [{'params': with_decay, 'weight_decay': weight_decay, 'param_names': with_decay_param_names}]
    if len(without_decay) > 0:
        param_list.append({'params': without_decay, 'weight_decay': 0.0, 'param_names': without_decay_param_names})
    return param_list


class BaseImageProjectionHead(nn.Module):
    """Base class that projects image representations to the same space as text representations"""

    def __init__(self, opts, *args, **kwargs) ->None:
        super().__init__()
        self.lr_mult = getattr(opts, 'model.image_projection_head.lr_multiplier', 1.0)

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser'):
        """Add model specific arguments"""
        group = parser.add_argument_group(title=cls.__name__)
        group.add_argument('--model.image-projection-head.name', type=str, default=None, help='Name of the image projection head')
        group.add_argument('--model.image-projection-head.lr-multiplier', type=float, default=1.0, help='LR multiplier for image projection head')
        return parser

    def reset_parameters(self) ->None:
        """Reset weights of a given layer"""
        raise NotImplementedError

    def get_trainable_parameters(self, weight_decay: 'Optional[float]'=0.0, no_decay_bn_filter_bias: 'Optional[bool]'=False, *args, **kwargs):
        param_list = parameter_list(named_parameters=self.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias)
        return param_list, [self.lr_mult] * len(param_list)

    def forward(self, input: 'Dict', *args, **kwargs) ->Dict:
        raise NotImplementedError


class BaseLayer(nn.Module):
    """
    Base class for neural network layers. Subclass must implement `forward` function.
    """

    def __init__(self, *args, **kwargs) ->None:
        super().__init__()

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser') ->argparse.ArgumentParser:
        """Add layer specific arguments"""
        return parser

    def get_trainable_parameters(self, weight_decay: 'Optional[float]'=0.0, no_decay_bn_filter_bias: 'Optional[bool]'=False, *args, **kwargs) ->Tuple[List[Dict], List[float]]:
        """
        Get parameters for training along with the learning rate.

        Args:
            weight_decay: weight decay
            no_decay_bn_filter_bias: Do not decay BN and biases. Defaults to False.

        Returns:
             Returns a tuple of length 2. The first entry is a list of dictionary with three keys
             (params, weight_decay, param_names). The second entry is a list of floats containing
             learning rate for each parameter.

        Note:
            Learning rate multiplier is set to 1.0 here as it is handled inside the Central Model.
        """
        param_list = parameter_list(*args, named_parameters=self.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, **kwargs)
        return param_list, [1.0] * len(param_list)

    def forward(self, *args, **kwargs) ->Any:
        """Forward function."""
        raise NotImplementedError('Sub-classes should implement forward method')

    def __repr__(self):
        return '{}'.format(self.__class__.__name__)


class GlobalPool(BaseLayer):
    """
    This layers applies global pooling over a 4D or 5D input tensor

    Args:
        pool_type (Optional[str]): Pooling type. It can be mean, rms, or abs. Default: `mean`
        keep_dim (Optional[bool]): Do not squeeze the dimensions of a tensor. Default: `False`

    Shape:
        - Input: :math:`(N, C, H, W)` or :math:`(N, C, D, H, W)`
        - Output: :math:`(N, C, 1, 1)` or :math:`(N, C, 1, 1, 1)` if keep_dim else :math:`(N, C)`
    """
    pool_types = ['mean', 'rms', 'abs']

    def __init__(self, pool_type: 'Optional[str]'='mean', keep_dim: 'Optional[bool]'=False, *args, **kwargs) ->None:
        super().__init__()
        if pool_type not in self.pool_types:
            logger.error('Supported pool types are: {}. Got {}'.format(self.pool_types, pool_type))
        self.pool_type = pool_type
        self.keep_dim = keep_dim

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser'):
        cls_name = '{} arguments'.format(cls.__name__)
        group = parser.add_argument_group(title=cls_name, description=cls_name)
        group.add_argument('--model.layer.global-pool', type=str, default='mean', help='Which global pooling?')
        return parser

    def _global_pool(self, x: 'Tensor', dims: 'List'):
        if self.pool_type == 'rms':
            x = x ** 2
            x = torch.mean(x, dim=dims, keepdim=self.keep_dim)
            x = x ** -0.5
        elif self.pool_type == 'abs':
            x = torch.mean(torch.abs(x), dim=dims, keepdim=self.keep_dim)
        else:
            x = torch.mean(x, dim=dims, keepdim=self.keep_dim)
        return x

    def forward(self, x: 'Tensor') ->Tensor:
        if x.dim() == 4:
            dims = [-2, -1]
        elif x.dim() == 5:
            dims = [-3, -2, -1]
        else:
            raise NotImplementedError('Currently 2D and 3D global pooling supported')
        return self._global_pool(x, dims=dims)

    def __repr__(self):
        return '{}(type={})'.format(self.__class__.__name__, self.pool_type)


def is_master(opts) ->bool:
    node_rank = getattr(opts, 'ddp.rank', 0)
    return node_rank == 0


class GlobalPool2D(BaseImageProjectionHead):
    """This class implements global pooling with linear projection"""

    def __init__(self, opts, in_dim: 'int', out_dim: 'int', *args, **kwargs) ->None:
        super().__init__(opts, *args, **kwargs)
        scale = in_dim ** -0.5
        self.use_identity = getattr(opts, 'model.image_projection_head.global_pool_nchw2nc.identity_if_same_size') and in_dim == out_dim
        self.pool = GlobalPool(pool_type='mean', keep_dim=False)
        if not self.use_identity:
            self.proj = nn.Parameter(scale * torch.randn(size=(in_dim, out_dim)))
        elif is_master(opts):
            logger.log(f'Using identity projection for GlobalPool2D given input/out size = {in_dim}.')
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.feature_normalization = not getattr(opts, 'model.image_projection_head.global_pool_nchw2nc.no_feature_normalization')
        self.reset_parameters()

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser') ->argparse.ArgumentParser:
        group = parser.add_argument_group(title=cls.__name__)
        group.add_argument('--model.image-projection-head.global-pool-nchw2nc.no-feature-normalization', action='store_true', help="Don't normalize image features. Defaults to False.")
        group.add_argument('--model.image-projection-head.global-pool-nchw2nc.identity-if-same-size', action='store_true', help='Use identity projection when projection input/output dims are the same. Defaults to False.')
        return parser

    def reset_parameters(self):
        pass

    def forward(self, x: 'Tensor', *args, **kwargs) ->Tensor:
        assert x.dim() == 4, 'Input should be 4-dimensional (Batch x in_dim x in_height x in_width). Got: {}'.format(x.shape)
        x = self.pool(x)
        if not self.use_identity:
            x = x @ self.proj
        if self.feature_normalization:
            x = F.normalize(x, dim=-1)
        return x


class SimpleImageProjectionHead(BaseImageProjectionHead):
    """This class implements simple projection head"""

    def __init__(self, opts, in_dim: 'int', out_dim: 'int', *args, **kwargs) ->None:
        super().__init__(opts, *args, **kwargs)
        scale = in_dim ** -0.5
        self.use_identity = getattr(opts, 'model.image_projection_head.simple_projection_nc2nc.identity_if_same_size') and in_dim == out_dim
        if not self.use_identity:
            self.proj = nn.Parameter(scale * torch.randn(size=(in_dim, out_dim)))
        elif is_master(opts):
            logger.log(f'Using identity projection for SimpleImageProjectionHead given input/out size = {in_dim}.')
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.feature_normalization = not getattr(opts, 'model.image_projection_head.simple_projection_nc2nc.no_feature_normalization')
        self.reset_parameters()

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser') ->argparse.ArgumentParser:
        group = parser.add_argument_group(title=cls.__name__)
        group.add_argument('--model.image-projection-head.simple-projection-nc2nc.no-feature-normalization', action='store_true', help="Don't normalize image features. Defaults to False.")
        group.add_argument('--model.image-projection-head.simple-projection-nc2nc.identity-if-same-size', action='store_true', help='Use identity projection when projection input/output dims are the same. Defaults to False')
        return parser

    def reset_parameters(self):
        pass

    def forward(self, x: 'Tensor', *args, **kwargs) ->Tensor:
        assert x.dim() == 2, 'Input should be 2-dimensional (Batch x in_dim). Got: {}'.format(x.shape)
        if not self.use_identity:
            x = x @ self.proj
        if self.feature_normalization:
            x = F.normalize(x, dim=-1)
        return x


class GELU(nn.GELU):
    """
    Applies the `Gaussian Error Linear Units <https://arxiv.org/abs/1606.08415>`_ function
    """

    def __init__(self, *args, **kwargs) ->None:
        super().__init__()


class Hardsigmoid(nn.Hardsigmoid):
    """
    Applies the `Hard Sigmoid <https://arxiv.org/abs/1511.00363v3>`_ function
    """

    def __init__(self, inplace: 'Optional[bool]'=False, *args, **kwargs) ->None:
        super().__init__(inplace=inplace)

    def forward(self, input: 'Tensor', *args, **kwargs) ->Tensor:
        if hasattr(F, 'hardsigmoid'):
            return F.hardsigmoid(input, self.inplace)
        else:
            return F.relu(input + 3) / 6


class Hardswish(nn.Hardswish):
    """
    Applies the HardSwish function, as described in the paper
    `Searching for MobileNetv3 <https://arxiv.org/abs/1905.02244>`_
    """

    def __init__(self, inplace: 'Optional[bool]'=False, *args, **kwargs) ->None:
        super().__init__(inplace=inplace)

    def forward(self, input: 'Tensor', *args, **kwargs) ->Tensor:
        if hasattr(F, 'hardswish'):
            return F.hardswish(input, self.inplace)
        else:
            x_hard_sig = F.relu(input + 3) / 6
            return input * x_hard_sig


class LeakyReLU(nn.LeakyReLU):
    """
    Applies a leaky relu function. See `Rectifier Nonlinearities Improve Neural Network Acoustic Models`
    for more details.
    """

    def __init__(self, negative_slope: 'Optional[float]'=0.01, inplace: 'Optional[bool]'=False, *args, **kwargs) ->None:
        super().__init__(negative_slope=negative_slope, inplace=inplace)


class PReLU(nn.PReLU):
    """
    Applies the `Parametric Rectified Linear Unit <https://arxiv.org/abs/1502.01852>`_ function
    """

    def __init__(self, num_parameters: 'Optional[int]'=1, init: 'Optional[float]'=0.25, *args, **kwargs) ->None:
        super().__init__(num_parameters=num_parameters, init=init)


class ReLU(nn.ReLU):
    """
    Applies Rectified Linear Unit function
    """

    def __init__(self, inplace: 'Optional[bool]'=False, *args, **kwargs) ->None:
        super().__init__(inplace=inplace)


class ReLU6(nn.ReLU6):
    """
    Applies the ReLU6 function
    """

    def __init__(self, inplace: 'Optional[bool]'=False, *args, **kwargs) ->None:
        super().__init__(inplace=inplace)


class Sigmoid(nn.Sigmoid):
    """
    Applies the sigmoid function
    """

    def __init__(self, *args, **kwargs) ->None:
        super().__init__()


class Swish(nn.SiLU):
    """
    Applies the `Swish (also known as SiLU) <https://arxiv.org/abs/1702.03118>`_ function.
    """

    def __init__(self, inplace: 'Optional[bool]'=False, *args, **kwargs) ->None:
        super().__init__(inplace=inplace)


class Tanh(nn.Tanh):
    """
    Applies Tanh function
    """

    def __init__(self, *args, **kwargs) ->None:
        super().__init__()


class AdaptiveAvgPool2d(nn.AdaptiveAvgPool2d):
    """
    Applies a 2D adaptive average pooling over an input tensor.

    Args:
        output_size (Optional, int or Tuple[int, int]): The target output size. If a single int :math:`h` is passed,
        then a square output of size :math:`hxh` is produced. If a tuple of size :math:`hxw` is passed, then an
        output of size `hxw` is produced. Default is 1.
    Shape:
        - Input: :math:`(N, C, H, W)` where :math:`N` is the batch size, :math:`C` is the number of input channels,
        :math:`H` is the input height, and :math:`W` is the input width
        - Output: :math:`(N, C, h, h)` or :math:`(N, C, h, w)`
    """

    def __init__(self, output_size: 'Union[int, Tuple[int, int]]'=1, *args, **kwargs) ->None:
        super().__init__(output_size=output_size)


class Conv2d(nn.Conv2d):
    """
    Applies a 2D convolution over an input.

    Args:
        in_channels: :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H_{in}, W_{in})`.
        out_channels: :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H_{out}, W_{out})`.
        kernel_size: Kernel size for convolution.
        stride: Stride for convolution. Default: 1.
        padding: Padding for convolution. Default: 0.
        dilation: Dilation rate for convolution. Default: 1.
        groups: Number of groups in convolution. Default: 1.
        bias: Use bias. Default: ``False``.
        padding_mode: Padding mode ('zeros', 'reflect', 'replicate' or 'circular'). Default: ``zeros``.
        use_norm: Use normalization layer after convolution. Default: ``True``.
        use_act: Use activation layer after convolution (or convolution and normalization).
                                Default: ``True``.
        act_name: Use specific activation function. Overrides the one specified in command line args.

    Shape:
        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`.
        - Output: :math:`(N, C_{out}, H_{out}, W_{out})`.
    """

    def __init__(self, in_channels: 'int', out_channels: 'int', kernel_size: 'Union[int, Tuple[int, int]]', stride: 'Optional[Union[int, Tuple[int, int]]]'=1, padding: 'Optional[Union[int, Tuple[int, int]]]'=0, dilation: 'Optional[Union[int, Tuple[int, int]]]'=1, groups: 'Optional[int]'=1, bias: 'Optional[bool]'=False, padding_mode: 'Optional[str]'='zeros', *args, **kwargs) ->None:
        super().__init__(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias, padding_mode=padding_mode)


class LayerNorm(nn.LayerNorm):
    """
    Applies `Layer Normalization <https://arxiv.org/abs/1607.06450>`_ over a input tensor

    Args:
        normalized_shape (int or list or torch.Size): input shape from an expected input
            of size

            .. math::
                [* \\times \\text{normalized\\_shape}[0] \\times \\text{normalized\\_shape}[1]
                    \\times \\ldots \\times \\text{normalized\\_shape}[-1]]

            If a single integer is used, it is treated as a singleton list, and this module will
            normalize over the last dimension which is expected to be of that specific size.
        eps (Optional, float): Value added to the denominator for numerical stability. Default: 1e-5
        elementwise_affine (bool): If ``True``, use learnable affine parameters. Default: ``True``

    Shape:
        - Input: :math:`(N, *)` where :math:`N` is the batch size
        - Output: same shape as the input
    """

    def __init__(self, normalized_shape: 'Union[int, List[int], Size]', eps: 'Optional[float]'=1e-05, elementwise_affine: 'Optional[bool]'=True, *args, **kwargs):
        super().__init__(normalized_shape=normalized_shape, eps=eps, elementwise_affine=elementwise_affine)

    def forward(self, x: 'Tensor') ->Tensor:
        n_dim = x.ndim
        if x.shape[1] == self.normalized_shape[0] and n_dim > 2:
            s, u = torch.std_mean(x, dim=1, keepdim=True, unbiased=False)
            x = (x - u) / (s + self.eps)
            if self.weight is not None:
                n_dim = x.ndim - 2
                new_shape = [1, self.normalized_shape[0]] + [1] * n_dim
                x = torch.addcmul(input=self.bias.reshape(*[new_shape]), value=1.0, tensor1=x, tensor2=self.weight.reshape(*[new_shape]))
            return x
        elif x.shape[-1] == self.normalized_shape[0]:
            return super().forward(x)
        else:
            raise NotImplementedError('LayerNorm is supported for channel-first and channel-last format only')


class LayerNorm2D_NCHW(nn.GroupNorm):
    """
    Applies `Layer Normalization <https://arxiv.org/abs/1607.06450>`_ over a 4D input tensor

    Args:
        num_features (int): :math:`C` from an expected input of size :math:`(N, C, H, W)`
        eps (Optional, float): Value added to the denominator for numerical stability. Default: 1e-5
        elementwise_affine (bool): If ``True``, use learnable affine parameters. Default: ``True``

    Shape:
        - Input: :math:`(N, C, H, W)` where :math:`N` is the batch size, :math:`C` is the number of input channels,
        :math:`H` is the input height, and :math:`W` is the input width
        - Output: same shape as the input
    """

    def __init__(self, num_features: 'int', eps: 'Optional[float]'=1e-05, elementwise_affine: 'Optional[bool]'=True, *args, **kwargs) ->None:
        super().__init__(num_channels=num_features, eps=eps, affine=elementwise_affine, num_groups=1)
        self.num_channels = num_features

    def __repr__(self):
        return '{}(num_channels={}, eps={}, affine={})'.format(self.__class__.__name__, self.num_channels, self.eps, self.affine)


ACT_FN_REGISTRY = {}


SUPPORTED_ACT_FNS = []


def build_activation_layer(opts: 'argparse.Namespace', act_type: 'Optional[str]'=None, inplace: 'Optional[bool]'=None, negative_slope: 'Optional[float]'=None, num_parameters: 'int'=-1) ->torch.nn.Module:
    """
    Helper function to build the activation function. If any of the optional
    arguments are not provided (i.e. None), the corresponding ``model.activation.*``
    config entry will be used as default value.

    Args:
        act_type: Name of the activation layer.
            Default: --model.activation.name config value.
        inplace: If true, operation will be inplace.
            Default: --model.activation.inplace config value.
        negative_slope: Negative slope parameter for leaky_relu.
            Default: --model.activation.neg_slop config value.
    """
    assert isinstance(opts, argparse.Namespace), f'Expected first argument to be an argparse.Namespace, but received a {type(opts)}.'
    if act_type is None:
        act_type = getattr(opts, 'model.activation.name')
    if inplace is None:
        inplace = getattr(opts, 'model.activation.inplace')
    if negative_slope is None:
        negative_slope = getattr(opts, 'model.activation.neg_slope')
    act_type = act_type.lower()
    act_layer = None
    if act_type in ACT_FN_REGISTRY:
        act_layer = ACT_FN_REGISTRY[act_type](num_parameters=num_parameters, inplace=inplace, negative_slope=negative_slope)
    else:
        logger.error('Supported activation layers are: {}. Supplied argument is: {}'.format(SUPPORTED_ACT_FNS, act_type))
    return act_layer


class Identity(nn.Module):

    def __init__(self, *args, **kwargs):
        super().__init__()

    def forward(self, x: 'Any') ->Any:
        return x


NORM_LAYER_REGISTRY = {}


SUPPORTED_NORM_FNS = []


def build_normalization_layer(opts: 'argparse.Namespace', num_features: 'int', norm_type: 'Optional[str]'=None, num_groups: 'Optional[int]'=None, momentum: 'Optional[float]'=None) ->torch.nn.Module:
    """
    Helper function to build the normalization layer.
    The function can be used in either of below mentioned ways:
    Scenario 1: Set the default normalization layers using command line arguments. This is useful when the same normalization
    layer is used for the entire network (e.g., ResNet).
    Scenario 2: Network uses different normalization layers. In that case, we can override the default normalization
    layer by specifying the name using `norm_type` argument.
    """
    if norm_type is None:
        norm_type = getattr(opts, 'model.normalization.name')
    if num_groups is None:
        num_groups = getattr(opts, 'model.normalization.groups')
    if momentum is None:
        momentum = getattr(opts, 'model.normalization.momentum')
    norm_layer = None
    norm_type = norm_type.lower()
    if norm_type in NORM_LAYER_REGISTRY:
        if 'cuda' not in str(getattr(opts, 'dev.device', 'cpu')) and 'sync_batch' in norm_type:
            norm_type = norm_type.replace('sync_', '')
        norm_layer = NORM_LAYER_REGISTRY[norm_type](normalized_shape=num_features, num_features=num_features, momentum=momentum, num_groups=num_groups)
    elif norm_type == 'identity':
        norm_layer = Identity()
    else:
        logger.error('Supported normalization layer arguments are: {}. Got: {}'.format(SUPPORTED_NORM_FNS, norm_type))
    return norm_layer


get_normalization_layer = build_normalization_layer


class _BaseConvNormActLayer(BaseLayer):
    """
    Applies an N-dimensional convolution over an input.

    Args:
        opts: Command line options.
        in_channels: :math:`C_{out}` from an expected output of size
            :math:`(bs, C_{in}, X_{1}, ..., X_{N})`.
        out_channels: :math:`C_{out}` from an expected output of size
            :math:`(bs, C_{out}, Y_{1}, ..., Y_{N})`.
        kernel_size: Kernel size for convolution. An integer, or tuple of length ``N``.
        stride: Stride for convolution. An integer, or tuple of length ``N``. Default: 1.
        dilation: Dilation rate for convolution. An integer, or tuple of length ``N``.
            Default: ``1``.
        padding: Padding for convolution. An integer, or tuple of length ``N``.
            If not specified, padding is automatically computed based on kernel size and
            dilation range. Default : ``None`` (equivalent to ``[
            int((kernel_size[i] - 1) / 2) * dilation[i] for i in range(N)]``).
        groups: Number of groups in convolution. Default: ``1``.
        bias: Use bias. Default: ``False``.
        padding_mode: Padding mode ('zeros', 'reflect', 'replicate' or 'circular').
            Default: ``zeros``.
        use_norm: Use normalization layer after convolution. Default: ``True``.
        use_act: Use activation layer after convolution (or convolution and normalization).
            Default: ``True``.
        norm_layer: If not None, the provided normalization layer object will be used.
            Otherwise, a normalization object will be created based on config
            ``model.normalization.*`` opts.
        act_layer: If not None, the provided activation function will be used.
            Otherwise, an activation function will be created based on config
            ``model.activation.*`` opts.

    Shape:
        - Input: :math:`(bs, C_{in}, X_{1}, ..., X_{N})`.
        - Output: :math:`(bs, C_{out}, Y_{1}, ..., Y_{N})`.

    .. note::
        For depth-wise convolution, `groups=C_{in}=C_{out}`.
    """

    @property
    def ndim(self) ->int:
        raise NotImplementedError('subclasses should override ndim property')

    @property
    def module_cls(self) ->Type[nn.Module]:
        raise NotImplementedError('subclasses should override module_cls property')

    def __init__(self, opts: 'argparse.Namespace', in_channels: 'int', out_channels: 'int', kernel_size: 'Union[int, Tuple[int, ...]]', stride: 'Union[int, Tuple[int, ...]]'=1, dilation: 'Union[int, Tuple[int, ...]]'=1, padding: 'Optional[Union[int, Tuple[int, ...]]]'=None, groups: 'int'=1, bias: 'bool'=False, padding_mode: 'str'='zeros', use_norm: 'bool'=True, use_act: 'bool'=True, norm_layer: 'Optional[nn.Module]'=None, act_layer: 'Optional[nn.Module]'=None, *args, **kwargs) ->None:
        super().__init__()
        if norm_layer is None and use_norm:
            norm_type = getattr(opts, 'model.normalization.name')
            if norm_type == 'batch_norm':
                norm_type = f'batch_norm_{self.ndim}d'
            norm_layer = get_normalization_layer(opts=opts, num_features=out_channels, norm_type=norm_type)
        elif norm_layer is not None and use_norm:
            logger.error(f'When use_norm is False, norm_layer should be None, but norm_layer={norm_layer} is provided.')
        if act_layer is None and use_act:
            act_layer = build_activation_layer(opts, num_parameters=out_channels)
        elif act_layer is not None and use_act:
            logger.error(f'When use_act is False, act_layer should be None, but act_layer={act_layer} is provided.')
        if use_norm and any(param[0] == 'bias' for param in norm_layer.named_parameters()) and bias:
            assert not bias, 'Do not use bias when using normalization layers with bias.'
        if use_norm and isinstance(norm_layer, (LayerNorm, LayerNorm2D_NCHW)):
            bias = True
        if isinstance(kernel_size, int):
            kernel_size = (kernel_size,) * self.ndim
        if isinstance(stride, int):
            stride = (stride,) * self.ndim
        if isinstance(dilation, int):
            dilation = (dilation,) * self.ndim
        assert isinstance(kernel_size, Tuple)
        assert isinstance(stride, Tuple)
        assert isinstance(dilation, Tuple)
        if padding is None:
            padding = (int((kernel_size[i] - 1) / 2) * dilation[i] for i in range(self.ndim))
        if in_channels % groups != 0:
            logger.error('Input channels are not divisible by groups. {}%{} != 0 '.format(in_channels, groups))
        if out_channels % groups != 0:
            logger.error('Output channels are not divisible by groups. {}%{} != 0 '.format(out_channels, groups))
        block = nn.Sequential()
        conv_layer = self.module_cls(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias, padding_mode=padding_mode)
        block.add_module(name='conv', module=conv_layer)
        self.norm_name = None
        if use_norm:
            block.add_module(name='norm', module=norm_layer)
            self.norm_name = norm_layer.__class__.__name__
        self.act_name = None
        if use_act:
            block.add_module(name='act', module=act_layer)
            self.act_name = act_layer.__class__.__name__
        self.block = block
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.stride = stride
        self.groups = groups
        self.kernel_size = conv_layer.kernel_size
        self.bias = bias
        self.dilation = dilation

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser'):
        if cls != _BaseConvNormActLayer:
            return parser
        group = parser.add_argument_group(cls.__name__)
        group.add_argument('--model.layer.conv-init', type=str, default='kaiming_normal', help='Init type for conv layers')
        parser.add_argument('--model.layer.conv-init-std-dev', type=float, default=None, help='Std deviation for conv layers')
        return parser

    def forward(self, x: 'Tensor') ->Tensor:
        return self.block(x)

    def __repr__(self):
        repr_str = self.block[0].__repr__()
        repr_str = repr_str[:-1]
        if self.norm_name is not None:
            repr_str += ', normalization={}'.format(self.norm_name)
        if self.act_name is not None:
            repr_str += ', activation={}'.format(self.act_name)
        repr_str += ')'
        return repr_str


class ConvLayer1d(_BaseConvNormActLayer):
    ndim = 1
    module_cls = nn.Conv1d


class ConvLayer2d(_BaseConvNormActLayer):
    ndim = 2
    module_cls = Conv2d


class ConvLayer3d(_BaseConvNormActLayer):
    ndim = 3
    module_cls = nn.Conv3d


class TransposeConvLayer2d(BaseLayer):
    """
    Applies a 2D Transpose convolution (aka as Deconvolution) over an input.

    Args:
        opts: Command line arguments.
        in_channels: :math:`C_{in}` from an expected input of size
          :math:`(N, C_{in}, H_{in}, W_{in})`.
        out_channels: :math:`C_{out}` from an expected output of size
          :math:`(N, C_{out}, H_{out}, W_{out})`.
        kernel_size: Kernel size for convolution.
        stride: Stride for convolution. Default: 1.
        dilation: Dilation rate for convolution. Default: 1.
        groups: Number of groups in convolution. Default: 1.
        bias: Use bias. Default: ``False``.
        padding_mode: Padding mode. Default: ``zeros``.
        use_norm: Use normalization layer after convolution. Default: ``True``.
        use_act: Use activation layer after convolution (or convolution and normalization).
        Default: ``True``.
        padding: Padding will be done on both sides of each dimension in the input.
        output_padding: Additional padding on the output tensor.
        auto_padding: Compute padding automatically. Default: ``True``.

    Shape:
        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`.
        - Output: :math:`(N, C_{out}, H_{out}, W_{out})`.
    """

    def __init__(self, opts: 'argparse.Namespace', in_channels: 'int', out_channels: 'int', kernel_size: 'Union[int, Tuple]', stride: 'Optional[Union[int, Tuple]]'=1, dilation: 'Optional[Union[int, Tuple]]'=1, groups: 'Optional[int]'=1, bias: 'Optional[bool]'=False, padding_mode: 'Optional[str]'='zeros', use_norm: 'Optional[bool]'=True, use_act: 'Optional[bool]'=True, padding: 'Optional[Union[int, Tuple]]'=(0, 0), output_padding: 'Optional[Union[int, Tuple]]'=None, auto_padding: 'Optional[bool]'=True, *args, **kwargs):
        super().__init__()
        if use_norm:
            assert not bias, 'Do not use bias when using normalization layers.'
        if isinstance(kernel_size, int):
            kernel_size = kernel_size, kernel_size
        if isinstance(stride, int):
            stride = stride, stride
        if isinstance(dilation, int):
            dilation = dilation, dilation
        if output_padding is None:
            output_padding = stride[0] - 1, stride[1] - 1
        assert isinstance(kernel_size, (tuple, list))
        assert isinstance(stride, (tuple, list))
        assert isinstance(dilation, (tuple, list))
        if auto_padding:
            padding = int((kernel_size[0] - 1) / 2) * dilation[0], int((kernel_size[1] - 1) / 2) * dilation[1]
        if in_channels % groups != 0:
            logger.error('Input channels are not divisible by groups. {}%{} != 0 '.format(in_channels, groups))
        if out_channels % groups != 0:
            logger.error('Output channels are not divisible by groups. {}%{} != 0 '.format(out_channels, groups))
        block = nn.Sequential()
        conv_layer = nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=bias, padding_mode=padding_mode, output_padding=output_padding)
        block.add_module(name='conv', module=conv_layer)
        self.norm_name = None
        if use_norm:
            norm_layer = get_normalization_layer(opts=opts, num_features=out_channels)
            block.add_module(name='norm', module=norm_layer)
            self.norm_name = norm_layer.__class__.__name__
        self.act_name = None
        act_type = getattr(opts, 'model.activation.name', 'relu')
        if act_type is not None and use_act:
            act_layer = build_activation_layer(opts, act_type=act_type, num_parameters=out_channels)
            block.add_module(name='act', module=act_layer)
            self.act_name = act_layer.__class__.__name__
        self.block = block
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.stride = stride
        self.groups = groups
        self.kernel_size = conv_layer.kernel_size
        self.bias = bias

    def forward(self, x: 'Tensor') ->Tensor:
        return self.block(x)

    def __repr__(self):
        repr_str = self.block[0].__repr__()
        repr_str = repr_str[:-1]
        if self.norm_name is not None:
            repr_str += ', normalization={}'.format(self.norm_name)
        if self.act_name is not None:
            repr_str += ', activation={}'.format(self.act_name)
        repr_str += ')'
        return repr_str


class NormActLayer(BaseLayer):
    """
    Applies a normalization layer followed by an activation layer.

    Args:
        opts: Command-line arguments.
        num_features: :math:`C` from an expected input of size :math:`(N, C, H, W)`.

    Shape:
        - Input: :math:`(N, C, H, W)`.
        - Output: :math:`(N, C, H, W)`.
    """

    def __init__(self, opts, num_features, *args, **kwargs):
        super().__init__()
        block = nn.Sequential()
        self.norm_name = None
        norm_layer = get_normalization_layer(opts=opts, num_features=num_features)
        block.add_module(name='norm', module=norm_layer)
        self.norm_name = norm_layer.__class__.__name__
        self.act_name = None
        act_layer = build_activation_layer(opts, num_parameters=num_features)
        block.add_module(name='act', module=act_layer)
        self.act_name = act_layer.__class__.__name__
        self.block = block

    def forward(self, x: 'Tensor') ->Tensor:
        return self.block(x)

    def __repr__(self):
        repr_str = '{}(normalization={}, activation={})'.format(self.__class__.__name__, self.norm_type, self.act_type)
        return repr_str


class _BaseSeparableConv(BaseLayer):
    """
    Applies an N-dimensional depth-wise separable convolution
        <https://arxiv.org/abs/1610.02357> over an N-dimensional input tensor.

    Args:
        opts: Command line arguments.
        in_channels: :math:`C_{in}` from an expected input of size
            :math:`(N, C_{in}, X_{1}, ..., X_{N})`.
        out_channels: :math:`C_{out}` from an expected output of size
            :math:`(N, C_{out}, Y_{1}, ..., Y_{N})`.
        kernel_size: Kernel size for convolution.
        stride: Stride for convolution. Default: 1.
        dilation: Dilation rate for convolution. Default: 1.
        use_norm: Use normalization layer after convolution. Default: ``True``.
        use_act: Use activation layer after convolution (or convolution and normalization).
            Default: ``True``.
        use_act_deptwise: Use activation layer after depthwise convolution (or
            convolution and normalization). Default: ``False``.
            NOTE: We recommend against using activation function in depth-wise convolution.
        bias: Use bias. Default: ``False``.
        padding_mode: Padding mode ('zeros', 'reflect', 'replicate' or 'circular').
            Default: ``zeros``.
        act_name: Use specific activation function. Overrides the one specified in
            command line args. Default: ``None``.

    Shape:
        - Input: :math:`(N, C_{in}, X_{1}, ..., X_{N})`.
        - Output: :math:`(N, C_{out}, Y_{1}, ..., Y_{N})`.

    .. note::
        For depth-wise convolution, `groups=C_{in}=C_{out}`.
    """

    def __init__(self, opts: 'argparse.Namespace', in_channels: 'int', out_channels: 'int', kernel_size: 'Union[int, Tuple[int, ...]]', stride: 'Union[int, Tuple[int, ...]]'=1, dilation: 'Union[int, Tuple[int, ...]]'=1, use_norm: 'bool'=True, use_act: 'bool'=True, use_act_depthwise: 'bool'=False, bias: 'bool'=False, padding_mode: 'str'='zeros', act_name: 'Optional[str]'=None, *args, **kwargs) ->None:
        super().__init__(*args, **kwargs)
        self.dw_conv = self.conv_layer_cls(opts=opts, in_channels=in_channels, out_channels=in_channels, kernel_size=kernel_size, stride=stride, dilation=dilation, groups=in_channels, bias=False, padding_mode=padding_mode, use_norm=True, use_act=use_act_depthwise, act_name=act_name)
        self.pw_conv = self.conv_layer_cls(opts=opts, in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, dilation=1, groups=1, bias=bias, padding_mode=padding_mode, use_norm=use_norm, use_act=use_act, act_name=act_name)
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.stride = stride
        self.kernel_size = kernel_size
        self.dilation = dilation

    @property
    def conv_layer_cls(self):
        raise NotImplementedError('Subclasses should override conv_layer_cls.')

    def __repr__(self):
        repr_str = '{}(in_channels={}, out_channels={}, kernel_size={}, stride={}, dilation={})'.format(self.__class__.__name__, self.in_channels, self.out_channels, self.kernel_size, self.stride, self.dilation)
        return repr_str

    def forward(self, x: 'Tensor') ->Tensor:
        x = self.dw_conv(x)
        x = self.pw_conv(x)
        return x


class SeparableConv1d(_BaseSeparableConv):
    conv_layer_cls = ConvLayer1d


class SeparableConv2d(_BaseSeparableConv):
    conv_layer_cls = ConvLayer2d


class SeparableConv3d(_BaseSeparableConv):
    conv_layer_cls = ConvLayer3d


class Dropout(nn.Dropout):
    """
    This layer, during training, randomly zeroes some of the elements of the input tensor with probability `p`
    using samples from a Bernoulli distribution.

    Args:
        p: probability of an element to be zeroed. Default: 0.5
        inplace: If set to ``True``, will do this operation in-place. Default: ``False``

    Shape:
        - Input: :math:`(N, *)` where :math:`N` is the batch size
        - Output: same as the input

    """

    def __init__(self, p: 'Optional[float]'=0.5, inplace: 'Optional[bool]'=False, *args, **kwargs) ->None:
        super().__init__(p=p, inplace=inplace)


class Dropout2d(nn.Dropout2d):
    """
    This layer, during training, randomly zeroes some of the elements of the 4D input tensor with probability `p`
    using samples from a Bernoulli distribution.

    Args:
        p: probability of an element to be zeroed. Default: 0.5
        inplace: If set to ``True``, will do this operation in-place. Default: ``False``

    Shape:
        - Input: :math:`(N, C, H, W)` where :math:`N` is the batch size, :math:`C` is the input channels,
            :math:`H` is the input tensor height, and :math:`W` is the input tensor width
        - Output: same as the input

    """

    def __init__(self, p: 'float'=0.5, inplace: 'bool'=False):
        super().__init__(p=p, inplace=inplace)


class Embedding(nn.Embedding):
    """A lookup table that stores embeddings of a fixed dictionary and size.

    Args:
        num_embeddings (int): size of the dictionary of embeddings
        embedding_dim (int): the size of each embedding vector
        padding_idx (int, optional): If specified, the entries at :attr:`padding_idx` do not contribute to the gradient;
                                     therefore, the embedding vector at :attr:`padding_idx` is not updated during training,
                                     i.e. it remains as a fixed "pad". For a newly constructed Embedding,
                                     the embedding vector at :attr:`padding_idx` will default to all zeros,
                                     but can be updated to another value to be used as the padding vector.

    Shape:
        - Input: :math:`(*)`, IntTensor or LongTensor of arbitrary shape containing the indices to extract
        - Output: :math:`(*, H)`, where `*` is the input shape and :math:`H=\\text{embedding\\_dim}`
    """

    def __init__(self, opts, num_embeddings: 'int', embedding_dim: 'int', padding_idx: 'Optional[int]'=None, *args, **kwargs):
        super().__init__(num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=padding_idx)

    def reset_parameters(self) ->None:
        nn.init.normal_(self.weight, mean=0, std=self.embedding_dim ** -0.5)
        if self.padding_idx is not None:
            nn.init.constant_(self.weight[self.padding_idx], 0)


class Flatten(nn.Flatten):
    """
    This layer flattens a contiguous range of dimensions into a tensor.

    Args:
        start_dim (Optional[int]): first dim to flatten. Default: 1
        end_dim (Optional[int]): last dim to flatten. Default: -1

    Shape:
        - Input: :math:`(*, S_{\\text{start}},..., S_{i}, ..., S_{\\text{end}}, *)`,'
          where :math:`S_{i}` is the size at dimension :math:`i` and :math:`*` means any
          number of dimensions including none.
        - Output: :math:`(*, \\prod_{i=\\text{start}}^{\\text{end}} S_{i}, *)`.
    """

    def __init__(self, start_dim: 'Optional[int]'=1, end_dim: 'Optional[int]'=-1):
        super(Flatten, self).__init__(start_dim=start_dim, end_dim=end_dim)


class LinearSelfAttention(BaseLayer):
    """
    This layer applies a self-attention with linear complexity, as described in `MobileViTv2 <https://arxiv.org/abs/2206.02680>`_ paper.
    This layer can be used for self- as well as cross-attention.

    Args:
        opts: command line arguments
        embed_dim (int): :math:`C` from an expected input of size :math:`(N, C, H, W)`
        attn_dropout (Optional[float]): Dropout value for context scores. Default: 0.0
        bias (Optional[bool]): Use bias in learnable layers. Default: True

    Shape:
        - Input: :math:`(N, C, P, N)` where :math:`N` is the batch size, :math:`C` is the input channels,
        :math:`P` is the number of pixels in the patch, and :math:`N` is the number of patches
        - Output: same as the input

    .. note::
        For MobileViTv2, we unfold the feature map [B, C, H, W] into [B, C, P, N] where P is the number of pixels
        in a patch and N is the number of patches. Because channel is the first dimension in this unfolded tensor,
        we use point-wise convolution (instead of a linear layer). This avoids a transpose operation (which may be
        expensive on resource-constrained devices) that may be required to convert the unfolded tensor from
        channel-first to channel-last format in case of a linear layer.
    """

    def __init__(self, opts, embed_dim: 'int', attn_dropout: 'Optional[float]'=0.0, bias: 'Optional[bool]'=True, *args, **kwargs) ->None:
        super().__init__()
        self.qkv_proj = ConvLayer2d(opts=opts, in_channels=embed_dim, out_channels=1 + 2 * embed_dim, bias=bias, kernel_size=1, use_norm=False, use_act=False)
        self.attn_dropout = Dropout(p=attn_dropout)
        self.out_proj = ConvLayer2d(opts=opts, in_channels=embed_dim, out_channels=embed_dim, bias=bias, kernel_size=1, use_norm=False, use_act=False)
        self.embed_dim = embed_dim

    def __repr__(self):
        return '{}(embed_dim={}, attn_dropout={})'.format(self.__class__.__name__, self.embed_dim, self.attn_dropout.p)

    @staticmethod
    def visualize_context_scores(context_scores):
        batch_size, channels, num_pixels, num_patches = context_scores.shape
        assert batch_size == 1, 'For visualization purposes, use batch size of 1'
        assert channels == 1, 'The inner-product between input and latent node (query) is a scalar'
        up_scale_factor = int(num_pixels ** 0.5)
        patch_h = patch_w = int(context_scores.shape[-1] ** 0.5)
        context_scores = context_scores.reshape(1, num_pixels, patch_h, patch_w)
        context_map = F.pixel_shuffle(context_scores, upscale_factor=up_scale_factor)
        context_map = context_map.squeeze()
        min_val = torch.min(context_map)
        max_val = torch.max(context_map)
        context_map = (context_map - min_val) / (max_val - min_val)
        try:
            context_map = (context_map * 255).byte().cpu().numpy()
            context_map = cv2.resize(context_map, (80, 80), interpolation=cv2.INTER_NEAREST)
            colored_context_map = cv2.applyColorMap(context_map, cv2.COLORMAP_JET)
            res_dir_name = 'attn_res'
            if not os.path.isdir(res_dir_name):
                os.makedirs(res_dir_name)
            f_name = '{}/h_{}_w_{}_index_'.format(res_dir_name, patch_h, patch_w)
            files_cmap = glob('{}/h_{}_w_{}_index_*.png'.format(res_dir_name, patch_h, patch_w))
            idx = len(files_cmap)
            f_name += str(idx)
            cv2.imwrite('{}.png'.format(f_name), colored_context_map)
            return colored_context_map
        except ModuleNotFoundError as mnfe:
            None
            return context_map

    def _forward_self_attn(self, x: 'Tensor', *args, **kwargs) ->Tensor:
        qkv = self.qkv_proj(x)
        query, key, value = torch.split(qkv, split_size_or_sections=[1, self.embed_dim, self.embed_dim], dim=1)
        context_scores = F.softmax(query, dim=-1)
        context_scores = self.attn_dropout(context_scores)
        context_vector = key * context_scores
        context_vector = torch.sum(context_vector, dim=-1, keepdim=True)
        out = F.relu(value) * context_vector.expand_as(value)
        out = self.out_proj(out)
        return out

    def _forward_cross_attn(self, x: 'Tensor', x_prev: 'Optional[Tensor]'=None, *args, **kwargs) ->Tensor:
        batch_size, in_dim, kv_patch_area, kv_num_patches = x.shape
        q_patch_area, q_num_patches = x.shape[-2:]
        assert kv_patch_area == q_patch_area, 'The number of pixels in a patch for query and key_value should be the same'
        qk = F.conv2d(x_prev, weight=self.qkv_proj.block.conv.weight[:self.embed_dim + 1, ...], bias=self.qkv_proj.block.conv.bias[:self.embed_dim + 1, ...])
        query, key = torch.split(qk, split_size_or_sections=[1, self.embed_dim], dim=1)
        value = F.conv2d(x, weight=self.qkv_proj.block.conv.weight[self.embed_dim + 1:, ...], bias=self.qkv_proj.block.conv.bias[self.embed_dim + 1:, ...])
        context_scores = F.softmax(query, dim=-1)
        context_scores = self.attn_dropout(context_scores)
        context_vector = key * context_scores
        context_vector = torch.sum(context_vector, dim=-1, keepdim=True)
        out = F.relu(value) * context_vector.expand_as(value)
        out = self.out_proj(out)
        return out

    def forward(self, x: 'Tensor', x_prev: 'Optional[Tensor]'=None, *args, **kwargs) ->Tensor:
        if x_prev is None:
            return self._forward_self_attn(x, *args, **kwargs)
        else:
            return self._forward_cross_attn(x, *args, x_prev=x_prev, **kwargs)


class LinearLayer(BaseLayer):
    """
    Applies a linear transformation to the input data

    Args:
        in_features (int): number of features in the input tensor
        out_features (int): number of features in the output tensor
        bias  (Optional[bool]): use bias or not
        channel_first (Optional[bool]): Channels are first or last dimension. If first, then use Conv2d

    Shape:
        - Input: :math:`(N, *, C_{in})` if not channel_first else :math:`(N, C_{in}, *)` where :math:`*` means any number of dimensions.
        - Output: :math:`(N, *, C_{out})` if not channel_first else :math:`(N, C_{out}, *)`

    """

    def __init__(self, in_features: 'int', out_features: 'int', bias: 'Optional[bool]'=True, channel_first: 'Optional[bool]'=False, *args, **kwargs) ->None:
        super().__init__()
        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))
        self.bias = nn.Parameter(torch.Tensor(out_features)) if bias else None
        self.in_features = in_features
        self.out_features = out_features
        self.channel_first = channel_first
        self.reset_params()

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser'):
        parser.add_argument('--model.layer.linear-init', type=str, default='xavier_uniform', help='Init type for linear layers')
        parser.add_argument('--model.layer.linear-init-std-dev', type=float, default=0.01, help='Std deviation for Linear layers')
        return parser

    def reset_params(self):
        if self.weight is not None:
            torch.nn.init.xavier_uniform_(self.weight)
        if self.bias is not None:
            torch.nn.init.constant_(self.bias, 0)

    def forward(self, x: 'Tensor') ->Tensor:
        if self.channel_first:
            if not self.training:
                logger.error('Channel-first mode is only supported during inference')
            if x.dim() != 4:
                logger.error('Input should be 4D, i.e., (B, C, H, W) format')
            with torch.no_grad():
                return F.conv2d(input=x, weight=self.weight.clone().detach().reshape(self.out_features, self.in_features, 1, 1), bias=self.bias)
        else:
            x = F.linear(x, weight=self.weight, bias=self.bias)
        return x

    def __repr__(self):
        repr_str = '{}(in_features={}, out_features={}, bias={}, channel_first={})'.format(self.__class__.__name__, self.in_features, self.out_features, True if self.bias is not None else False, self.channel_first)
        return repr_str


class GroupLinear(BaseLayer):
    """
    Applies a GroupLinear transformation layer, as defined `here <https://arxiv.org/abs/1808.09029>`_,
    `here <https://arxiv.org/abs/1911.12385>`_ and `here <https://arxiv.org/abs/2008.00623>`_

    Args:
        in_features (int): number of features in the input tensor
        out_features (int): number of features in the output tensor
        n_groups (int): number of groups
        bias (Optional[bool]): use bias or not
        feature_shuffle (Optional[bool]): Shuffle features between groups

    Shape:
        - Input: :math:`(N, *, C_{in})`
        - Output: :math:`(N, *, C_{out})`

    """

    def __init__(self, in_features: 'int', out_features: 'int', n_groups: 'int', bias: 'Optional[bool]'=True, feature_shuffle: 'Optional[bool]'=False, *args, **kwargs) ->None:
        if in_features % n_groups != 0:
            logger.error('Input dimensions ({}) must be divisible by n_groups ({})'.format(in_features, n_groups))
        if out_features % n_groups != 0:
            logger.error('Output dimensions ({}) must be divisible by n_groups ({})'.format(out_features, n_groups))
        in_groups = in_features // n_groups
        out_groups = out_features // n_groups
        super().__init__()
        self.weight = nn.Parameter(torch.Tensor(n_groups, in_groups, out_groups))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(n_groups, 1, out_groups))
        else:
            self.bias = None
        self.out_features = out_features
        self.in_features = in_features
        self.n_groups = n_groups
        self.feature_shuffle = feature_shuffle
        self.reset_params()

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser'):
        parser.add_argument('--model.layer.group-linear-init', type=str, default='xavier_uniform', help='Init type for group linear layers')
        parser.add_argument('--model.layer.group-linear-init-std-dev', type=float, default=0.01, help='Std deviation for group linear layers')
        return parser

    def reset_params(self):
        if self.weight is not None:
            torch.nn.init.xavier_uniform_(self.weight.data)
        if self.bias is not None:
            torch.nn.init.constant_(self.bias.data, 0)

    def _forward(self, x: 'Tensor') ->Tensor:
        bsz = x.shape[0]
        x = x.reshape(bsz, self.n_groups, -1)
        x = x.transpose(0, 1)
        x = torch.bmm(x, self.weight)
        if self.bias is not None:
            x = torch.add(x, self.bias)
        if self.feature_shuffle:
            x = x.permute(1, 2, 0)
            x = x.reshape(bsz, self.n_groups, -1)
        else:
            x = x.transpose(0, 1)
        return x.reshape(bsz, -1)

    def forward(self, x: 'Tensor') ->Tensor:
        if x.dim() == 2:
            x = self._forward(x)
            return x
        else:
            in_dims = x.shape[:-1]
            n_elements = x.numel() // self.in_features
            x = x.reshape(n_elements, -1)
            x = self._forward(x)
            x = x.reshape(*in_dims, -1)
            return x

    def __repr__(self):
        repr_str = '{}(in_features={}, out_features={}, groups={}, bias={}, shuffle={})'.format(self.__class__.__name__, self.in_features, self.out_features, self.n_groups, True if self.bias is not None else False, self.feature_shuffle)
        return repr_str


class MultiHeadAttention(BaseLayer):
    """
    This layer applies a multi-head self- or cross-attention as described in
    `Attention is all you need <https://arxiv.org/abs/1706.03762>`_ paper

    Args:
        embed_dim (int): :math:`C_{in}` from an expected input of size :math:`(N, S, C_{in})`
        num_heads (int): Number of heads in multi-head attention
        attn_dropout (Optional[float]): Attention dropout. Default: 0.0
        bias (Optional[bool]): Use bias or not. Default: ``True``

    Shape:
        - Input:
           - Query tensor (x_q) :math:`(N, S, C_{in})` where :math:`N` is batch size, :math:`S` is number of source tokens,
        and :math:`C_{in}` is input embedding dim
           - Optional Key-Value tensor (x_kv) :math:`(N, T, C_{in})` where :math:`T` is number of target tokens
        - Output: same shape as the input

    """

    def __init__(self, embed_dim: 'int', num_heads: 'int', attn_dropout: 'Optional[float]'=0.0, bias: 'Optional[bool]'=True, output_dim: 'Optional[int]'=None, coreml_compatible: 'Optional[bool]'=False, *args, **kwargs) ->None:
        if output_dim is None:
            output_dim = embed_dim
        super().__init__()
        if embed_dim % num_heads != 0:
            logger.error('Embedding dim must be divisible by number of heads in {}. Got: embed_dim={} and num_heads={}'.format(self.__class__.__name__, embed_dim, num_heads))
        self.qkv_proj = LinearLayer(in_features=embed_dim, out_features=3 * embed_dim, bias=bias)
        self.attn_dropout = Dropout(p=attn_dropout)
        self.out_proj = LinearLayer(in_features=embed_dim, out_features=output_dim, bias=bias)
        self.head_dim = embed_dim // num_heads
        self.scaling = self.head_dim ** -0.5
        self.softmax = nn.Softmax(dim=-1)
        self.num_heads = num_heads
        self.embed_dim = embed_dim
        self.coreml_compatible = coreml_compatible
        self.use_separate_proj_weight = embed_dim != output_dim

    def __repr__(self):
        return '{}(head_dim={}, num_heads={}, attn_dropout={})'.format(self.__class__.__name__, self.head_dim, self.num_heads, self.attn_dropout.p)

    def forward_tracing(self, x_q: 'Tensor', x_kv: 'Optional[Tensor]'=None, key_padding_mask: 'Optional[Tensor]'=None, attn_mask: 'Optional[Tensor]'=None) ->Tensor:
        if x_kv is None:
            qkv = self.qkv_proj(x_q)
            query, key, value = torch.chunk(qkv, chunks=3, dim=-1)
        else:
            query = F.linear(x_q, weight=self.qkv_proj.weight[:self.embed_dim, ...], bias=self.qkv_proj.bias[:self.embed_dim] if self.qkv_proj.bias is not None else None)
            kv = F.linear(x_kv, weight=self.qkv_proj.weight[self.embed_dim:, ...], bias=self.qkv_proj.bias[self.embed_dim:] if self.qkv_proj.bias is not None else None)
            key, value = torch.chunk(kv, chunks=2, dim=-1)
        query = query * self.scaling
        query = torch.chunk(query, chunks=self.num_heads, dim=-1)
        value = torch.chunk(value, chunks=self.num_heads, dim=-1)
        key = torch.chunk(key, chunks=self.num_heads, dim=-1)
        wt_out = []
        for h in range(self.num_heads):
            attn_h = torch.matmul(query[h], key[h].transpose(-1, -2))
            attn_h = self.softmax(attn_h)
            attn_h = self.attn_dropout(attn_h)
            out_h = torch.matmul(attn_h, value[h])
            wt_out.append(out_h)
        wt_out = torch.cat(wt_out, dim=-1)
        wt_out = self.out_proj(wt_out)
        return wt_out

    def forward_default(self, x_q: 'Tensor', x_kv: 'Optional[Tensor]'=None, key_padding_mask: 'Optional[Tensor]'=None, attn_mask: 'Optional[Tensor]'=None) ->Tensor:
        b_sz, S_len, in_channels = x_q.shape
        if x_kv is None:
            qkv = self.qkv_proj(x_q).reshape(b_sz, S_len, 3, self.num_heads, -1)
            qkv = qkv.transpose(1, 3).contiguous()
            query, key, value = qkv[:, :, 0], qkv[:, :, 1], qkv[:, :, 2]
        else:
            T_len = x_kv.shape[1]
            query = F.linear(x_q, weight=self.qkv_proj.weight[:self.embed_dim, ...], bias=self.qkv_proj.bias[:self.embed_dim] if self.qkv_proj.bias is not None else None)
            query = query.reshape(b_sz, S_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()
            kv = F.linear(x_kv, weight=self.qkv_proj.weight[self.embed_dim:, ...], bias=self.qkv_proj.bias[self.embed_dim:] if self.qkv_proj.bias is not None else None)
            kv = kv.reshape(b_sz, T_len, 2, self.num_heads, self.head_dim)
            kv = kv.transpose(1, 3).contiguous()
            key, value = kv[:, :, 0], kv[:, :, 1]
        query = query * self.scaling
        key = key.transpose(-1, -2)
        attn = torch.matmul(query, key)
        batch_size, num_heads, num_src_tokens, num_tgt_tokens = attn.shape
        if attn_mask is not None:
            assert list(attn_mask.shape) == [batch_size, num_src_tokens, num_tgt_tokens], 'Shape of attention mask should be [{}, {}, {}]. Got: {}'.format(batch_size, num_src_tokens, num_tgt_tokens, attn_mask.shape)
            attn_mask = attn_mask.unsqueeze(1)
            attn = attn + attn_mask
        if key_padding_mask is not None:
            assert key_padding_mask.dim() == 2 and list(key_padding_mask.shape) == [batch_size, num_tgt_tokens], 'Key_padding_mask should be 2-dimension with shape [{}, {}]. Got: {}'.format(batch_size, num_tgt_tokens, key_padding_mask.shape)
            attn = attn.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2), float('-inf'))
        attn_dtype = attn.dtype
        attn_as_float = self.softmax(attn.float())
        attn = attn_as_float
        attn = self.attn_dropout(attn)
        out = torch.matmul(attn, value)
        out = out.transpose(1, 2).reshape(b_sz, S_len, -1)
        out = self.out_proj(out)
        return out

    def forward_pytorch(self, x_q: 'Tensor', x_kv: 'Optional[Tensor]'=None, key_padding_mask: 'Optional[Tensor]'=None, attn_mask: 'Optional[Tensor]'=None) ->Tensor:
        out, _ = F.multi_head_attention_forward(query=x_q, key=x_kv if x_kv is not None else x_q, value=x_kv if x_kv is not None else x_q, embed_dim_to_check=self.embed_dim, num_heads=self.num_heads, in_proj_weight=torch.empty([0]), in_proj_bias=self.qkv_proj.bias, bias_k=None, bias_v=None, add_zero_attn=False, dropout_p=self.attn_dropout.p, out_proj_weight=self.out_proj.weight, out_proj_bias=self.out_proj.bias, training=self.training, key_padding_mask=key_padding_mask, need_weights=False, attn_mask=attn_mask, use_separate_proj_weight=True, q_proj_weight=self.qkv_proj.weight[:self.embed_dim, ...], k_proj_weight=self.qkv_proj.weight[self.embed_dim:2 * self.embed_dim, ...], v_proj_weight=self.qkv_proj.weight[2 * self.embed_dim:, ...])
        return out

    def forward(self, x_q: 'Tensor', x_kv: 'Optional[Tensor]'=None, key_padding_mask: 'Optional[Tensor]'=None, attn_mask: 'Optional[Tensor]'=None, *args, **kwargs) ->Tensor:
        if self.coreml_compatible:
            return self.forward_tracing(x_q=x_q, x_kv=x_kv, key_padding_mask=key_padding_mask, attn_mask=attn_mask)
        elif kwargs.get('use_pytorch_mha', False):
            return self.forward_pytorch(x_q=x_q, x_kv=x_kv, key_padding_mask=key_padding_mask, attn_mask=attn_mask)
        else:
            return self.forward_default(x_q=x_q, x_kv=x_kv, key_padding_mask=key_padding_mask, attn_mask=attn_mask)


class BatchNorm2d(nn.BatchNorm2d):
    """
    Applies a `Batch Normalization <https://arxiv.org/abs/1502.03167>`_ over a 4D input tensor

    Args:
        num_features (Optional, int): :math:`C` from an expected input of size :math:`(N, C, H, W)`
        eps (Optional, float): Value added to the denominator for numerical stability. Default: 1e-5
        momentum (Optional, float): Value used for the running_mean and running_var computation. Default: 0.1
        affine (bool): If ``True``, use learnable affine parameters. Default: ``True``
        track_running_stats: If ``True``, tracks running mean and variance. Default: ``True``

    Shape:
        - Input: :math:`(N, C, H, W)` where :math:`N` is the batch size, :math:`C` is the number of input channels,
        :math:`H` is the input height, and :math:`W` is the input width
        - Output: same shape as the input
    """

    def __init__(self, num_features: 'int', eps: 'Optional[float]'=1e-05, momentum: 'Optional[float]'=0.1, affine: 'Optional[bool]'=True, track_running_stats: 'Optional[bool]'=True, *args, **kwargs) ->None:
        super().__init__(num_features=num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats)


class BatchNorm2dFP32(BatchNorm2d):
    """
    Applies a `Batch Normalization <https://arxiv.org/abs/1502.03167>`_ over a 4D input tensor in FP32
    """

    def __init__(self, num_features: 'int', eps: 'Optional[float]'=1e-05, momentum: 'Optional[float]'=0.1, affine: 'Optional[bool]'=True, track_running_stats: 'Optional[bool]'=True, *args, **kwargs) ->None:
        super().__init__(*args, num_features=num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats, **kwargs)

    def forward(self, input: 'Tensor') ->Tensor:
        inp_dtype = input.dtype
        return super().forward(input.to(torch.float32))


class BatchNorm1d(nn.BatchNorm1d):
    """
    Applies a `Batch Normalization <https://arxiv.org/abs/1502.03167>`_ over a 2D or 3D input tensor

    Args:
        num_features (Optional, int): :math:`C` from an expected input of size :math:`(N, C)` or :math:`(N, C, L)`
        eps (Optional, float): Value added to the denominator for numerical stability. Default: 1e-5
        momentum (Optional, float): Value used for the running_mean and running_var computation. Default: 0.1
        affine (bool): If ``True``, use learnable affine parameters. Default: ``True``
        track_running_stats: If ``True``, tracks running mean and variance. Default: ``True``

    Shape:
        - Input: :math:`(N, C)` or :math:`(N, C, L)` where :math:`N` is the batch size,
        :math:`C` is the number of input channels,  and :math:`L` is the sequence length
        - Output: same shape as the input
    """

    def __init__(self, num_features: 'int', eps: 'Optional[float]'=1e-05, momentum: 'Optional[float]'=0.1, affine: 'Optional[bool]'=True, track_running_stats: 'Optional[bool]'=True, *args, **kwargs) ->None:
        super().__init__(num_features=num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats)


class BatchNorm3d(nn.BatchNorm3d):

    def __init__(self, num_features: 'int', eps: 'Optional[float]'=1e-05, momentum: 'Optional[float]'=0.1, affine: 'Optional[bool]'=True, track_running_stats: 'Optional[bool]'=True, *args, **kwargs) ->None:
        """
        Applies a `Batch Normalization <https://arxiv.org/abs/1502.03167>`_ over a 5D input tensor

        Args:
            num_features (Optional, int): :math:`C` from an expected input of size :math:`(N, C, D, H, W)`
            eps (Optional, float): Value added to the denominator for numerical stability. Default: 1e-5
            momentum (Optional, float): Value used for the running_mean and running_var computation. Default: 0.1
            affine (bool): If ``True``, use learnable affine parameters. Default: ``True``
            track_running_stats: If ``True``, tracks running mean and variance. Default: ``True``

        Shape:
            - Input: :math:`(N, C, D, H, W)` where :math:`N` is the batch size, :math:`C` is the number of input
            channels, :math:`D` is the input depth, :math:`H` is the input height, and :math:`W` is the input width
            - Output: same shape as the input
        """
        super().__init__(num_features=num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats)


class GroupNorm(nn.GroupNorm):
    """
    Applies a `Group Normalization <https://arxiv.org/abs/1803.08494>`_ over an input tensor

    Args:
        num_groups (int): number of groups to separate the input channels into
        num_features (int): :math:`C` from an expected input of size :math:`(N, C, *)`
        eps (Optional, float): Value added to the denominator for numerical stability. Default: 1e-5
        affine (bool): If ``True``, use learnable affine parameters. Default: ``True``

    Shape:
        - Input: :math:`(N, C, *)` where :math:`N` is the batch size, :math:`C` is the number of input channels,
        and :math:`*` is the remaining dimensions of the input tensor
        - Output: same shape as the input

    .. note::
        GroupNorm is the same as LayerNorm when `num_groups=1` and it is the same as InstanceNorm when
        `num_groups=C`.
    """

    def __init__(self, num_groups: 'int', num_features: 'int', eps: 'Optional[float]'=1e-05, affine: 'Optional[bool]'=True, *args, **kwargs) ->None:
        super().__init__(num_groups=num_groups, num_channels=num_features, eps=eps, affine=affine)


class InstanceNorm2d(nn.InstanceNorm2d):
    """
    Applies a `Instance Normalization <https://arxiv.org/abs/1607.08022>`_ over a 4D input tensor

    Args:
        num_features (int): :math:`C` from an expected input of size :math:`(N, C, H, W)`
        eps (Optional, float): Value added to the denominator for numerical stability. Default: 1e-5
        momentum (Optional, float): Value used for the running_mean and running_var computation. Default: 0.1
        affine (bool): If ``True``, use learnable affine parameters. Default: ``True``
        track_running_stats: If ``True``, tracks running mean and variance. Default: ``True``

    Shape:
        - Input: :math:`(N, C, H, W)` where :math:`N` is the batch size, :math:`C` is the number of input channels,
        :math:`H` is the input height, and :math:`W` is the input width
        - Output: same shape as the input
    """

    def __init__(self, num_features: 'int', eps: 'Optional[float]'=1e-05, momentum: 'Optional[float]'=0.1, affine: 'Optional[bool]'=True, track_running_stats: 'Optional[bool]'=True, *args, **kwargs) ->None:
        super().__init__(num_features=num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats)


class InstanceNorm1d(nn.InstanceNorm1d):
    """
    Applies a `Instance Normalization <https://arxiv.org/abs/1607.08022>`_ over a 2D or 3D input tensor

    Args:
        num_features (int): :math:`C` from an expected input of size :math:`(N, C)` or :math:`(N, C, L)`
        eps (Optional, float): Value added to the denominator for numerical stability. Default: 1e-5
        momentum (Optional, float): Value used for the running_mean and running_var computation. Default: 0.1
        affine (bool): If ``True``, use learnable affine parameters. Default: ``True``
        track_running_stats: If ``True``, tracks running mean and variance. Default: ``True``

    Shape:
        - Input: :math:`(N, C)` or :math:`(N, C, L)` where :math:`N` is the batch size, :math:`C` is the number
        of input channels,  and :math:`L` is the sequence length
    - Output: same shape as the input
    """

    def __init__(self, num_features: 'int', eps: 'Optional[float]'=1e-05, momentum: 'Optional[float]'=0.1, affine: 'Optional[bool]'=True, track_running_stats: 'Optional[bool]'=True, *args, **kwargs) ->None:
        super().__init__(num_features=num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats)


class LayerNormFP32(LayerNorm):
    """
    Applies `Layer Normalization <https://arxiv.org/abs/1607.06450>`_ over a input tensor with FP32 precision
    """

    def __init__(self, normalized_shape: 'Union[int, List[int], Size]', eps: 'Optional[float]'=1e-05, elementwise_affine: 'Optional[bool]'=True, *args, **kwargs):
        super().__init__(*args, normalized_shape=normalized_shape, eps=eps, elementwise_affine=elementwise_affine, **kwargs)

    def forward(self, x: 'Tensor') ->Tensor:
        inp_dtype = x.dtype
        return super().forward(x.to(torch.float32))


class SyncBatchNorm(nn.SyncBatchNorm):
    """
    Applies a `Syncronized Batch Normalization <https://arxiv.org/abs/1502.03167>`_ over the input tensor

    Args:
        num_features (Optional, int): :math:`C` from an expected input of size :math:`(N, C, *)`
        eps (Optional, float): Value added to the denominator for numerical stability. Default: 1e-5
        momentum (Optional, float): Value used for the running_mean and running_var computation. Default: 0.1
        affine (bool): If ``True``, use learnable affine parameters. Default: ``True``
        track_running_stats: If ``True``, tracks running mean and variance. Default: ``True``

    Shape:
        - Input: :math:`(N, C, *)` where :math:`N` is the batch size, :math:`C` is the number of input channels,
        :math:`*` is the remaining input dimensions
        - Output: same shape as the input

    """

    def __init__(self, num_features: 'int', eps: 'Optional[float]'=1e-05, momentum: 'Optional[float]'=0.1, affine: 'Optional[bool]'=True, track_running_stats: 'Optional[bool]'=True, *args, **kwargs) ->None:
        super().__init__(num_features=num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats)


class SyncBatchNormFP32(SyncBatchNorm):
    """
    Synchronized BN in FP32
    """

    def __init__(self, num_features: 'int', eps: 'Optional[float]'=1e-05, momentum: 'Optional[float]'=0.1, affine: 'Optional[bool]'=True, track_running_stats: 'Optional[bool]'=True, *args, **kwargs) ->None:
        super().__init__(num_features=num_features, eps=eps, momentum=momentum, affine=affine, track_running_stats=track_running_stats)

    def forward(self, x: 'Tensor', *args, **kwargs) ->Tensor:
        in_dtype = x.dtype
        return super().forward(x.to(dtype=torch.float))


class PixelShuffle(nn.PixelShuffle):
    """
    Rearranges elements in a tensor of shape :math:`(*, C 	imes r^2, H, W)`
    to a tensor of shape :math:`(*, C, H 	imes r, W 	imes r)`, where r is an upscale factor.

    Args:
        upscale_factor (int): factor to increase spatial resolution by

    Shape:
        - Input: :math:`(*, C 	imes r^2, H, W)`, where * is zero or more dimensions
        - Output: :math:`(*, C, H 	imes r, W 	imes r)`
    """

    def __init__(self, upscale_factor: 'int', *args, **kwargs) ->None:
        super(PixelShuffle, self).__init__(upscale_factor=upscale_factor)

    def __repr__(self):
        return '{}(upscale_factor={})'.format(self.__class__.__name__, self.upscale_factor)


class MaxPool2d(nn.MaxPool2d):
    """
    Applies a 2D max pooling over a 4D input tensor.

    Args:
        kernel_size (Optional[int]): the size of the window to take a max over
        stride (Optional[int]): The stride of the window. Default: 2
        padding (Optional[int]): Padding to be added on both sides of the tensor. Default: 1

    Shape:
        - Input: :math:`(N, C, H_{in}, W_{in})` where :math:`N` is the batch size, :math:`C` is the input channels,
            :math:`H_{in}` is the input height, and :math:`W_{in}` is the input width
        - Output: :math:`(N, C, H_{out}, W_{out})` where :math:`H_{out}` is the output height, and :math:`W_{in}` is
            the output width
    """

    def __init__(self, kernel_size: 'Optional[int]'=3, stride: 'Optional[int]'=2, padding: 'Optional[int]'=1, *args, **kwargs) ->None:
        super().__init__(kernel_size=kernel_size, stride=stride, padding=padding)

    def __repr__(self):
        return '{}(kernel_size={}, stride={})'.format(self.__class__.__name__, self.kernel_size, self.stride)


class AvgPool2d(nn.AvgPool2d):
    """
    Applies a 2D average pooling over a 4D input tensor.

    Args:
        kernel_size (Optional[int]): the size of the window to take a max over
        stride (Optional[int]): The stride of the window. Default: 2
        padding (Optional[int]): Padding to be added on both sides of the tensor. Default: 1
        ceil_mode (Optional[bool]): When True, will use `ceil` instead of `floor` to compute the output shape. Default: False
        count_include_pad (Optional[bool]): When True, will include the zero-padding in the averaging calculation. Default: True
        divisor_override: if specified, it will be used as divisor, otherwise size of the pooling region will be used. Default: None

    Shape:
        - Input: :math:`(N, C, H_{in}, W_{in})` where :math:`N` is the batch size, :math:`C` is the input channels,
            :math:`H_{in}` is the input height, and :math:`W_{in}` is the input width
        - Output: :math:`(N, C, H_{out}, W_{out})` where :math:`H_{out}` is the output height, and :math:`W_{in}` is
            the output width
    """

    def __init__(self, kernel_size: 'tuple', stride: 'Optional[tuple]'=None, padding: 'Optional[tuple]'=(0, 0), ceil_mode: 'Optional[bool]'=False, count_include_pad: 'Optional[bool]'=True, divisor_override: 'Optional[bool]'=None):
        super(AvgPool2d, self).__init__(kernel_size=kernel_size, stride=stride, padding=padding, ceil_mode=ceil_mode, count_include_pad=count_include_pad, divisor_override=divisor_override)

    def __repr__(self):
        return '{}(upscale_factor={})'.format(self.__class__.__name__, self.upscale_factor)


class LearnablePositionalEmbedding(nn.Module):
    """Learnable Positional embedding"""

    def __init__(self, opts, num_embeddings: 'int', embedding_dim: 'int', padding_idx: 'Optional[int]'=None, sequence_first: 'Optional[bool]'=False, interpolation_mode: 'Optional[str]'='bilinear', *args, **kwargs):
        super().__init__()
        self.pos_embed = nn.Parameter(torch.empty(1, 1, num_embeddings, embedding_dim))
        self.embedding_dim = embedding_dim
        self.num_embeddings = num_embeddings
        self.padding_idx = padding_idx
        self.sequence_first = sequence_first
        self.interpolation_mode = interpolation_mode
        self.reset_parameters()

    def reset_parameters(self) ->None:
        nn.init.trunc_normal_(self.pos_embed, mean=0, std=self.embedding_dim ** -0.5)
        if self.padding_idx is not None:
            with torch.no_grad():
                self.pos_embed[:, :, self.padding_idx, ...] = 0.0

    def forward(self, seq_len: 'int', *args, **kwargs) ->Tensor:
        pos_embed = self.pos_embed
        if self.padding_idx is not None:
            with torch.no_grad():
                pos_embed[:, :, self.padding_idx, ...] = 0.0
        if seq_len != self.num_embeddings:
            pos_embed = F.interpolate(pos_embed, size=(seq_len, self.embedding_dim), mode=self.interpolation_mode)
        if self.sequence_first:
            return pos_embed.reshape(seq_len, 1, self.embedding_dim)
        else:
            return pos_embed.reshape(1, seq_len, self.embedding_dim)

    def __repr__(self):
        return '{}(num_embeddings={}, embedding_dim={}, padding_idx={}, sequence_first={})'.format(self.__class__.__name__, self.num_embeddings, self.embedding_dim, self.padding_idx, self.sequence_first)


class SinusoidalPositionalEmbedding(nn.Module):

    def __init__(self, opts, num_embeddings: 'int', embedding_dim: 'int', padding_idx: 'Optional[int]'=None, sequence_first: 'Optional[bool]'=False, interpolation_mode: 'Optional[str]'='bilinear', *args, **kwargs):
        super().__init__()
        self.padding_idx = padding_idx
        self.num_embeddings = num_embeddings
        self.embedding_dim = embedding_dim
        self.sequence_first = sequence_first
        self.interpolation_mode = interpolation_mode
        self.register_buffer('pos_embed', self.get_weights())

    def get_weights(self) ->Tensor:
        """Build sinusoidal embeddings. Adapted from Fairseq."""
        half_dim = self.embedding_dim // 2
        emb = math.log(10000) / (half_dim - 1)
        emb = torch.exp(torch.arange(half_dim, dtype=torch.float) * -emb)
        emb = torch.arange(self.num_embeddings, dtype=torch.float).unsqueeze(1) * emb.unsqueeze(0)
        emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=1).reshape(self.num_embeddings, -1)
        if self.embedding_dim % 2 == 1:
            emb = torch.cat([emb, torch.zeros(self.num_embeddings, 1)], dim=1)
        if self.padding_idx is not None:
            emb[self.padding_idx, :] = 0
        return emb.unsqueeze(0).unsqueeze(0)

    def forward(self, seq_len: 'int', *args, **kwargs) ->Tensor:
        pos_embed = self.pos_embed
        if seq_len != self.num_embeddings:
            pos_embed = F.interpolate(pos_embed, size=(seq_len, self.embedding_dim), mode=self.interpolation_mode)
        if self.sequence_first:
            return pos_embed.reshape(seq_len, 1, self.embedding_dim)
        else:
            return pos_embed.reshape(1, seq_len, self.embedding_dim)

    def __repr__(self):
        return '{}(num_embeddings={}, embedding_dim={}, padding_idx={}, sequence_first={})'.format(self.__class__.__name__, self.num_embeddings, self.embedding_dim, self.padding_idx, self.sequence_first)


class PositionalEmbedding(BaseLayer):

    def __init__(self, opts, num_embeddings: 'int', embedding_dim: 'int', padding_idx: 'Optional[int]'=None, is_learnable: 'Optional[bool]'=False, sequence_first: 'Optional[bool]'=False, interpolation_mode: 'Optional[str]'='bilinear', *args, **kwargs):
        super().__init__(*args, **kwargs)
        module = LearnablePositionalEmbedding if is_learnable else SinusoidalPositionalEmbedding
        self.pos_embed = module(opts, *args, num_embeddings=num_embeddings, embedding_dim=embedding_dim, padding_idx=padding_idx, sequence_first=sequence_first, interpolation_mode=interpolation_mode, **kwargs)

    def forward(self, seq_len: 'int', *args, **kwargs) ->Tensor:
        return self.pos_embed(seq_len, *args, **kwargs)

    def __repr__(self):
        return self.pos_embed.__repr__()


class SinusoidalPositionalEncoding(BaseLayer):
    """
    This layer adds sinusoidal positional embeddings to a 3D input tensor. The code has been adapted from
    `Pytorch tutorial <https://pytorch.org/tutorials/beginner/transformer_tutorial.html>`_

    Args:
        d_model (int): dimension of the input tensor
        dropout (Optional[float]): Dropout rate. Default: 0.0
        max_len (Optional[int]): Max. number of patches (or seq. length). Default: 5000
        channels_last (Optional[bool]): Channels dimension is the last in the input tensor

    Shape:
        - Input: :math:`(N, C, P)` or :math:`(N, P, C)` where :math:`N` is the batch size, :math:`C` is the embedding dimension,
            :math:`P` is the number of patches
        - Output: same shape as the input

    """

    def __init__(self, d_model: 'int', dropout: 'Optional[float]'=0.0, max_len: 'Optional[int]'=5000, channels_last: 'Optional[bool]'=True, *args, **kwargs) ->None:
        position_last = not channels_last
        pos_encoding = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        pos_encoding[:, 0::2] = torch.sin(position * div_term)
        pos_encoding[:, 1::2] = torch.cos(position * div_term)
        pos_encoding = pos_encoding.unsqueeze(0)
        patch_dim = -2
        if position_last:
            pos_encoding = pos_encoding.transpose(1, 2)
            patch_dim = -1
        super().__init__()
        self.dropout = Dropout(p=dropout)
        self.patch_dim = patch_dim
        self.register_buffer('pe', pos_encoding)

    def forward_patch_last(self, x, indices: 'Optional[Tensor]'=None, *args, **kwargs) ->Tensor:
        if indices is None:
            x = x + self.pe[..., :x.shape[-1]]
        else:
            ndim = x.ndim
            repeat_size = [x.shape[0]] + [-1] * (ndim - 1)
            pe = self.pe.expand(repeat_size)
            selected_pe = torch.gather(pe, index=indices, dim=-1)
            x = x + selected_pe
        return self.dropout(x)

    def forward_others(self, x, indices: 'Optional[Tensor]'=None, *args, **kwargs) ->Tensor:
        if indices is None:
            x = x + self.pe[..., :x.shape[-2], :]
        else:
            ndim = x.ndim
            repeat_size = [x.shape[0]] + [-1] * (ndim - 1)
            pe = self.pe.expand(repeat_size)
            selected_pe = torch.gather(pe, index=indices, dim=-2)
            x = x + selected_pe
        return self.dropout(x)

    def forward(self, x, indices: 'Optional[Tensor]'=None, *args, **kwargs) ->Tensor:
        if self.patch_dim == -1:
            return self.forward_patch_last(x, indices=indices)
        else:
            return self.forward_others(x, indices=indices)

    def __repr__(self):
        return '{}(dropout={})'.format(self.__class__.__name__, self.dropout.p)


class LearnablePositionEncoding(BaseLayer):
    """
    This layer adds learnable positional embeddings to a 3D input tensor.

    Args:
        embed_dim (int): dimension of the input tensor
        num_embeddings (int): number of input embeddings. This is similar to vocab size in NLP.
        dropout (Optional[float]): Dropout rate. Default: 0.0
        channels_last (Optional[bool]): Channels dimension is the last in the input tensor

    Shape:
        - Input: :math:`(N, *, C, P)` or :math:`(N, *, P, C)` where :math:`N` is the batch size, :math:`C` is the embedding dimension,
            :math:`P` is the number of patches
        - Output: same shape as the input

    """

    def __init__(self, embed_dim: 'int', num_embeddings: 'int', dropout: 'Optional[float]'=0.0, channels_last: 'Optional[bool]'=True, *args, **kwargs) ->None:
        super().__init__()
        self.pos_emb = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embed_dim)
        self.channel_last = channels_last
        self.dropout = Dropout(p=dropout)

    def forward(self, x, *args, **kwargs) ->Tensor:
        num_embeddings = x.shape[-2] if self.channel_last else x.shape[-1]
        posistions = torch.arange(num_embeddings, dtype=torch.int64, device=x.device)
        position_emb = self.pos_emb(posistions)
        position_emb = position_emb.expand_as(x)
        x = x + position_emb
        return self.dropout(x)

    def __repr__(self):
        return '{}(embed_dim={}, vocab_size={}, dropout={})'.format(self.__class__.__name__, self.pos_emb.embedding_dim, self.pos_emb.num_embeddings, self.dropout.p)


def bound_fn(min_val: 'Union[float, int]', max_val: 'Union[float, int]', value: 'Union[float, int]') ->Union[float, int]:
    return max(min_val, min(max_val, value))


class RandomApply(BaseLayer):
    """
    This layer randomly applies a list of modules during training.

    Args:
        module_list (List): List of modules
        keep_p (Optional[float]): Keep P modules from the list during training. Default: 0.8 (or 80%)
    """

    def __init__(self, module_list: 'List', keep_p: 'Optional[float]'=0.8, *args, **kwargs) ->None:
        super().__init__()
        n_modules = len(module_list)
        self.module_list = module_list
        self.module_indexes = [i for i in range(1, n_modules)]
        k = int(round(n_modules * keep_p))
        self.keep_k = bound_fn(min_val=1, max_val=n_modules, value=k)

    def forward(self, x: 'Tensor') ->Tensor:
        if self.training:
            indexes = [0] + sorted(random.sample(self.module_indexes, k=self.keep_k))
            for idx in indexes:
                x = self.module_list[idx](x)
        else:
            for layer in self.module_list:
                x = layer(x)
        return x

    def __repr__(self):
        format_string = '{}(apply_k (N={})={}, '.format(self.__class__.__name__, len(self.module_list), self.keep_k)
        for layer in self.module_list:
            format_string += '\n\t {}'.format(layer)
        format_string += '\n)'
        return format_string


class SingleHeadAttention(BaseLayer):
    """
    This layer applies a single-head attention as described in `DeLighT <https://arxiv.org/abs/2008.00623>`_ paper

    Args:
        embed_dim (int): :math:`C_{in}` from an expected input of size :math:`(N, P, C_{in})`
        attn_dropout (Optional[float]): Attention dropout. Default: 0.0
        bias (Optional[bool]): Use bias or not. Default: ``True``

    Shape:
        - Input: :math:`(N, P, C_{in})` where :math:`N` is batch size, :math:`P` is number of patches,
        and :math:`C_{in}` is input embedding dim
        - Output: same shape as the input

    """

    def __init__(self, embed_dim: 'int', attn_dropout: 'Optional[float]'=0.0, bias: 'Optional[bool]'=True, *args, **kwargs) ->None:
        super().__init__()
        self.qkv_proj = LinearLayer(in_features=embed_dim, out_features=3 * embed_dim, bias=bias)
        self.attn_dropout = Dropout(p=attn_dropout)
        self.out_proj = LinearLayer(in_features=embed_dim, out_features=embed_dim, bias=bias)
        self.softmax = nn.Softmax(dim=-1)
        self.embed_dim = embed_dim
        self.scaling = self.embed_dim ** -0.5

    def __repr__(self) ->str:
        return '{}(embed_dim={}, attn_dropout={})'.format(self.__class__.__name__, self.embed_dim, self.attn_dropout.p)

    def forward(self, x_q: 'Tensor', x_kv: 'Optional[Tensor]'=None, key_padding_mask: 'Optional[Tensor]'=None, attn_mask: 'Optional[Tensor]'=None, *args, **kwargs) ->Tensor:
        if x_kv is None:
            qkv = self.qkv_proj(x_q)
            query, key, value = torch.chunk(qkv, chunks=3, dim=-1)
        else:
            query = F.linear(x_q, weight=self.qkv_proj.weight[:self.embed_dim, ...], bias=self.qkv_proj.bias[:self.embed_dim])
            kv = F.linear(x_kv, weight=self.qkv_proj.weight[self.embed_dim:, ...], bias=self.qkv_proj.bias[self.embed_dim:])
            key, value = torch.chunk(kv, chunks=2, dim=-1)
        query = query * self.scaling
        key = key.transpose(-2, -1)
        attn = torch.matmul(query, key)
        if attn_mask is not None:
            assert list(attn_mask.shape) == list(attn.shape), 'Shape of attention mask and attn should be the same. Got: {} and {}'.format(attn_mask.shape, attn.shape)
            attn = attn + attn_mask
        if key_padding_mask is not None:
            batch_size, num_src_tokens, num_tgt_tokens = attn.shape
            assert key_padding_mask.dim() == 2 and list(key_padding_mask.shape) == [batch_size, num_tgt_tokens], 'Key_padding_mask should be 2-dimension with shape [{}, {}]. Got: {}'.format(batch_size, num_tgt_tokens, key_padding_mask.shape)
            attn = attn.masked_fill(key_padding_mask.unsqueeze(1), float('-inf'))
        attn = self.softmax(attn)
        attn = self.attn_dropout(attn)
        out = torch.matmul(attn, value)
        out = self.out_proj(out)
        return out


class Softmax(nn.Softmax):
    """
    Applies the Softmax function to an input tensor along the specified dimension

    Args:
        dim (int): Dimension along which softmax to be applied. Default: -1

    Shape:
        - Input: :math:`(*)` where :math:`*` is one or more dimensions
        - Output: same shape as the input
    """

    def __init__(self, dim: 'Optional[int]'=-1, *args, **kwargs):
        super().__init__(dim=dim)


def pad_x_and_mask(x: 'torch.Tensor', key_padding_mask: 'torch.Tensor', window_size: 'int') ->Tuple[torch.Tensor, torch.Tensor]:
    """
    Apply padding to @x and @key_padding_mask to make their lengths divisible
    by @window_size.

    Args:
        x: The input tensor of shape [B, N, C].
        key_padding_mask: The mask of shape [B, N].
        window_size: the N dimension of @x and @key_padding_mask will be padded
            to make them divisble by this number.

    Returns:
        A tuple containing @x and @key_padding_mask, with padding applied.
    """
    B, N, _ = x.shape
    padding = (window_size - N % window_size) % window_size
    if key_padding_mask is not None:
        key_padding_mask = F.pad(key_padding_mask, (0, padding), value=float('-inf'))
    x = F.pad(x, (0, 0, 0, padding), value=0)
    return x, key_padding_mask


class TokenMerging(nn.Module):
    """
    Merge tokens from a [batch_size, sequence_length, num_channels] tensor
    using a linear projection.

    This function also updates masks and adds padding as needed to make the
    sequence length divisible by the window size before merging tokens.

    Args:
        dim: Number of input channels.
        window: The size of the window to merge into a single token.
    """

    def __init__(self, dim: 'int', window: 'int'=2) ->None:
        super().__init__()
        self.dim = dim
        self.reduction = linear_layer.LinearLayer(window * dim, dim, bias=False)
        self.norm = layer_norm.LayerNorm(dim)
        self.window = window

    def forward(self, x: 'torch.Tensor', key_padding_mask: 'torch.Tensor') ->Tuple[torch.Tensor, torch.Tensor]:
        """
        Perform token merging.

        Args:
            x: A tensor of shape [batch_size, sequence_length, num_channels].
            key_padding_mask: A tensor of shape [batch_size, sequence_length]
                with "-inf" values at mask tokens, and "0" values at unmasked
                tokens.

        Returns:
            A tensor of shape [batch_size, math.ceil(sequence_length /
                self.window), num_channels], where @self.window is the window
                size.
        """
        if key_padding_mask is not None:
            x[key_padding_mask == float('-inf')] = 0
        x, key_padding_mask = pad_x_and_mask(x, key_padding_mask, self.window)
        B, N, C = x.shape
        x = x.unfold(1, self.window, self.window)
        x = x.reshape(B, N // self.window, C * self.window)
        x = self.reduction(x)
        x = self.norm(x)
        if key_padding_mask is not None:
            assert key_padding_mask.shape == (B, N)
            key_padding_mask = key_padding_mask.unfold(1, self.window, self.window)
            key_padding_mask = key_padding_mask.max(dim=-1).values
        return x, key_padding_mask

    def extra_repr(self) ->str:
        return f'dim={self.dim}, window={self.window}'


class UpSample(nn.Upsample):
    """
    This layer upsamples a given input tensor.

    Args:
        size (Optional[Union[int, Tuple[int, ...]]): Output spatial size. Default: None
        scale_factor (Optional[float]): Scale each spatial dimension of the input by this factor. Default: None
        mode (Optional[str]): Upsampling algorithm (``'nearest'``, ``'linear'``, ``'bilinear'``, ``'bicubic'`` and ``'trilinear'``. Default: ``'nearest'``
        align_corners (Optional[bool]): if ``True``, the corner pixels of the input and output tensors are aligned, and thus preserving the values at
            those pixels. This only has effect when :attr:`mode` is ``'linear'``, ``'bilinear'``, ``'bicubic'``, or ``'trilinear'``.
            Default: ``None``

    Shape:
        - Input: :math:`(N, C, W_{in})` or :math:`(N, C, H_{in}, W_{in})` or :math:`(N, C, D_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C, W_{out})` or :math:`(N, C, H_{out}, W_{out})` or :math:`(N, C, D_{out}, H_{out}, W_{out})`
    """

    def __init__(self, size: 'Optional[Union[int, Tuple[int, ...]]]'=None, scale_factor: 'Optional[float]'=None, mode: 'Optional[str]'='nearest', align_corners: 'Optional[bool]'=None, *args, **kwargs) ->None:
        super().__init__(size=size, scale_factor=scale_factor, mode=mode, align_corners=align_corners)


class JsonValidator:

    def __init__(self, expected_type: 'type'):
        """
        JsonValidator(T) is function (s)->x that parses json string s into python value x, where x is of type T.

        Example Usage:
        >>> from typing import Union, List
        >>> import argparse
        >>> parser = argparse.ArgumentParser()
        >>> parser.add_argument("--x", type=JsonValidator(Union[int, List[float]]))
        >>> assert parser.parse_args(["--x=123"]).x == 123
        >>> assert parser.parse_args(["--x=[1, 2]"]).x == [1., 2.]
        """
        self.expected_type = expected_type

    @classmethod
    def _validate_and_cast(cls, json_value: 'Any', expected_type: 'Any'):
        type_cls = typing.get_origin(expected_type) or expected_type
        type_args = typing.get_args(expected_type)
        if type_cls is typing.Any:
            return json_value
        if type_cls is float and isinstance(json_value, (int, float)):
            return float(json_value)
        elif type_cls in (int, str, bool) and isinstance(json_value, type_cls):
            return json_value
        elif type_cls is None and json_value is None:
            return None
        elif type_cls is typing.Union:
            for arg in type_args:
                try:
                    return cls._validate_and_cast(json_value, arg)
                except TypeError:
                    continue
        elif type_cls is dict and isinstance(json_value, dict):
            if not type_args:
                type_args = Any, Any
            type_key, type_value = type_args
            return {cls._validate_and_cast(key, type_key): cls._validate_and_cast(value, type_value) for key, value in json_value.items()}
        elif type_cls is list and isinstance(json_value, list):
            if not type_args:
                type_args = [Any]
            return [cls._validate_and_cast(x, type_args[0]) for x in json_value]
        elif type_cls is tuple and isinstance(json_value, list) and (type_args is None or len(type_args) == len(json_value)):
            if type_args is None:
                type_args = [Any] * len(json_value)
            return tuple(type_cls(cls._validate_and_cast(item, type_arg) for item, type_arg in zip(json_value, type_args)))
        raise TypeError(f'Cannot cast {json_value} with type {type(json_value)} to {expected_type}')

    def __call__(self, str_value: 'str') ->Any:
        try:
            value = json.loads(str_value)
        except json.JSONDecodeError:
            raise TypeError(f"Cannot parse json value '{str_value}' for {self}")
        return self._validate_and_cast(value, self.expected_type)

    def __repr__(self):
        return f'JSON[{self.expected_type}]'


NORM_LAYER_CLS = []


norm_layers_tuple = tuple(NORM_LAYER_CLS)


def unwrap_model_fn(model: 'torch.nn.Module') ->torch.nn.Module:
    """Helper function to unwrap the model.

    Args:
        model: An instance of torch.nn.Module.

    Returns:
        Unwrapped instance of torch.nn.Module.
    """
    unwrapped_model = model
    while True:
        if hasattr(unwrapped_model, 'module'):
            unwrapped_model = unwrapped_model.module
        elif hasattr(unwrapped_model, '_fsdp_wrapped_module'):
            unwrapped_model = unwrapped_model._fsdp_wrapped_module
        else:
            break
    return unwrapped_model


def check_frozen_norm_layer(model: 'torch.nn.Module') ->Tuple[bool, int]:
    unwrapped_model = unwrap_model_fn(model)
    count_norm = 0
    frozen_state = False
    for m in unwrapped_model.modules():
        if isinstance(m, norm_layers_tuple):
            frozen_state = m.weight.requires_grad
    return frozen_state, count_norm


def get_tensor_sizes(data: 'Union[Dict, Tensor]') ->Union[List[str], List[Tuple[int]]]:
    """Utility function for extracting tensor shapes (for printing purposes only)."""
    if isinstance(data, Dict):
        tensor_sizes = []
        for k, v in data.items():
            size_ = get_tensor_sizes(v)
            if size_:
                tensor_sizes.append(f'{k}: {size_}')
        return tensor_sizes
    elif isinstance(data, Tensor):
        return [*data.shape]
    else:
        return []


supported_conv_inits = ['kaiming_normal', 'kaiming_uniform', 'xavier_normal', 'xavier_uniform', 'normal', 'trunc_normal']


def _init_nn_layers(module, init_method: 'Optional[str]'='kaiming_normal', std_val: 'Optional[float]'=None) ->None:
    """
    Helper function to initialize neural network module
    """
    init_method = init_method.lower()
    if init_method == 'kaiming_normal':
        if module.weight is not None:
            nn.init.kaiming_normal_(module.weight, mode='fan_out')
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif init_method == 'kaiming_uniform':
        if module.weight is not None:
            nn.init.kaiming_uniform_(module.weight, mode='fan_out')
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif init_method == 'xavier_normal':
        if module.weight is not None:
            nn.init.xavier_normal_(module.weight)
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif init_method == 'xavier_uniform':
        if module.weight is not None:
            nn.init.xavier_uniform_(module.weight)
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif init_method == 'normal':
        if module.weight is not None:
            std = 1.0 / module.weight.size(1) ** 0.5 if std_val is None else std_val
            nn.init.normal_(module.weight, mean=0.0, std=std)
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    elif init_method == 'trunc_normal':
        if module.weight is not None:
            std = 1.0 / module.weight.size(1) ** 0.5 if std_val is None else std_val
            nn.init.trunc_normal_(module.weight, mean=0.0, std=std)
        if module.bias is not None:
            nn.init.zeros_(module.bias)
    else:
        supported_conv_message = 'Supported initialization methods are:'
        for i, l in enumerate(supported_conv_inits):
            supported_conv_message += '\n \t {}) {}'.format(i, l)
        logger.error('{} \n Got: {}'.format(supported_conv_message, init_method))


def initialize_conv_layer(module, init_method: 'Optional[str]'='kaiming_normal', std_val: 'Optional[float]'=0.01) ->None:
    """Helper function to initialize convolution layers"""
    _init_nn_layers(module=module, init_method=init_method, std_val=std_val)


def initialize_fc_layer(module, init_method: 'Optional[str]'='normal', std_val: 'Optional[float]'=0.01) ->None:
    """Helper function to initialize fully-connected layers"""
    if hasattr(module, 'layer'):
        _init_nn_layers(module=module.layer, init_method=init_method, std_val=std_val)
    else:
        _init_nn_layers(module=module, init_method=init_method, std_val=std_val)


def initialize_norm_layers(module) ->None:
    """Helper function to initialize normalization layers"""

    def _init_fn(module):
        if hasattr(module, 'weight') and module.weight is not None:
            nn.init.ones_(module.weight)
        if hasattr(module, 'bias') and module.bias is not None:
            nn.init.zeros_(module.bias)
    _init_fn(module.layer) if hasattr(module, 'layer') else _init_fn(module=module)


def initialize_weights(opts, modules) ->None:
    """Helper function to initialize differnet layers in a model"""
    conv_init_type = getattr(opts, 'model.layer.conv_init', 'kaiming_normal')
    linear_init_type = getattr(opts, 'model.layer.linear_init', 'normal')
    conv_std = getattr(opts, 'model.layer.conv_init_std_dev', None)
    linear_std = getattr(opts, 'model.layer.linear_init_std_dev', 0.01)
    group_linear_std = getattr(opts, 'model.layer.group_linear_init_std_dev', 0.01)
    if isinstance(modules, nn.Sequential):
        for m in modules:
            if isinstance(m, (nn.Conv2d, nn.Conv3d)):
                initialize_conv_layer(module=m, init_method=conv_init_type, std_val=conv_std)
            elif isinstance(m, norm_layers_tuple):
                initialize_norm_layers(module=m)
            elif isinstance(m, (nn.Linear, LinearLayer)):
                initialize_fc_layer(module=m, init_method=linear_init_type, std_val=linear_std)
            elif isinstance(m, GroupLinear):
                initialize_fc_layer(module=m, init_method=linear_init_type, std_val=group_linear_std)
    elif isinstance(modules, (nn.Conv2d, nn.Conv3d)):
        initialize_conv_layer(module=modules, init_method=conv_init_type, std_val=conv_std)
    elif isinstance(modules, norm_layers_tuple):
        initialize_norm_layers(module=modules)
    elif isinstance(modules, (nn.Linear, LinearLayer)):
        initialize_fc_layer(module=modules, init_method=linear_init_type, std_val=linear_std)
    elif isinstance(modules, GroupLinear):
        initialize_fc_layer(module=modules, init_method=linear_init_type, std_val=group_linear_std)


class UniformSampler(nn.Module):

    def __init__(self, low: 'float', high: 'float', min_fn: 'Optional[nn.Module]'=Identity(), max_fn: 'Optional[nn.Module]'=Identity(), *args, **kwargs):
        super().__init__()
        self._low = nn.Parameter(torch.tensor(low, dtype=torch.float))
        self._high = nn.Parameter(torch.tensor(high, dtype=torch.float))
        self.min_fn = min_fn
        self.max_fn = max_fn

    def forward(self, sample_shape=(), data_type=torch.float, device=torch.device('cpu')) ->Tensor:
        rand_tensor = torch.rand(sample_shape, dtype=data_type, device=device)
        return self.low + rand_tensor * (self.high - self.low)

    @property
    def high(self):
        return self.max_fn(self._high)

    @property
    def low(self):
        return self.min_fn(self._low)

    def __repr__(self):
        return '{}(min_fn={}, max_fn={})'.format(self.__class__.__name__, self.min_fn, self.max_fn)


_distribution_tuple = UniformSampler,


def random_brightness(x: 'Tensor', magnitude: 'Tensor', *args, **kwargs) ->Tensor:
    """
    Brightness function.
    """
    x = x * magnitude
    return x


def random_contrast(x: 'Tensor', magnitude: 'Tensor', *args, **kwargs) ->Tensor:
    per_channel_mean = torch.mean(x, dim=[-1, -2], keepdim=True)
    x = (1.0 - magnitude) * per_channel_mean + x * magnitude
    return x


def random_noise(x: 'Tensor', variance: 'Tensor', *args, **kwargs) ->Tensor:
    """Apply random noise sampled."""
    noise = torch.randn_like(x) * variance
    x = x + noise
    return x


class BaseNeuralAugmentor(nn.Module):
    """
    Base class for `neural (or range) augmentation <https://arxiv.org/abs/2212.10553>`_
    """

    def __init__(self, opts, *args, **kwargs):
        super().__init__()
        self.opts = opts
        self.lr_multiplier = getattr(opts, 'model.learn_augmentation.lr_multiplier', 1.0)
        self.brightness = None
        self.contrast = None
        self.noise = None
        self.aug_fns = []

    def _is_valid_aug_fn_list(self, aug_fns):
        if self.training:
            if len(aug_fns) == 0:
                logger.error('{} needs at least one learnable function.'.format(self.__class__.__name__))

    def get_trainable_parameters(self, weight_decay: 'Optional[float]'=0.0, no_decay_bn_filter_bias: 'Optional[bool]'=False, *args, **kwargs):
        """Get trainable parameters"""
        param_list = parameter_list(named_parameters=self.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias)
        return param_list, [self.lr_multiplier] * len(param_list)

    def __repr__(self):
        aug_str = '{}('.format(self.__class__.__name__)
        if self.brightness is not None:
            aug_str += '\n\tBrightness={}, '.format(self.brightness.data.shape if isinstance(self.brightness, nn.Parameter) else self.brightness)
        if self.contrast is not None:
            aug_str += '\n\tContrast={}, '.format(self.contrast.data.shape if isinstance(self.contrast, nn.Parameter) else self.contrast)
        if self.noise is not None:
            aug_str += '\n\tNoise={}, '.format(self.noise.data.shape if isinstance(self.noise, nn.Parameter) else self.noise)
        aug_str += self.extra_repr()
        aug_str += ')'
        return aug_str

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser'):
        """Add model-specific arguments"""
        group = parser.add_argument_group(title=cls.__name__)
        group.add_argument('--model.learn-augmentation.mode', type=str, default=None, choices=['basic', 'distribution'], help='Neural augmentation mode')
        group.add_argument('--model.learn-augmentation.brightness', action='store_true', help='Learn parameters for brightness')
        group.add_argument('--model.learn-augmentation.contrast', action='store_true', help='Learn parameters for contrast')
        group.add_argument('--model.learn-augmentation.noise', action='store_true', help='Learn parameters for noise')
        group.add_argument('--model.learn-augmentation.lr-multiplier', type=float, default=1.0, help='LR multiplier for neural aug parameters')
        return parser

    def _build_aug_fns(self, opts) ->List:
        raise NotImplementedError

    def _apply_brightness(self, x: 'Tensor', *args, **kwargs) ->Tensor:
        """
        Apply brightness augmentation function with learnable parameters.
        """
        x_shape = [*x.shape]
        x_shape[1:] = [1] * (len(x_shape) - 1)
        if isinstance(self.brightness, nn.Parameter):
            magnitude = self.brightness
        elif isinstance(self.brightness, _distribution_tuple):
            magnitude = self.brightness(x_shape, device=x.device, data_type=x.dtype)
        else:
            raise NotImplementedError
        return random_brightness(x, magnitude, *args, **kwargs)

    def _apply_contrast(self, x: 'Tensor', *args, **kwargs) ->Tensor:
        """
        Apply contrast augmentation function with learnable parameters.
        """
        x_shape = [*x.shape]
        x_shape[1:] = [1] * (len(x_shape) - 1)
        if isinstance(self.contrast, nn.Parameter):
            magnitude = self.contrast
        elif isinstance(self.contrast, _distribution_tuple):
            magnitude = self.contrast(x_shape, device=x.device, data_type=x.dtype)
        else:
            raise NotImplementedError
        return random_contrast(x, magnitude, *args, *kwargs)

    def _apply_noise(self, x: 'Tensor', *args, **kwargs) ->Tensor:
        x_shape = [*x.shape]
        x_shape[1:] = [1] * (len(x_shape) - 1)
        if isinstance(self.noise, nn.Parameter):
            variance = self.noise
        elif isinstance(self.noise, _distribution_tuple):
            variance = self.noise(x_shape, device=x.device, data_type=x.dtype)
        else:
            raise NotImplementedError
        return random_noise(x, variance, *args, *kwargs)

    def forward(self, x: 'Tensor', *args, **kwargs) ->Tensor:
        batch_size, in_channels, in_height, in_width = x.shape
        n_aug_samples = max(1, batch_size // 2)
        random.shuffle(self.aug_fns)
        for aug_fn in self.aug_fns:
            sample_ids = torch.randperm(n=batch_size, dtype=torch.long, device=x.device)[:n_aug_samples]
            x_aug = torch.index_select(x, dim=0, index=sample_ids)
            x_aug = aug_fn(x=x_aug)
            x = torch.index_copy(x, dim=0, source=x_aug, index=sample_ids)
        x = torch.clip(x, min=0.0, max=1.0)
        return x


class Clip(nn.Module):

    def __init__(self, min_val: 'float', max_val: 'float', hard_clip: 'Optional[bool]'=False, *args, **kwargs) ->None:
        super().__init__()
        self.min_val = min_val
        self.max_val = max_val
        self.hard_clip = hard_clip

    def forward(self, x: 'Any') ->Any:
        if self.hard_clip:
            with torch.no_grad():
                return x.clamp_(min=self.min_val, max=self.max_val)
        else:
            return torch.sigmoid(x) * (self.max_val - self.min_val) + self.min_val

    def __repr__(self):
        return '{}(min={}, max={}, clipping={})'.format(self.__class__.__name__, self.min_val, self.max_val, 'hard' if self.hard_clip else 'soft')


class FixedSampler(nn.Module):

    def __init__(self, value: 'float', clip_fn: 'Optional[nn.Module]'=Identity(), *args, **kwargs):
        super().__init__()
        self._value = nn.Parameter(torch.FloatTensor(1, 3, 1, 1).fill_(value))
        self.clip_fn = clip_fn

    def forward(self, sample_shape=(), data_type=torch.float, device=torch.device('cpu')) ->Tensor:
        return self.clip_fn(self._value)

    def __repr__(self):
        return '{}(clip_fn={})'.format(self.__class__.__name__, self.clip_fn)


class BasicNeuralAugmentor(BaseNeuralAugmentor):
    """
    Basic neural augmentation. This class learns per-channel augmentation parameters
    and apply the same parameter to all images in a batch.

    See `neural (or range) augmentation <https://arxiv.org/abs/2212.10553>`_ paper for details.
    """

    def __init__(self, opts, *args, **kwargs) ->None:
        super().__init__(*args, opts=opts, **kwargs)
        aug_fns = self._build_aug_fns(opts=opts)
        self._is_valid_aug_fn_list(aug_fns)
        self.aug_fns = aug_fns

    def _build_aug_fns(self, opts) ->List:
        aug_fns = []
        if getattr(opts, 'model.learn_augmentation.brightness', False):
            self.brightness = FixedSampler(value=1.0, clip_fn=Clip(min_val=0.1, max_val=10.0))
            aug_fns.append(self._apply_brightness)
        if getattr(opts, 'model.learn_augmentation.contrast', False):
            self.contrast = FixedSampler(value=1.0, clip_fn=Clip(min_val=0.1, max_val=10.0))
            aug_fns.append(self._apply_contrast)
        if getattr(opts, 'model.learn_augmentation.noise', False):
            self.noise = FixedSampler(value=0.0, clip_fn=Clip(min_val=0.0, max_val=1.0))
            aug_fns.append(self._apply_noise)
        return aug_fns


class DistributionNeuralAugmentor(BaseNeuralAugmentor):
    """
    Distribution-based neural (or range) augmentation. This class samples the augmentation parameters
    from a specified distribution with learnable range.

    See `neural (or range) augmentation <https://arxiv.org/abs/2212.10553>`_ paper for details.
    """

    def __init__(self, opts, *args, **kwargs) ->None:
        super().__init__(*args, opts=opts, **kwargs)
        aug_fns = self._build_aug_fns_with_uniform_dist(opts=opts)
        self._is_valid_aug_fn_list(aug_fns)
        self.aug_fns = aug_fns

    def _build_aug_fns_with_uniform_dist(self, opts) ->List:
        aug_fns = []
        if getattr(opts, 'model.learn_augmentation.brightness', False):
            self.brightness = UniformSampler(low=0.5, high=1.5, min_fn=Clip(min_val=0.1, max_val=0.9), max_fn=Clip(min_val=1.1, max_val=10.0))
            aug_fns.append(self._apply_brightness)
        if getattr(opts, 'model.learn_augmentation.contrast', False):
            self.contrast = UniformSampler(low=0.5, high=1.5, min_fn=Clip(min_val=0.1, max_val=0.9), max_fn=Clip(min_val=1.1, max_val=10.0))
            aug_fns.append(self._apply_contrast)
        if getattr(opts, 'model.learn_augmentation.noise', False):
            self.noise = UniformSampler(low=0.0, high=0.1, min_fn=Clip(min_val=0.0, max_val=5e-05), max_fn=Clip(min_val=0.0001, max_val=1.0))
            aug_fns.append(self._apply_noise)
        return aug_fns


def build_neural_augmentor(opts, *args, **kwargs):
    mode = getattr(opts, 'model.learn_augmentation.mode', None)
    if mode is None:
        mode = 'none'
    mode = mode.lower()
    if mode == 'distribution':
        return DistributionNeuralAugmentor(*args, opts=opts, **kwargs)
    elif mode == 'basic':
        return BasicNeuralAugmentor(*args, opts=opts, **kwargs)
    else:
        return None


def is_test_env() ->bool:
    return 'PYTEST_CURRENT_TEST' in os.environ


def set_model_specific_opts_before_model_building(opts: 'argparse.Namespace') ->Dict[str, Any]:
    """Override library-level defaults with model-specific default values.

    Args:
        opts: Command-line arguments

    Returns:
        A dictionary containing the name of arguments that are updated along with their original values.
        This dictionary is used in `unset_model_specific_opts_after_model_building` function to unset the
        model-specific to library-specific defaults.
    """
    seg_act_fn = getattr(opts, 'model.segmentation.activation.name')
    if seg_act_fn is not None:
        default_act_fn = getattr(opts, 'model.activation.name', 'relu')
        default_act_inplace = getattr(opts, 'model.activation.inplace', False)
        default_act_neg_slope = getattr(opts, 'model.activation.neg_slope', 0.1)
        setattr(opts, 'model.activation.name', seg_act_fn)
        setattr(opts, 'model.activation.inplace', getattr(opts, 'model.segmentation.activation.inplace', False))
        setattr(opts, 'model.activation.neg_slope', getattr(opts, 'model.segmentation.activation.neg_slope', 0.1))
        return {'model.activation.name': default_act_fn, 'model.activation.inplace': default_act_inplace, 'model.activation.neg_slope': default_act_neg_slope}
    return {}


def unset_model_specific_opts_after_model_building(opts: 'argparse.Namespace', default_opts_info: 'Dict[str, Any]', *ars, **kwargs) ->None:
    """Given command-line arguments and a mapping of opts that needs to be unset, this function
    unsets the library-level defaults that were over-ridden previously
    in `set_model_specific_opts_before_model_building`.
    """
    assert isinstance(default_opts_info, dict), f'Please ensure set_model_specific_opts_before_model_building() returns a dict.'
    for k, v in default_opts_info.items():
        setattr(opts, k, v)


def window_partition_reverse(t: 'torch.Tensor', B: 'int', num_windows: 'int', C: 'int') ->torch.Tensor:
    """
    Undo the @window_partition operation.

    Args:
        t: The input tensor of shape [batch_size * num_windows, window_size,
            embed_dim].
        B: The batch size.
        num_windows: The number of windows.
        C: The embedding dimension.

    Returns:
        A tensor of shape [batch_size, num_windows * window_size, embed_dim].
    """
    t = t.reshape(B, num_windows * t.shape[1], C)
    return t


def unwindow_x(x_windows: 'torch.Tensor', B: 'int', N: 'int', C: 'int', window_shift: 'int'):
    """
    Undoes the operation of @window_x_and_attention on the input tensor @x_windows.

    Args:
        x_windows: The input tensor to unwindow. Its shape is [batch_size *
              padded_sequence_length // window_size, window_size, embed_dim].
        B: The batch size. Referred to as batch_size in this docstring.
        N: The sequence length of the tensor before windowing. Referred to as
            sequence_length in this docstring.
        C: The number of channels. Referred to as embed_dim in this docstring.
        window_shift: The shift applied to the sequence before the windowing
            originally occurred.

    Returns:
        A tensor of shape [batch_size, sequence_length, embed_dim].
    """
    num_windows = x_windows.shape[0] // B
    x = window_partition_reverse(x_windows, B, num_windows, C)
    if window_shift > 0:
        x = torch.roll(x, shifts=window_shift, dims=1)
    x = x[:, :N]
    return x


def get_windows_shift_mask(N: 'int', window_size: 'int', window_shift: 'int', device: 'torch.device') ->torch.Tensor:
    """
    Get the mask window required due to window shifting (needed for shifted
    window attention).

    This produces a tensor with mask values for each window. Most windows don't
    require masking, but windows that bleed across the beginning/end of the
    tensor (due to shifting) require it.

    Args:
        N: The sequence length.
        window_size: The window size.
        window_shift: The window shift.
        device: The device on which to create the tensor.

    Returns:
        A tensor of shape [N // window_size, window_size, window_size]
        containing mask values. The values are 0 (unmasked) or float("-inf")
        (masked).
    """
    ret = torch.zeros(N // window_size, window_size, window_size, device=device)
    ret[-1].fill_(float('-inf'))
    ret[-1, :window_size - window_shift, :window_size - window_shift] = 0
    ret[-1, -window_shift:, -window_shift:] = 0
    return ret


def window_partition(t: 'torch.Tensor', window_size: 'int') ->torch.Tensor:
    """
    Partition tensor @t into chunks of size @window_size.

    @t's sequence length must be divisible by @window_size.

    Args:
        t: A tensor of shape [batch_size, sequence_length, embed_dim].
        window_size: The desired window size.

    Returns:
        A tensor of shape [batch_size * sequence_length // window_size,
        window_size, embed_dim].
    """
    B, N, C = t.shape
    if not N % window_size == 0:
        raise ValueError(f'sequence length {N} must be divisible by window size {window_size}')
    t = t.reshape(B * N // window_size, window_size, C)
    return t


def window_x_and_key_padding_mask(x: 'torch.Tensor', key_padding_mask: 'torch.Tensor', window_size: 'int', window_shift: 'int') ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    """
    Perform windowing on @x and @key_padding_mask in preparation for windowed
    attention.

    Args:
        x: The input tensor of shape [batch_size, sequence_length, num_channels].
        key_padding_mask: The mask, as a tensor of shape [batch_size, sequence_length].
        window_size: The window size to be used for windowed attention.
        window_shift: The window shift to be used for windowed attention.

    Returns:
        A tuple containing 3 tensors. The first is the windowed input. The second
        is the windowed mask. The third is the mask needed to perform shifted
        window attention (to avoid the first and last windows from bleeding
        into each other).
    """
    B, N = key_padding_mask.shape
    assert x.shape[:2] == (B, N)
    x, key_padding_mask = token_merging.pad_x_and_mask(x, key_padding_mask, window_size)
    if window_shift > 0:
        x = torch.roll(x, shifts=-window_shift, dims=1)
        key_padding_mask = torch.roll(key_padding_mask, shifts=-window_shift, dims=1)
    x_windows = window_partition(x, window_size)
    token_mask_windows = key_padding_mask.reshape(B * x.shape[1] // window_size, window_size)
    window_mask = get_windows_shift_mask(x.shape[1], window_size, window_shift, x_windows.device).expand(B, -1, -1, -1)
    window_mask = window_mask.reshape(window_mask.shape[0] * window_mask.shape[1], window_mask.shape[2], window_mask.shape[3])
    return x_windows, token_mask_windows, window_mask


def get_configuration(opts: 'argparse.Namespace') ->Dict:
    """
    Get configuration parameters associated with ByteFormer.

    These parameters are similar to those of DeIT
    (https://arxiv.org/pdf/2012.12877.pdf).

    Args:
        opts: The options configuration.

    Returns:
        A dict with keys specifying the parameters needed for ByteFormer.
    """
    mode = getattr(opts, 'model.classification.byteformer.mode')
    mode = mode.lower()
    dropout = getattr(opts, 'model.classification.byteformer.dropout')
    norm_layer = getattr(opts, 'model.classification.byteformer.norm_layer')
    byteformer_config = dict()
    if mode == 'tiny':
        byteformer_config = {'embed_dim': 192, 'n_transformer_layers': 12, 'n_attn_heads': 3, 'ffn_dim': 192 * 4, 'norm_layer': norm_layer, 'pos_emb_drop_p': 0.1, 'attn_dropout': 0.0, 'ffn_dropout': 0.0, 'dropout': dropout}
    elif mode == 'small':
        byteformer_config = {'embed_dim': 384, 'n_transformer_layers': 12, 'n_attn_heads': 6, 'ffn_dim': 384 * 4, 'norm_layer': norm_layer, 'pos_emb_drop_p': 0.0, 'attn_dropout': 0.0, 'ffn_dropout': 0.0, 'dropout': dropout}
    elif mode == 'base':
        byteformer_config = {'embed_dim': 768, 'n_transformer_layers': 12, 'n_attn_heads': 12, 'ffn_dim': 768 * 4, 'norm_layer': norm_layer, 'pos_emb_drop_p': 0.0, 'attn_dropout': 0.0, 'ffn_dropout': 0.0, 'dropout': dropout}
    elif mode == 'huge':
        byteformer_config = {'embed_dim': 1280, 'n_transformer_layers': 32, 'n_attn_heads': 20, 'ffn_dim': 1280 * 4, 'norm_layer': norm_layer, 'pos_emb_drop_p': 0.0, 'attn_dropout': 0.0, 'ffn_dropout': 0.0, 'dropout': dropout}
    else:
        logger.error('Got unsupported ByteFormer configuration: {}'.format(mode))
    return byteformer_config


def unfold_tokens(t: 'Tensor', kernel_size: 'int') ->Tensor:
    """
    Group tokens from tensor @t using torch.Tensor.unfold, using the given
    kernel size. This amounts to windowing @t using overlapping windows
    of size @kernel_size, with overlap of @kernel_size // 2.

    Args:
        t: A tensor of shape [batch_size, sequence_length, num_channels].
        kernel_size: The kernel size.

    Returns:
        A tensor of shape [batch_size * (sequence_length - kernel_size)
        // (kernel_size // 2) + 1, kernel_size, num_channels].
    """
    t = t.unfold(dimension=1, size=kernel_size, step=kernel_size // 2)
    B, L, C, _ = t.shape
    t = t.reshape(B * L, C, kernel_size)
    t = t.transpose(1, 2)
    return t


class BaseModule(nn.Module):
    """Base class for all modules"""

    def __init__(self, *args, **kwargs):
        super(BaseModule, self).__init__()

    def forward(self, x: 'Any', *args, **kwargs) ->Any:
        raise NotImplementedError

    def __repr__(self):
        return '{}'.format(self.__class__.__name__)


def make_divisible(v: 'Union[float, int]', divisor: 'Optional[int]'=8, min_value: 'Optional[Union[float, int]]'=None) ->Union[float, int]:
    """
    This function is taken from the original tf repo.
    It ensures that all layers have a channel number that is divisible by 8
    It can be seen here:
    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py
    :param v:
    :param divisor:
    :param min_value:
    :return:
    """
    if min_value is None:
        min_value = divisor
    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)
    if new_v < 0.9 * v:
        new_v += divisor
    return new_v


class SqueezeExcitation(BaseModule):
    """
    This class defines the Squeeze-excitation module, in the `SENet paper <https://arxiv.org/abs/1709.01507>`_

    Args:
        opts: command-line arguments
        in_channels (int): :math:`C` from an expected input of size :math:`(N, C, H, W)`
        squeeze_factor (Optional[int]): Reduce :math:`C` by this factor. Default: 4
        squeeze_channels (Optional[int]): This module's output channels. Overrides squeeze_factor if specified
        scale_fn_name (Optional[str]): Scaling function name. Default: sigmoid

    Shape:
        - Input: :math:`(N, C, H, W)`
        - Output: :math:`(N, C, H, W)`
    """

    def __init__(self, opts, in_channels: 'int', squeeze_factor: 'Optional[int]'=4, squeeze_channels: 'Optional[int]'=None, scale_fn_name: 'Optional[str]'='sigmoid', *args, **kwargs) ->None:
        if squeeze_channels is None:
            squeeze_channels = max(make_divisible(in_channels // squeeze_factor, 8), 32)
        fc1 = ConvLayer2d(opts=opts, in_channels=in_channels, out_channels=squeeze_channels, kernel_size=1, stride=1, bias=True, use_norm=False, use_act=True)
        fc2 = ConvLayer2d(opts=opts, in_channels=squeeze_channels, out_channels=in_channels, kernel_size=1, stride=1, bias=True, use_norm=False, use_act=False)
        act_fn = build_activation_layer(opts, act_type=scale_fn_name, inplace=True)
        super().__init__()
        self.se_layer = nn.Sequential()
        self.se_layer.add_module(name='global_pool', module=AdaptiveAvgPool2d(output_size=1))
        self.se_layer.add_module(name='fc1', module=fc1)
        self.se_layer.add_module(name='fc2', module=fc2)
        self.se_layer.add_module(name='scale_act', module=act_fn)
        self.in_channels = in_channels
        self.squeeze_factor = squeeze_factor
        self.scale_fn = scale_fn_name

    def forward(self, x: 'Tensor', *args, **kwargs) ->Tensor:
        return x * self.se_layer(x)

    def __repr__(self) ->str:
        return '{}(in_channels={}, squeeze_factor={}, scale_fn={})'.format(self.__class__.__name__, self.in_channels, self.squeeze_factor, self.scale_fn)


class InvertedResidualSE(BaseModule):
    """
    This class implements the inverted residual block with squeeze-excitation unit, as described in
    `MobileNetv3 <https://arxiv.org/abs/1905.02244>`_ paper

    Args:
        opts: command-line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H_{in}, W_{in})`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H_{out}, W_{out)`
        expand_ratio (Union[int, float]): Expand the input channels by this factor in depth-wise conv
        dilation (Optional[int]): Use conv with dilation. Default: 1
        stride (Optional[int]): Use convolutions with a stride. Default: 1
        use_se (Optional[bool]): Use squeeze-excitation block. Default: False
        act_fn_name (Optional[str]): Activation function name. Default: relu
        se_scale_fn_name (Optional [str]): Scale activation function inside SE unit. Defaults to hard_sigmoid
        kernel_size (Optional[int]): Kernel size in depth-wise convolution. Defaults to 3.
        squeeze_factor (Optional[bool]): Squeezing factor in SE unit. Defaults to 4.

    Shape:
        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C_{out}, H_{out}, W_{out})`
    """

    def __init__(self, opts, in_channels: 'int', out_channels: 'int', expand_ratio: 'Union[int, float]', dilation: 'Optional[int]'=1, stride: 'Optional[int]'=1, use_se: 'Optional[bool]'=False, act_fn_name: 'Optional[str]'='relu', se_scale_fn_name: 'Optional[str]'='hard_sigmoid', kernel_size: 'Optional[int]'=3, squeeze_factor: 'Optional[int]'=4, *args, **kwargs) ->None:
        hidden_dim = make_divisible(int(round(in_channels * expand_ratio)), 8)
        act_fn = build_activation_layer(opts, act_type=act_fn_name, inplace=True)
        super().__init__()
        block = nn.Sequential()
        if expand_ratio != 1:
            block.add_module(name='exp_1x1', module=ConvLayer2d(opts, in_channels=in_channels, out_channels=hidden_dim, kernel_size=1, use_act=False, use_norm=True))
            block.add_module(name='act_fn_1', module=act_fn)
        block.add_module(name='conv_3x3', module=ConvLayer2d(opts, in_channels=hidden_dim, out_channels=hidden_dim, stride=stride, kernel_size=kernel_size, groups=hidden_dim, use_act=False, use_norm=True, dilation=dilation))
        block.add_module(name='act_fn_2', module=act_fn)
        if use_se:
            se = SqueezeExcitation(opts=opts, in_channels=hidden_dim, squeeze_factor=squeeze_factor, scale_fn_name=se_scale_fn_name)
            block.add_module(name='se', module=se)
        block.add_module(name='red_1x1', module=ConvLayer2d(opts, in_channels=hidden_dim, out_channels=out_channels, kernel_size=1, use_act=False, use_norm=True))
        self.block = block
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.exp = expand_ratio
        self.dilation = dilation
        self.use_se = use_se
        self.stride = stride
        self.act_fn_name = act_fn_name
        self.kernel_size = kernel_size
        self.use_res_connect = self.stride == 1 and in_channels == out_channels

    def forward(self, x: 'Tensor', *args, **kwargs) ->Tensor:
        y = self.block(x)
        return x + y if self.use_res_connect else y

    def __repr__(self) ->str:
        return '{}(in_channels={}, out_channels={}, stride={}, exp={}, dilation={}, use_se={}, kernel_size={}, act_fn={})'.format(self.__class__.__name__, self.in_channels, self.out_channels, self.stride, self.exp, self.dilation, self.use_se, self.kernel_size, self.act_fn_name)


class StochasticDepth(StochasticDepthTorch):
    """
    Implements the Stochastic Depth `"Deep Networks with Stochastic Depth"
    <https://arxiv.org/abs/1603.09382>`_ used for randomly dropping residual
    branches of residual architectures.
    """

    def __init__(self, p: 'float', mode: 'str') ->None:
        super().__init__(p=p, mode=mode)


class EfficientNetBlock(InvertedResidualSE):
    """
    This class implements a variant of the inverted residual block with squeeze-excitation unit,
    as described in `MobileNetv3 <https://arxiv.org/abs/1905.02244>`_ paper. This variant
    includes stochastic depth, as used in `EfficientNet <https://arxiv.org/abs/1905.11946>`_ paper.

    Args:
        stochastic_depth_prob: float,
        For other arguments, refer to the parent class.

    Shape:
        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C_{out}, H_{out}, W_{out})`
    """

    def __init__(self, stochastic_depth_prob: 'float', *args, **kwargs) ->None:
        super().__init__(*args, **kwargs)
        self.stochastic_depth = StochasticDepth(p=stochastic_depth_prob, mode='row')

    def forward(self, x: 'Tensor', *args, **kwargs) ->Tensor:
        y = self.block(x)
        if self.use_res_connect:
            y = self.stochastic_depth(y)
            y = y + x
        return y

    def __repr__(self) ->str:
        return super().__repr__()[:-1] + f', stochastic_depth_prob={self.stochastic_depth.p})'


@dataclass
class EfficientNetBlockConfig:
    """This class stores the config for each block in EfficientNet i.e. MBConv layers
    in Table 1 of `EfficientNet paper <https://arxiv.org/abs/1905.11946>`_
    Notably, this class takes width_mult and depth_mult as input too and adjusts
    layers' depth and width, as is required in different modes of EfficientNet.
    """

    def __init__(self, expand_ratio: 'float', kernel: 'int', stride: 'int', in_channels: 'int', out_channels: 'int', num_layers: 'int', width_mult: 'float', depth_mult: 'float'):
        self.expand_ratio = expand_ratio
        self.kernel = kernel
        self.stride = stride
        self.in_channels = int(make_divisible(in_channels * width_mult, 8))
        self.out_channels = int(make_divisible(out_channels * width_mult, 8))
        self.num_layers = int(math.ceil(num_layers * depth_mult))


class MobileOneBlock(BaseModule):
    """
    MobileOne building block.

    For more details, please refer to our paper:
    `An Improved One millisecond Mobile Backbone <https://arxiv.org/pdf/2206.04040.pdf>`
    """

    def __init__(self, opts: 'argparse.Namespace', in_channels: 'int', out_channels: 'int', kernel_size: 'int', stride: 'int'=1, padding: 'int'=0, dilation: 'int'=1, groups: 'int'=1, inference_mode: 'bool'=False, use_se: 'bool'=False, use_act: 'bool'=True, use_scale_branch: 'bool'=True, num_conv_branches: 'int'=1) ->None:
        """
        Construct a MobileOneBlock.

        Args:
            opts: Command line arguments.
            in_channels: Number of channels in the input.
            out_channels: Number of channels produced by the block.
            kernel_size: Size of the convolution kernel.
            stride: Stride size. Default: 1
            padding: Zero-padding size. Default: 0
            dilation: Kernel dilation factor. Default: 1
            groups: Group number. Default: 1
            inference_mode: If True, instantiates model in inference mode. Default: ``False``
            use_se: Whether to use SE-ReLU activations. Default: ``False``
            use_act: Whether to use activation. Default: ``True``
            use_scale_branch: Whether to use scale branch. Default: ``True``
            num_conv_branches: Number of linear conv branches. Default: 1
        """
        super(MobileOneBlock, self).__init__()
        self.inference_mode = inference_mode
        self.groups = groups
        self.stride = stride
        self.padding = padding
        self.dilation = dilation
        self.kernel_size = kernel_size
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.num_conv_branches = num_conv_branches
        if use_se:
            self.se = SqueezeExcitation(opts, out_channels, squeeze_factor=16)
        else:
            self.se = Identity()
        if use_act:
            self.activation = build_activation_layer(opts)
        else:
            self.activation = Identity()
        if inference_mode:
            self.reparam_conv = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=groups, bias=True)
        else:
            self.rbr_skip = BatchNorm2d(num_features=in_channels, affine=True) if out_channels == in_channels and stride == 1 else None
            if num_conv_branches > 0:
                rbr_conv = list()
                for _ in range(self.num_conv_branches):
                    rbr_conv.append(ConvLayer2d(opts, in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=kernel_size, stride=self.stride, padding=padding, groups=self.groups, bias=False, use_act=False))
                self.rbr_conv = nn.ModuleList(rbr_conv)
            else:
                self.rbr_conv = None
            self.rbr_scale = None
            if kernel_size > 1 and use_scale_branch:
                self.rbr_scale = ConvLayer2d(opts, in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=1, stride=self.stride, padding=0, groups=self.groups, bias=False, use_act=False)

    def forward(self, x: 'torch.Tensor', *args, **kwargs) ->torch.Tensor:
        """
        Forward pass implements inference logic for module
        before and after reparameterization.

        Args:
            x: Input tensor of shape :math:`(B, C, H, W)`.

        Returns:
            torch.Tensor of shape :math:`(B, C, H, W)`.
        """
        if self.inference_mode:
            return self.activation(self.se(self.reparam_conv(x)))
        identity_out = 0
        if self.rbr_skip is not None:
            identity_out = self.rbr_skip(x)
        scale_out = 0
        if self.rbr_scale is not None:
            scale_out = self.rbr_scale(x)
        out = scale_out + identity_out
        if self.rbr_conv is not None:
            for ix in range(self.num_conv_branches):
                out += self.rbr_conv[ix](x)
        return self.activation(self.se(out))

    def reparameterize(self) ->None:
        """
        Following works like `RepVGG: Making VGG-style ConvNets Great Again` -
        https://arxiv.org/pdf/2101.03697.pdf. We re-parameterize multi-branched
        architecture used at training time to obtain a plain CNN-like structure
        for inference.
        """
        if self.inference_mode:
            return
        kernel, bias = self._get_kernel_bias()
        self.reparam_conv = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding, dilation=self.dilation, groups=self.groups, bias=True)
        self.reparam_conv.weight.data = kernel
        self.reparam_conv.bias.data = bias
        for para in self.parameters():
            para.detach_()
        if hasattr(self, 'rbr_conv'):
            self.__delattr__('rbr_conv')
        if hasattr(self, 'rbr_scale'):
            self.__delattr__('rbr_scale')
        if hasattr(self, 'rbr_skip'):
            self.__delattr__('rbr_skip')
        self.inference_mode = True

    def _get_kernel_bias(self) ->Tuple[torch.Tensor, torch.Tensor]:
        """
        Method to obtain re-parameterized kernel and bias.
        Reference: https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py#L83

        Returns:
            Tuple of (kernel, bias) after fusing branches.
        """
        kernel_scale = 0
        bias_scale = 0
        if self.rbr_scale is not None:
            kernel_scale, bias_scale = self._fuse_branch_ops(self.rbr_scale.block)
            pad = self.kernel_size // 2
            kernel_scale = torch.nn.functional.pad(kernel_scale, [pad, pad, pad, pad])
        kernel_identity = 0
        bias_identity = 0
        if self.rbr_skip is not None:
            kernel_identity, bias_identity = self._fuse_branch_ops(self.rbr_skip)
        kernel_conv = 0
        bias_conv = 0
        if self.rbr_conv is not None:
            for ix in range(self.num_conv_branches):
                _kernel, _bias = self._fuse_branch_ops(self.rbr_conv[ix].block)
                kernel_conv += _kernel
                bias_conv += _bias
        kernel_final = kernel_conv + kernel_scale + kernel_identity
        bias_final = bias_conv + bias_scale + bias_identity
        return kernel_final, bias_final

    def _fuse_branch_ops(self, branch: 'Union[nn.Sequential, nn.BatchNorm2d]') ->Tuple[torch.Tensor, torch.Tensor]:
        """
        Method to fuse all linear ops in a branch.
        Reference: https://github.com/DingXiaoH/RepVGG/blob/main/repvgg.py#L95

        Args:
            branch: Sequence of ops to be fused.

        Returns:
            Tuple of (kernel, bias) after fusing batchnorm.
        """
        if isinstance(branch, nn.Sequential):
            kernel = branch.conv.weight
            return self._fuse_conv_bn(kernel, branch.norm)
        else:
            assert isinstance(branch, nn.BatchNorm2d)
            if not hasattr(self, 'id_tensor'):
                input_dim = self.in_channels // self.groups
                kernel_value = torch.zeros((self.in_channels, input_dim, self.kernel_size, self.kernel_size), dtype=branch.weight.dtype, device=branch.weight.device)
                for i in range(self.in_channels):
                    kernel_value[i, i % input_dim, self.kernel_size // 2, self.kernel_size // 2] = 1
                self.id_tensor = kernel_value
            kernel = self.id_tensor
            return self._fuse_conv_bn(kernel, branch)

    @staticmethod
    def _fuse_conv_bn(kernel: 'torch.Tensor', bn: 'nn.BatchNorm2d') ->Tuple[torch.Tensor, torch.Tensor]:
        """
        Method to fuse batchnorm layer with conv layer.

        Args:
            kernel: Convolutional kernel weights.
            bn: Batchnorm 2d layer.

        Returns:
            Tuple of (kernel, bias) after fusing batchnorm.
        """
        assert bn.affine, 'Expected BatchNorm layer to have affine parameters instead got BatchNorm layer without affine parameters.'
        running_mean = bn.running_mean
        running_var = bn.running_var
        gamma = bn.weight
        beta = bn.bias
        eps = bn.eps
        std = (running_var + eps).sqrt()
        t = (gamma / std).reshape(-1, 1, 1, 1)
        return kernel * t, beta - running_mean * gamma / std


class RepLKBlock(BaseModule):
    """
    This class defines overparameterized large kernel conv block in `RepLKNet <https://arxiv.org/abs/2203.06717>`_
    Reference: https://github.com/DingXiaoH/RepLKNet-pytorch

    Args:
        opts: Command-line arguments.
        in_channels: Number of input channels.
        out_channels: Number of output channels.
        kernel_size: Kernel size of the large kernel conv branch.
        stride: Stride size. Default: 1
        dilation: Kernel dilation factor. Default: 1
        groups: Group number. Default: 1
        small_kernel_size: Kernel size of small kernel conv branch.
        inference_mode: If True, instantiates model in inference mode. Default: ``False``
        use_act: If True, activation is used. Default: ``True``
    """

    def __init__(self, opts: 'argparse.Namespace', in_channels: 'int', out_channels: 'int', kernel_size: 'int', stride: 'int'=1, dilation: 'int'=1, groups: 'int'=1, small_kernel_size: 'int'=None, inference_mode: 'bool'=False, use_act: 'bool'=True) ->None:
        super().__init__()
        self.stride = stride
        self.groups = groups
        self.dilation = dilation
        self.in_channels = in_channels
        self.out_channels = out_channels
        if use_act:
            self.activation = build_activation_layer(opts)
        else:
            self.activation = Identity()
        self.kernel_size = kernel_size
        self.small_kernel_size = small_kernel_size
        self.padding = kernel_size // 2
        if inference_mode:
            self.lkb_reparam = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=self.padding, dilation=self.dilation, groups=groups, bias=True)
        else:
            self.lkb_origin = ConvLayer2d(opts, in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding, groups=self.groups, bias=False, use_act=False)
            if small_kernel_size is not None:
                assert small_kernel_size <= kernel_size, 'The kernel size for re-param cannot be larger than the large kernel'
                self.small_conv = ConvLayer2d(opts, in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=self.small_kernel_size, stride=self.stride, padding=self.small_kernel_size // 2, groups=self.groups, bias=False, use_act=False)

    def forward(self, x: 'torch.Tensor', *args, **kwargs) ->torch.Tensor:
        """
        Forward pass implements inference logic for module
        before and after reparameterization.

        Args:
            x: Input tensor of shape :math:`(B, C, H, W)`.

        Returns:
            torch.Tensor of shape :math:`(B, C, H, W)`.
        """
        if hasattr(self, 'lkb_reparam'):
            out = self.lkb_reparam(x)
        else:
            out = self.lkb_origin(x)
            if hasattr(self, 'small_conv'):
                out += self.small_conv(x)
        self.activation(out)
        return out

    def _get_kernel_bias(self) ->Tuple[torch.Tensor, torch.Tensor]:
        """
        Method to obtain re-parameterized kernel and bias.
        Reference: https://github.com/DingXiaoH/RepLKNet-pytorch

        Returns:
            Tuple of (kernel, bias) after fusing branches.
        """
        lk_kernel, lk_bias = MobileOneBlock._fuse_conv_bn(self.lkb_origin.block.conv.weight, self.lkb_origin.block.norm)
        if hasattr(self, 'small_conv'):
            sk_kernel, sk_bias = MobileOneBlock._fuse_conv_bn(self.small_conv.block.conv.weight, self.small_conv.block.norm)
            lk_bias += sk_bias
            lk_kernel += nn.functional.pad(sk_kernel, [(self.kernel_size - self.small_kernel_size) // 2] * 4)
        return lk_kernel, lk_bias

    def reparameterize(self) ->None:
        """
        Following works like `RepVGG: Making VGG-style ConvNets Great Again` -
        https://arxiv.org/pdf/2101.03697.pdf. We re-parameterize multi-branched
        architecture used at training time to obtain a plain CNN-like structure
        for inference.
        """
        kernel, bias = self._get_kernel_bias()
        self.lkb_reparam = nn.Conv2d(in_channels=self.in_channels, out_channels=self.out_channels, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding, dilation=self.dilation, groups=self.groups, bias=True)
        self.lkb_reparam.weight.data = kernel
        self.lkb_reparam.bias.data = bias
        self.__delattr__('lkb_origin')
        if hasattr(self, 'small_conv'):
            self.__delattr__('small_conv')


class PatchEmbed(BaseModule):
    """
    Convolutional Patch embedding layer.

    Args:
        opts: Command line arguments.
        patch_size: Patch size for embedding computation.
        stride: Stride for convolutional embedding layer.
        in_channels: Number of channels of input tensor.
        embed_dim: Number of embedding dimensions.
    """

    def __init__(self, opts: 'argparse.Namespace', patch_size: 'int', stride: 'int', in_channels: 'int', embed_dim: 'int'):
        super().__init__()
        inference_mode = getattr(opts, 'model.classification.fastvit.inference_mode')
        block = list()
        block.append(RepLKBlock(opts, in_channels=in_channels, out_channels=embed_dim, kernel_size=patch_size, stride=stride, groups=in_channels, small_kernel_size=3, inference_mode=inference_mode))
        block.append(MobileOneBlock(opts, in_channels=embed_dim, out_channels=embed_dim, kernel_size=1, stride=1, padding=0, groups=1, inference_mode=inference_mode, use_se=False, num_conv_branches=1))
        self.proj = nn.Sequential(*block)

    def forward(self, x: 'torch.Tensor', *args, **kwargs) ->torch.Tensor:
        """
        Forward pass

        Args:
            x: Input tensor of shape :math:`(B, C, H, W)`.

        Returns:
            torch.Tensor of shape :math:`(B, C, H//s, W//s)`,
            where `s` is the stride provide while instantiating the layer.
        """
        x = self.proj(x)
        return x


class ConvFFN(BaseModule):
    """
    Convolutional FFN Module.

    Args:
        opts: Command line arguments.
        in_channels: Number of input channels.
        hidden_channels: Number of channels after expansion. Default: None
        out_channels: Number of output channels. Default: None
        drop: Dropout rate. Default: ``0.0``.
    """

    def __init__(self, opts: 'argparse.Namespace', in_channels: 'int', hidden_channels: 'Optional[int]'=None, out_channels: 'Optional[int]'=None, drop: 'float'=0.0):
        super().__init__()
        out_channels = out_channels or in_channels
        hidden_channels = hidden_channels or in_channels
        self.conv = ConvLayer2d(opts, in_channels=in_channels, out_channels=in_channels, kernel_size=7, padding=3, groups=in_channels, bias=False, use_act=False)
        self.fc1 = ConvLayer2d(opts, in_channels, hidden_channels, kernel_size=1, use_norm=False, bias=True)
        self.fc2 = ConvLayer2d(opts, hidden_channels, out_channels, kernel_size=1, use_norm=False, use_act=False, bias=True)
        self.drop = nn.Dropout(drop)

    def forward(self, x: 'torch.Tensor', *args, **kwargs) ->torch.Tensor:
        """
        Forward pass

        Args:
            x: Input tensor of shape :math:`(B, C, H, W)`.

        Returns:
            torch.Tensor of shape :math:`(B, C, H, W)`.
        """
        x = self.conv(x)
        x = self.fc1(x)
        x = self.drop(x)
        x = self.fc2(x)
        x = self.drop(x)
        return x


class AttentionBlock(BaseModule):
    """
    Implementation of metaformer block with MHSA as token mixer.
    For more details on Metaformer structure, please refer to:
    `MetaFormer Is Actually What You Need for Vision <https://arxiv.org/pdf/2111.11418.pdf>`_

    Args:
        opts: Command line arguments.
        dim: Number of embedding dimensions.
        mlp_ratio: MLP expansion ratio. Default: 4.0
        drop: Dropout rate. Default: 0.0
        drop_path: Drop path rate. Default: 0.0
        use_layer_scale: Flag to turn on layer scale. Default: ``True``
        layer_scale_init_value: Layer scale value at initialization. Default: 1e-5
    """

    def __init__(self, opts: 'argparse.Namespace', dim: 'int', mlp_ratio: 'float'=4.0, drop: 'float'=0.0, drop_path: 'float'=0.0, use_layer_scale: 'bool'=True, layer_scale_init_value: 'float'=1e-05):
        super().__init__()
        self.norm = BatchNorm2d(num_features=dim)
        self.head_dim = 32
        num_heads = dim // self.head_dim
        self.token_mixer = MultiHeadAttention(embed_dim=dim, num_heads=num_heads, bias=False)
        assert mlp_ratio > 0, 'MLP ratio should be greater than 0, found: {}'.format(mlp_ratio)
        hidden_dim = int(dim * mlp_ratio)
        self.convffn = ConvFFN(opts, in_channels=dim, hidden_channels=hidden_dim, drop=drop)
        self.drop_path = StochasticDepth(drop_path, mode='row') if drop_path > 0.0 else nn.Identity()
        self.use_layer_scale = use_layer_scale
        if use_layer_scale:
            self.layer_scale_1 = nn.Parameter(layer_scale_init_value * torch.ones((dim, 1, 1)))
            self.layer_scale_2 = nn.Parameter(layer_scale_init_value * torch.ones((dim, 1, 1)))

    def _apply_mhsa(self, x: 'torch.Tensor') ->torch.Tensor:
        """
        Perform appropriate reshaping before and after MHSA block.

        Args:
            x: Input tensor of shape :math:`(B, C, H, W)`.

        Returns:
            torch.Tensor of shape :math:`(B, C, H, W)`.
        """
        x_norm = self.norm(x)
        B, C, H, W = x_norm.shape
        x_norm_reshaped = torch.flatten(x_norm, start_dim=2).transpose(-2, -1)
        out = self.token_mixer(x_norm_reshaped)
        out = out.transpose(-2, -1).reshape(B, C, H, W)
        return out

    def forward(self, x: 'torch.Tensor', *args, **kwargs) ->torch.Tensor:
        """
        Forward pass

        Args:
            x: Input tensor of shape :math:`(B, C, H, W)`.

        Returns:
            torch.Tensor output from the attention block.
        """
        if self.use_layer_scale:
            x = x + self.drop_path(self.layer_scale_1 * self._apply_mhsa(x))
            x = x + self.drop_path(self.layer_scale_2 * self.convffn(x))
        else:
            x = x + self.drop_path(self._apply_mhsa(x))
            x = x + self.drop_path(self.convffn(x))
        return x


class RepMixer(BaseModule):
    """
    Reparameterizable token mixer

    For more details, please refer to our paper:
    `FastViT: A Fast Hybrid Vision Transformer using Structural Reparameterization`

    Args:
        opts: Command line arguments.
        dim: Input feature map dimension. :math:`C_{in}` from an expected input of size :math:`(B, C_{in}, H, W)`.
        kernel_size: Kernel size for spatial mixing. Default: 3
        use_layer_scale: If True, learnable layer scale is used. Default: ``True``
        layer_scale_init_value: Initial value for layer scale. Default: 1e-5
        inference_mode: If True, instantiates model in inference mode. Default: ``False``
    """

    def __init__(self, opts: 'argparse.Namespace', dim: 'int', kernel_size: 'int'=3, use_layer_scale: 'bool'=True, layer_scale_init_value: 'float'=1e-05, inference_mode: 'bool'=False):
        super().__init__()
        self.dim = dim
        self.kernel_size = kernel_size
        if inference_mode:
            self.reparam_conv = nn.Conv2d(in_channels=self.dim, out_channels=self.dim, kernel_size=self.kernel_size, stride=1, padding=self.kernel_size // 2, groups=self.dim, bias=True)
        else:
            self.norm = MobileOneBlock(opts, dim, dim, kernel_size, padding=kernel_size // 2, groups=dim, use_act=False, use_scale_branch=False, num_conv_branches=0)
            self.mixer = MobileOneBlock(opts, dim, dim, kernel_size, padding=kernel_size // 2, groups=dim, use_act=False)
            self.use_layer_scale = use_layer_scale
            if use_layer_scale:
                self.layer_scale = nn.Parameter(layer_scale_init_value * torch.ones((dim, 1, 1)))

    def forward(self, x: 'torch.Tensor', *args, **kwargs) ->torch.Tensor:
        """
        Forward pass implements inference logic for module
        before and after reparameterization.

        Args:
            x: Input tensor of shape :math:`(B, C, H, W)`.

        Returns:
            torch.Tensor of shape :math:`(B, C, H, W)`.
        """
        if hasattr(self, 'reparam_conv'):
            x = self.reparam_conv(x)
            return x
        else:
            if self.use_layer_scale:
                x = x + self.layer_scale * (self.mixer(x) - self.norm(x))
            else:
                x = x + self.mixer(x) - self.norm(x)
            return x

    def reparameterize(self) ->None:
        """
        Reparameterize mixer and norm into a single
        convolutional layer for efficient inference.
        """
        self.mixer.reparameterize()
        self.norm.reparameterize()
        if self.use_layer_scale:
            w = self.mixer.id_tensor + self.layer_scale.unsqueeze(-1) * (self.mixer.reparam_conv.weight - self.norm.reparam_conv.weight)
            b = torch.squeeze(self.layer_scale) * (self.mixer.reparam_conv.bias - self.norm.reparam_conv.bias)
        else:
            w = self.mixer.id_tensor + self.mixer.reparam_conv.weight - self.norm.reparam_conv.weight
            b = self.mixer.reparam_conv.bias - self.norm.reparam_conv.bias
        self.reparam_conv = nn.Conv2d(in_channels=self.dim, out_channels=self.dim, kernel_size=self.kernel_size, stride=1, padding=self.kernel_size // 2, groups=self.dim, bias=True)
        self.reparam_conv.weight.data = w
        self.reparam_conv.bias.data = b
        for para in self.parameters():
            para.detach_()
        self.__delattr__('mixer')
        self.__delattr__('norm')


class RepMixerBlock(BaseModule):
    """
    Implementation of Metaformer block with RepMixer as token mixer.
    For more details on Metaformer structure, please refer to:
    `MetaFormer Is Actually What You Need for Vision <https://arxiv.org/pdf/2111.11418.pdf>`_

    Args:
        opts: Command line arguments.
        dim: Number of embedding dimensions.
        kernel_size: Kernel size for repmixer. Default: 3
        mlp_ratio: MLP expansion ratio. Default: 4.0
        drop: Dropout rate. Default: 0.0
        drop_path: Drop path rate. Default: 0.0
        use_layer_scale: Flag to turn on layer scale. Default: ``True``
        layer_scale_init_value: Layer scale value at initialization. Default: 1e-5
        inference_mode: Flag to instantiate block in inference mode. Default: ``False``
    """

    def __init__(self, opts: 'argparse.Namespace', dim: 'int', kernel_size: 'int'=3, mlp_ratio: 'float'=4.0, drop: 'float'=0.0, drop_path: 'float'=0.0, use_layer_scale: 'bool'=True, layer_scale_init_value: 'float'=1e-05, inference_mode: 'bool'=False):
        super().__init__()
        self.token_mixer = RepMixer(opts, dim=dim, kernel_size=kernel_size, use_layer_scale=use_layer_scale, layer_scale_init_value=layer_scale_init_value, inference_mode=inference_mode)
        assert mlp_ratio > 0, 'MLP ratio should be greater than 0, found: {}'.format(mlp_ratio)
        hidden_dim = int(dim * mlp_ratio)
        self.convffn = ConvFFN(opts, in_channels=dim, hidden_channels=hidden_dim, drop=drop)
        self.drop_path = StochasticDepth(drop_path, mode='row') if drop_path > 0.0 else nn.Identity()
        self.use_layer_scale = use_layer_scale
        if use_layer_scale:
            self.layer_scale = nn.Parameter(layer_scale_init_value * torch.ones((dim, 1, 1)))

    def forward(self, x: 'torch.Tensor', *args, **kwargs) ->torch.Tensor:
        """
        Forward pass

        Args:
            x: Input tensor of shape :math:`(B, C, H, W)`.

        Returns:
            torch.Tensor of shape :math:`(B, C, H, W)`.
        """
        if self.use_layer_scale:
            x = self.token_mixer(x)
            x = x + self.drop_path(self.layer_scale * self.convffn(x))
        else:
            x = self.token_mixer(x)
            x = x + self.drop_path(self.convffn(x))
        return x


def basic_blocks(opts: 'argparse.Namespace', dim: 'int', block_index: 'int', num_blocks: 'List[int]', token_mixer_type: 'str', kernel_size: 'int'=3, mlp_ratio: 'float'=4.0, drop_rate: 'float'=0.0, drop_path_rate: 'float'=0.0, inference_mode: 'bool'=False, use_layer_scale: 'bool'=True, layer_scale_init_value: 'float'=1e-05) ->nn.Sequential:
    """Build FastViT blocks within a stage.

    Args:
        opts: Command line arguments.
        dim: Number of embedding dimensions.
        block_index: block index.
        num_blocks: List containing number of blocks per stage.
        token_mixer_type: Token mixer type.
        kernel_size: Kernel size for repmixer.
        mlp_ratio: MLP expansion ratio.
        drop_rate: Dropout rate.
        drop_path_rate: Drop path rate.
        inference_mode: Flag to instantiate block in inference mode.
        use_layer_scale: Flag to turn on layer scale regularization.
        layer_scale_init_value: Layer scale value at initialization.

    Returns:
        nn.Sequential object of all the blocks within the stage.
    """
    blocks = []
    for block_idx in range(num_blocks[block_index]):
        block_dpr = drop_path_rate * (block_idx + sum(num_blocks[:block_index])) / (sum(num_blocks) - 1)
        if token_mixer_type == 'repmixer':
            blocks.append(RepMixerBlock(opts, dim, kernel_size=kernel_size, mlp_ratio=mlp_ratio, drop=drop_rate, drop_path=block_dpr, inference_mode=inference_mode, use_layer_scale=use_layer_scale, layer_scale_init_value=layer_scale_init_value))
        elif token_mixer_type == 'attention':
            blocks.append(AttentionBlock(opts, dim, mlp_ratio=mlp_ratio, drop=drop_rate, drop_path=block_dpr, use_layer_scale=use_layer_scale, layer_scale_init_value=layer_scale_init_value))
        else:
            raise ValueError('Token mixer type: {} not supported'.format(token_mixer_type))
    blocks = nn.Sequential(*blocks)
    return blocks


def convolutional_stem(opts: 'argparse.Namespace', in_channels: 'int', out_channels: 'int') ->nn.Sequential:
    """
    Build convolutional stem with MobileOne blocks.

    Args:
        opts: Command line arguments.
        in_channels: Number of input channels.
        out_channels: Number of output channels.

    Returns:
        nn.Sequential object with stem elements.
    """
    inference_mode = getattr(opts, 'model.classification.fastvit.inference_mode')
    return nn.Sequential(MobileOneBlock(opts, in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=2, padding=1, groups=1, inference_mode=inference_mode, use_se=False, num_conv_branches=1), MobileOneBlock(opts, in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=2, padding=1, groups=out_channels, inference_mode=inference_mode, use_se=False, num_conv_branches=1), MobileOneBlock(opts, in_channels=out_channels, out_channels=out_channels, kernel_size=1, stride=1, padding=0, groups=1, inference_mode=inference_mode, use_se=False, num_conv_branches=1))


class InvertedResidual(BaseModule):
    """
    This class implements the inverted residual block, as described in `MobileNetv2 <https://arxiv.org/abs/1801.04381>`_ paper

    Args:
        opts: command-line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H_{in}, W_{in})`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H_{out}, W_{out)`
        stride (Optional[int]): Use convolutions with a stride. Default: 1
        expand_ratio (Union[int, float]): Expand the input channels by this factor in depth-wise conv
        dilation (Optional[int]): Use conv with dilation. Default: 1
        skip_connection (Optional[bool]): Use skip-connection. Default: True

    Shape:
        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C_{out}, H_{out}, W_{out})`

    .. note::
        If `in_channels =! out_channels` and `stride > 1`, we set `skip_connection=False`

    """

    def __init__(self, opts, in_channels: 'int', out_channels: 'int', stride: 'int', expand_ratio: 'Union[int, float]', dilation: 'int'=1, skip_connection: 'Optional[bool]'=True, *args, **kwargs) ->None:
        assert stride in [1, 2]
        hidden_dim = make_divisible(int(round(in_channels * expand_ratio)), 8)
        super().__init__()
        block = nn.Sequential()
        if expand_ratio != 1:
            block.add_module(name='exp_1x1', module=ConvLayer2d(opts, in_channels=in_channels, out_channels=hidden_dim, kernel_size=1, use_act=True, use_norm=True))
        block.add_module(name='conv_3x3', module=ConvLayer2d(opts, in_channels=hidden_dim, out_channels=hidden_dim, stride=stride, kernel_size=3, groups=hidden_dim, use_act=True, use_norm=True, dilation=dilation))
        block.add_module(name='red_1x1', module=ConvLayer2d(opts, in_channels=hidden_dim, out_channels=out_channels, kernel_size=1, use_act=False, use_norm=True))
        self.block = block
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.exp = expand_ratio
        self.dilation = dilation
        self.stride = stride
        self.use_res_connect = self.stride == 1 and in_channels == out_channels and skip_connection

    def forward(self, x: 'Tensor', *args, **kwargs) ->Tensor:
        if self.use_res_connect:
            return x + self.block(x)
        else:
            return self.block(x)

    def __repr__(self) ->str:
        return '{}(in_channels={}, out_channels={}, stride={}, exp={}, dilation={}, skip_conn={})'.format(self.__class__.__name__, self.in_channels, self.out_channels, self.stride, self.exp, self.dilation, self.use_res_connect)


class TransformerEncoder(BaseModule):
    """
    This class defines the pre-norm `Transformer encoder <https://arxiv.org/abs/1706.03762>`_
    Args:
        opts: Command line arguments.
        embed_dim: :math:`C_{in}` from an expected input of size :math:`(N, P, C_{in})`.
        ffn_latent_dim: Inner dimension of the FFN.
        num_heads: Number of heads in multi-head attention. Default: 8.
        attn_dropout: Dropout rate for attention in multi-head attention. Default: 0.0
        dropout: Dropout rate. Default: 0.0.
        ffn_dropout: Dropout between FFN layers. Default: 0.0.
        transformer_norm_layer: Normalization layer. Default: layer_norm.
        stochastic_dropout: Stochastic dropout setting. Default: 0.0.

    Shape:
        - Input: :math:`(N, P, C_{in})` where :math:`N` is batch size, :math:`P` is number of patches,
        and :math:`C_{in}` is input embedding dim
        - Output: same shape as the input
    """

    def __init__(self, opts: 'argparse.Namespace', embed_dim: 'int', ffn_latent_dim: 'int', num_heads: 'Optional[int]'=8, attn_dropout: 'Optional[float]'=0.0, dropout: 'Optional[float]'=0.0, ffn_dropout: 'Optional[float]'=0.0, transformer_norm_layer: 'Optional[str]'='layer_norm', stochastic_dropout: 'Optional[float]'=0.0, *args, **kwargs) ->None:
        super().__init__()
        attn_unit = SingleHeadAttention(embed_dim=embed_dim, attn_dropout=attn_dropout, bias=True)
        if num_heads > 1:
            attn_unit = MultiHeadAttention(embed_dim, num_heads, attn_dropout=attn_dropout, bias=True, coreml_compatible=getattr(opts, 'common.enable_coreml_compatible_module', False))
        self.pre_norm_mha = nn.Sequential(get_normalization_layer(opts=opts, norm_type=transformer_norm_layer, num_features=embed_dim), attn_unit, Dropout(p=dropout))
        act_name = build_activation_layer(opts, num_parameters=1)
        self.pre_norm_ffn = nn.Sequential(get_normalization_layer(opts=opts, norm_type=transformer_norm_layer, num_features=embed_dim), LinearLayer(in_features=embed_dim, out_features=ffn_latent_dim, bias=True), act_name, Dropout(p=ffn_dropout), LinearLayer(in_features=ffn_latent_dim, out_features=embed_dim, bias=True), Dropout(p=dropout))
        self.drop_path = Identity()
        if stochastic_dropout > 0.0:
            if dropout > 0.0:
                logger.error('Stochastic dropout and dropout are mutually exclusive. Use either of them, but not both.Got: {} and {}'.format(stochastic_dropout, dropout))
            self.drop_path = StochasticDepth(p=stochastic_dropout, mode='row')
        self.embed_dim = embed_dim
        self.ffn_dim = ffn_latent_dim
        self.ffn_dropout = ffn_dropout
        self.stochastic_dropout = stochastic_dropout
        self.std_dropout = dropout
        self.attn_fn_name = attn_unit.__class__.__name__
        self.act_fn_name = act_name.__class__.__name__
        self.norm_type = transformer_norm_layer

    def __repr__(self) ->str:
        return '{}(embed_dim={}, ffn_dim={}, dropout={}, ffn_dropout={}, stochastic_dropout={}, attn_fn={}, act_fn={}, norm_fn={})'.format(self.__class__.__name__, self.embed_dim, self.ffn_dim, self.std_dropout, self.ffn_dropout, self.stochastic_dropout, self.attn_fn_name, self.act_fn_name, self.norm_type)

    def forward(self, x: 'Tensor', x_prev: 'Optional[Tensor]'=None, key_padding_mask: 'Optional[Tensor]'=None, attn_mask: 'Optional[Tensor]'=None, *args, **kwargs) ->Tensor:
        res = x
        x = self.pre_norm_mha[0](x)
        x = self.pre_norm_mha[1](*args, x_q=x, x_kv=x_prev, key_padding_mask=key_padding_mask, attn_mask=attn_mask, **kwargs)
        x = self.drop_path(self.pre_norm_mha[2](x))
        x = x + res
        x = x + self.drop_path(self.pre_norm_ffn(x))
        return x


class MobileViTBlock(BaseModule):
    """
    This class defines the `MobileViT block <https://arxiv.org/abs/2110.02178?context=cs.LG>`_

    Args:
        opts: command line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H, W)`
        transformer_dim (int): Input dimension to the transformer unit
        ffn_dim (int): Dimension of the FFN block
        n_transformer_blocks (Optional[int]): Number of transformer blocks. Default: 2
        head_dim (Optional[int]): Head dimension in the multi-head attention. Default: 32
        attn_dropout (Optional[float]): Dropout in multi-head attention. Default: 0.0
        dropout (Optional[float]): Dropout rate. Default: 0.0
        ffn_dropout (Optional[float]): Dropout between FFN layers in transformer. Default: 0.0
        patch_h (Optional[int]): Patch height for unfolding operation. Default: 8
        patch_w (Optional[int]): Patch width for unfolding operation. Default: 8
        transformer_norm_layer (Optional[str]): Normalization layer in the transformer block. Default: layer_norm
        conv_ksize (Optional[int]): Kernel size to learn local representations in MobileViT block. Default: 3
        dilation (Optional[int]): Dilation rate in convolutions. Default: 1
        no_fusion (Optional[bool]): Do not combine the input and output feature maps. Default: False
    """

    def __init__(self, opts, in_channels: 'int', transformer_dim: 'int', ffn_dim: 'int', n_transformer_blocks: 'Optional[int]'=2, head_dim: 'Optional[int]'=32, attn_dropout: 'Optional[float]'=0.0, dropout: 'Optional[int]'=0.0, ffn_dropout: 'Optional[int]'=0.0, patch_h: 'Optional[int]'=8, patch_w: 'Optional[int]'=8, transformer_norm_layer: 'Optional[str]'='layer_norm', conv_ksize: 'Optional[int]'=3, dilation: 'Optional[int]'=1, no_fusion: 'Optional[bool]'=False, *args, **kwargs) ->None:
        conv_3x3_in = ConvLayer2d(opts=opts, in_channels=in_channels, out_channels=in_channels, kernel_size=conv_ksize, stride=1, use_norm=True, use_act=True, dilation=dilation)
        conv_1x1_in = ConvLayer2d(opts=opts, in_channels=in_channels, out_channels=transformer_dim, kernel_size=1, stride=1, use_norm=False, use_act=False)
        conv_1x1_out = ConvLayer2d(opts=opts, in_channels=transformer_dim, out_channels=in_channels, kernel_size=1, stride=1, use_norm=True, use_act=True)
        conv_3x3_out = None
        if not no_fusion:
            conv_3x3_out = ConvLayer2d(opts=opts, in_channels=2 * in_channels, out_channels=in_channels, kernel_size=conv_ksize, stride=1, use_norm=True, use_act=True)
        super().__init__()
        self.local_rep = nn.Sequential()
        self.local_rep.add_module(name='conv_3x3', module=conv_3x3_in)
        self.local_rep.add_module(name='conv_1x1', module=conv_1x1_in)
        assert transformer_dim % head_dim == 0
        num_heads = transformer_dim // head_dim
        global_rep = [TransformerEncoder(opts=opts, embed_dim=transformer_dim, ffn_latent_dim=ffn_dim, num_heads=num_heads, attn_dropout=attn_dropout, dropout=dropout, ffn_dropout=ffn_dropout, transformer_norm_layer=transformer_norm_layer) for _ in range(n_transformer_blocks)]
        global_rep.append(get_normalization_layer(opts=opts, norm_type=transformer_norm_layer, num_features=transformer_dim))
        self.global_rep = nn.Sequential(*global_rep)
        self.conv_proj = conv_1x1_out
        self.fusion = conv_3x3_out
        self.patch_h = patch_h
        self.patch_w = patch_w
        self.patch_area = self.patch_w * self.patch_h
        self.cnn_in_dim = in_channels
        self.cnn_out_dim = transformer_dim
        self.n_heads = num_heads
        self.ffn_dim = ffn_dim
        self.dropout = dropout
        self.attn_dropout = attn_dropout
        self.ffn_dropout = ffn_dropout
        self.dilation = dilation
        self.n_blocks = n_transformer_blocks
        self.conv_ksize = conv_ksize

    def __repr__(self) ->str:
        repr_str = '{}('.format(self.__class__.__name__)
        repr_str += '\n\t Local representations'
        if isinstance(self.local_rep, nn.Sequential):
            for m in self.local_rep:
                repr_str += '\n\t\t {}'.format(m)
        else:
            repr_str += '\n\t\t {}'.format(self.local_rep)
        repr_str += '\n\t Global representations with patch size of {}x{}'.format(self.patch_h, self.patch_w)
        if isinstance(self.global_rep, nn.Sequential):
            for m in self.global_rep:
                repr_str += '\n\t\t {}'.format(m)
        else:
            repr_str += '\n\t\t {}'.format(self.global_rep)
        if isinstance(self.conv_proj, nn.Sequential):
            for m in self.conv_proj:
                repr_str += '\n\t\t {}'.format(m)
        else:
            repr_str += '\n\t\t {}'.format(self.conv_proj)
        if self.fusion is not None:
            repr_str += '\n\t Feature fusion'
            if isinstance(self.fusion, nn.Sequential):
                for m in self.fusion:
                    repr_str += '\n\t\t {}'.format(m)
            else:
                repr_str += '\n\t\t {}'.format(self.fusion)
        repr_str += '\n)'
        return repr_str

    def unfolding(self, feature_map: 'Tensor') ->Tuple[Tensor, Dict]:
        patch_w, patch_h = self.patch_w, self.patch_h
        patch_area = int(patch_w * patch_h)
        batch_size, in_channels, orig_h, orig_w = feature_map.shape
        new_h = int(math.ceil(orig_h / self.patch_h) * self.patch_h)
        new_w = int(math.ceil(orig_w / self.patch_w) * self.patch_w)
        interpolate = False
        if new_w != orig_w or new_h != orig_h:
            feature_map = F.interpolate(feature_map, size=(new_h, new_w), mode='bilinear', align_corners=False)
            interpolate = True
        num_patch_w = new_w // patch_w
        num_patch_h = new_h // patch_h
        num_patches = num_patch_h * num_patch_w
        reshaped_fm = feature_map.reshape(batch_size * in_channels * num_patch_h, patch_h, num_patch_w, patch_w)
        transposed_fm = reshaped_fm.transpose(1, 2)
        reshaped_fm = transposed_fm.reshape(batch_size, in_channels, num_patches, patch_area)
        transposed_fm = reshaped_fm.transpose(1, 3)
        patches = transposed_fm.reshape(batch_size * patch_area, num_patches, -1)
        info_dict = {'orig_size': (orig_h, orig_w), 'batch_size': batch_size, 'interpolate': interpolate, 'total_patches': num_patches, 'num_patches_w': num_patch_w, 'num_patches_h': num_patch_h}
        return patches, info_dict

    def folding(self, patches: 'Tensor', info_dict: 'Dict') ->Tensor:
        n_dim = patches.dim()
        assert n_dim == 3, 'Tensor should be of shape BPxNxC. Got: {}'.format(patches.shape)
        patches = patches.contiguous().view(info_dict['batch_size'], self.patch_area, info_dict['total_patches'], -1)
        batch_size, pixels, num_patches, channels = patches.size()
        num_patch_h = info_dict['num_patches_h']
        num_patch_w = info_dict['num_patches_w']
        patches = patches.transpose(1, 3)
        feature_map = patches.reshape(batch_size * channels * num_patch_h, num_patch_w, self.patch_h, self.patch_w)
        feature_map = feature_map.transpose(1, 2)
        feature_map = feature_map.reshape(batch_size, channels, num_patch_h * self.patch_h, num_patch_w * self.patch_w)
        if info_dict['interpolate']:
            feature_map = F.interpolate(feature_map, size=info_dict['orig_size'], mode='bilinear', align_corners=False)
        return feature_map

    def forward_spatial(self, x: 'Tensor') ->Tensor:
        res = x
        fm = self.local_rep(x)
        patches, info_dict = self.unfolding(fm)
        for transformer_layer in self.global_rep:
            patches = transformer_layer(patches)
        fm = self.folding(patches=patches, info_dict=info_dict)
        fm = self.conv_proj(fm)
        if self.fusion is not None:
            fm = self.fusion(torch.cat((res, fm), dim=1))
        return fm

    def forward_temporal(self, x: 'Tensor', x_prev: 'Optional[Tensor]'=None) ->Union[Tensor, Tuple[Tensor, Tensor]]:
        res = x
        fm = self.local_rep(x)
        patches, info_dict = self.unfolding(fm)
        for global_layer in self.global_rep:
            if isinstance(global_layer, TransformerEncoder):
                patches = global_layer(x=patches, x_prev=x_prev)
            else:
                patches = global_layer(patches)
        fm = self.folding(patches=patches, info_dict=info_dict)
        fm = self.conv_proj(fm)
        if self.fusion is not None:
            fm = self.fusion(torch.cat((res, fm), dim=1))
        return fm, patches

    def forward(self, x: 'Union[Tensor, Tuple[Tensor]]', *args, **kwargs) ->Union[Tensor, Tuple[Tensor, Tensor]]:
        if isinstance(x, Tuple) and len(x) == 2:
            return self.forward_temporal(x=x[0], x_prev=x[1])
        elif isinstance(x, Tensor):
            return self.forward_spatial(x)
        else:
            raise NotImplementedError


class XRegNetBlock(BaseModule):
    """
    This class implements the `X` block based on the ResNet bottleneck block. See figure 4 of RegNet
    paper `RegNet model <https://arxiv.org/pdf/2003.13678.pdf>`_

    Args:
        opts: command-line arguments
        width_in: The number of input channels
        width_out: The number of output channels
        stride: Stride for convolution
        groups: Number of groups for convolution
        bottleneck_multiplier: The number of in/out channels of the intermediate
            conv layer will be scaled by this value
        se_ratio: The numer squeeze-excitation ratio. The number of channels in the SE
            module will be scaled by this value
        stochastic_depth_prob: The stochastic depth probability
    """

    def __init__(self, opts: 'argparse.Namespace', width_in: 'int', width_out: 'int', stride: 'int', groups: 'int', bottleneck_multiplier: 'float', se_ratio: 'float', stochastic_depth_prob: 'float'=0.0) ->None:
        super().__init__()
        bottleneck_width = int(round(width_out * bottleneck_multiplier))
        bottleneck_groups = bottleneck_width // groups
        conv_1x1_1 = ConvLayer2d(opts=opts, in_channels=width_in, out_channels=bottleneck_width, kernel_size=1, stride=1, use_norm=True, use_act=True)
        conv_3x3 = ConvLayer2d(opts=opts, in_channels=bottleneck_width, out_channels=bottleneck_width, kernel_size=3, stride=stride, groups=bottleneck_groups, use_norm=True, use_act=True)
        se = Identity()
        if se_ratio > 0:
            squeeze_channels = int(round(se_ratio * width_in))
            se = SqueezeExcitation(opts, in_channels=bottleneck_width, squeeze_channels=squeeze_channels)
        conv_1x1_2 = ConvLayer2d(opts=opts, in_channels=bottleneck_width, out_channels=width_out, kernel_size=1, stride=1, use_norm=True, use_act=True)
        block = nn.Sequential()
        block.add_module('conv_1x1_1', module=conv_1x1_1)
        block.add_module('conv_3x3', module=conv_3x3)
        block.add_module('se', module=se)
        block.add_module('conv_1x1_2', module=conv_1x1_2)
        down_sample = Identity()
        if stride != 1 or width_out != width_in:
            down_sample = ConvLayer2d(opts, in_channels=width_in, out_channels=width_out, kernel_size=1, stride=stride, use_act=False)
        act_type = getattr(opts, 'model.activation.name')
        neg_slope = getattr(opts, 'model.activation.neg_slope')
        inplace = getattr(opts, 'model.activation.inplace')
        final_act = build_activation_layer(opts=opts, act_type=act_type, inplace=inplace, negative_slope=neg_slope, num_parameters=width_out)
        self.stochastic_depth = StochasticDepth(p=stochastic_depth_prob, mode='row')
        self.block = block
        self.down_sample = down_sample
        self.final_act = final_act
        self.width_in = width_in
        self.width_out = width_out
        self.stride = stride
        self.groups = groups
        self.bottleneck_multiplier = bottleneck_multiplier
        self.se_ratio = se_ratio
        self.stochastic_depth_prob = stochastic_depth_prob

    def forward(self, x: 'Tensor') ->Tensor:
        """Forward pass for XRegNetBlock.

        Args:
            x: Batch of images

        Retruns:
            * output of XRegNetBlock including stochastic depth layer and
                residual.

        Shape:
            x: :math:`(N, C_{in}, H_{in}, W_{in})`
            Output: :math:`(N, C_{out}, H_{out}, W_{out})`
        """
        out = self.block(x)
        out = self.stochastic_depth(out)
        res = self.down_sample(x)
        out = res + out
        return self.final_act(out)

    def __repr__(self) ->str:
        return '{}(width_in={}, width_out={}, stride={}, groups={}, bottleneck_multiplier={}, se_ratio={}, stochastic_depth_prob={})'.format(self.__class__.__name__, self.width_in, self.width_out, self.stride, self.groups, self.bottleneck_multiplier, self.se_ratio, self.stochastic_depth_prob)


class AnyRegNetStage(BaseModule):
    """
    This class implements a 'stage' as defined in the `RegNet paper <https://arxiv.org/pdf/2003.13678.pdf>`_.
    It consists of a sequence of bottleneck blocks.

    Args:
        opts: command-line arguments
        depth: The number of XRegNetBlocks in the stage
        width_in: The number of input channels of the first block
        width_out: The number of output channels of each block
        stride: Stride for convolution of first block
        groups: Number of groups for the intermediate convolution (bottleneck) layer in each block
        bottleneck_multiplier: The number of in/out channels of the intermediate
            conv layer of each block will be scaled by this value
        se_ratio: The numer squeeze-excitation ratio. The number of channels in the SE
            module of each block will be scaled by this value
        stage_depths: A list of the number of blocks in each stage
        stage_index: The index of the current stage being constructed
        stochastic_depth_prob: The stochastic depth probability
    """

    def __init__(self, opts: 'argparse.Namespace', depth: 'int', width_in: 'int', width_out: 'int', stride: 'int', groups: 'int', bottleneck_multiplier: 'float', se_ratio: 'float', stage_index: 'int', stochastic_depth_probs: 'List[float]') ->None:
        super().__init__()
        stage_blocks = nn.Sequential()
        for i, sd_prob in enumerate(stochastic_depth_probs):
            block = XRegNetBlock(opts, width_in=width_in if i == 0 else width_out, width_out=width_out, stride=stride if i == 0 else 1, groups=groups, bottleneck_multiplier=bottleneck_multiplier, se_ratio=se_ratio, stochastic_depth_prob=sd_prob)
            stage_blocks.add_module(f'Stage{stage_index}-Block{i}', module=block)
        self.stage = stage_blocks
        self.depth = depth
        self.width_in = width_in
        self.width_out = width_out
        self.stride = stride
        self.groups = groups
        self.bottleneck_multiplier = bottleneck_multiplier
        self.se_ratio = se_ratio
        self.stage_index = stage_index
        self.stochastic_depth_probs = stochastic_depth_probs

    def forward(self, x: 'Tensor') ->Tensor:
        """Forward pass through all blocks in the stage.

        Args:
            x: Batch of images.

        Returns:
            * output of passing x through all blocks in the stage.

        Shape:
            x: :math:`(N, C_{in}, H_{in}, W_{in})`
            Output: :math:`(N, C_{out}, H_{out}, W_{out})`
        """
        return self.stage(x)

    def __repr__(self) ->str:
        return '{}(depth={}, width_in={}, width_out={}, stride={}, groups={}, bottleneck_multiplier={}, se_ratio={}, stage_index={}, stochastic_depth_probs={})'.format(self.__class__.__name__, self.depth, self.width_in, self.width_out, self.stride, self.groups, self.bottleneck_multiplier, self.se_ratio, self.stage_index, self.stochastic_depth_probs)


supported_modes = ['x_200mf', 'x_400mf', 'x_600mf', 'x_800mf', 'x_1.6gf', 'x_3.2gf', 'x_4.0gf', 'x_6.4gf', 'x_8.0gf', 'x_12gf', 'x_16gf', 'x_32gf', 'y_200mf', 'y_400mf', 'y_800mf', 'y_600mf', 'y_1.6gf', 'y_3.2gf', 'y_4.0gf', 'y_6.4gf', 'y_8.0gf', 'y_12gf', 'y_16gf', 'y_32gf']


class BasicResNetBlock(BaseModule):
    """
    This class defines the Basic block in the `ResNet model <https://arxiv.org/abs/1512.03385>`_
    Args:
        opts: command-line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H_{in}, W_{in})`
        mid_channels (int): :math:`C_{mid}` from an expected tensor of size :math:`(N, C_{mid}, H_{out}, W_{out})`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H_{out}, W_{out})`
        stride (Optional[int]): Stride for convolution. Default: 1
        dilation (Optional[int]): Dilation for convolution. Default: 1
        dropout (Optional[float]): Dropout after second convolution. Default: 0.0
        stochastic_depth_prob (Optional[float]): Stochastic depth drop probability (1 - survival_prob). Default: 0.0
        squeeze_channels (Optional[int]): The number of channels to use in the Squeeze-Excitation block for SE-ResNet.
            Default: None.

    Shape:
        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C_{out}, H_{out}, W_{out})`

    """
    expansion: 'int' = 1

    def __init__(self, opts: 'argparse.Namespace', in_channels: 'int', mid_channels: 'int', out_channels: 'int', stride: 'Optional[int]'=1, dilation: 'Optional[int]'=1, dropout: 'Optional[float]'=0.0, stochastic_depth_prob: 'Optional[float]'=0.0, squeeze_channels: 'Optional[int]'=None, *args, **kwargs) ->None:
        act_type = getattr(opts, 'model.activation.name')
        neg_slope = getattr(opts, 'model.activation.neg_slope')
        inplace = getattr(opts, 'model.activation.inplace')
        cbr_1 = ConvLayer2d(opts=opts, in_channels=in_channels, out_channels=mid_channels, kernel_size=3, stride=stride, dilation=dilation, use_norm=True, use_act=True)
        cb_2 = ConvLayer2d(opts=opts, in_channels=mid_channels, out_channels=out_channels, kernel_size=3, stride=1, use_norm=True, use_act=False, dilation=dilation)
        block = nn.Sequential()
        block.add_module(name='conv_batch_act_1', module=cbr_1)
        block.add_module(name='conv_batch_2', module=cb_2)
        if 0.0 < dropout < 1.0:
            block.add_module(name='dropout', module=Dropout(p=dropout))
        down_sample = Identity()
        if stride == 2:
            down_sample = ConvLayer2d(opts=opts, in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, use_norm=True, use_act=False)
        se_block = Identity()
        if squeeze_channels is not None:
            se_block = SqueezeExcitation(opts=opts, in_channels=out_channels, squeeze_channels=squeeze_channels)
        super().__init__()
        self.block = block
        self.down_sample = down_sample
        self.final_act = build_activation_layer(opts, act_type=act_type, inplace=inplace, negative_slope=neg_slope, num_parameters=out_channels)
        self.stochastic_depth = StochasticDepth(p=stochastic_depth_prob, mode='row')
        self.se_block = se_block
        self.stride = stride
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.dilation = dilation
        self.dropout = dropout
        self.stochastic_depth_prob = stochastic_depth_prob
        self.squeeze_channels = squeeze_channels

    def forward(self, x: 'Tensor', *args, **kwargs) ->Tensor:
        out = self.block(x)
        out = self.se_block(out)
        res = self.down_sample(x)
        out = self.stochastic_depth(out)
        out = out + res
        return self.final_act(out)

    def __repr__(self) ->str:
        return '{}(in_channels={}, out_channels={}, stride={}, dilation={}, dropout={}, stochastic_depth_prob={}, squeeze_channels={})'.format(self.__class__.__name__, self.in_channels, self.out_channels, self.stride, self.dilation, self.dropout, self.stochastic_depth_prob, self.squeeze_channels)


class BottleneckResNetBlock(BaseModule):
    """
    This class defines the Bottleneck block in the `ResNet model <https://arxiv.org/abs/1512.03385>`_
    Args:
        opts: command-line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H_{in}, W_{in})`
        mid_channels (int): :math:`C_{mid}` from an expected tensor of size :math:`(N, C_{mid}, H_{out}, W_{out})`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H_{out}, W_{out})`
        stride (Optional[int]): Stride for convolution. Default: 1
        dilation (Optional[int]): Dilation for convolution. Default: 1
        dropout (Optional[float]): Dropout after third convolution. Default: 0.0
        stochastic_depth_prob (Optional[float]): Stochastic depth drop probability (1 - survival_prob). Default: 0.0
        squeeze_channels (Optional[int]): The number of channels to use in the Squeeze-Excitation block for SE-ResNet.

    Shape:
        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C_{out}, H_{out}, W_{out})`

    """
    expansion: 'int' = 4

    def __init__(self, opts: 'argparse.Namespace', in_channels: 'int', mid_channels: 'int', out_channels: 'int', stride: 'Optional[int]'=1, dilation: 'Optional[int]'=1, dropout: 'Optional[float]'=0.0, stochastic_depth_prob: 'Optional[float]'=0.0, squeeze_channels: 'Optional[int]'=None, *args, **kwargs) ->None:
        act_type = getattr(opts, 'model.activation.name')
        neg_slope = getattr(opts, 'model.activation.neg_slope')
        inplace = getattr(opts, 'model.activation.inplace')
        cbr_1 = ConvLayer2d(opts=opts, in_channels=in_channels, out_channels=mid_channels, kernel_size=1, stride=1, use_norm=True, use_act=True)
        cbr_2 = ConvLayer2d(opts=opts, in_channels=mid_channels, out_channels=mid_channels, kernel_size=3, stride=stride, use_norm=True, use_act=True, dilation=dilation)
        cb_3 = ConvLayer2d(opts=opts, in_channels=mid_channels, out_channels=out_channels, kernel_size=1, stride=1, use_norm=True, use_act=False)
        block = nn.Sequential()
        block.add_module(name='conv_batch_act_1', module=cbr_1)
        block.add_module(name='conv_batch_act_2', module=cbr_2)
        block.add_module(name='conv_batch_3', module=cb_3)
        if 0.0 < dropout < 1.0:
            block.add_module(name='dropout', module=Dropout(p=dropout))
        down_sample = Identity()
        if stride == 2:
            down_sample = ConvLayer2d(opts=opts, in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=stride, use_norm=True, use_act=False)
        elif in_channels != out_channels:
            down_sample = ConvLayer2d(opts=opts, in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, use_norm=True, use_act=False)
        se_block = Identity()
        if squeeze_channels is not None:
            se_block = SqueezeExcitation(opts=opts, in_channels=out_channels, squeeze_channels=squeeze_channels)
        super().__init__()
        self.block = block
        self.down_sample = down_sample
        self.final_act = build_activation_layer(opts, act_type=act_type, inplace=inplace, negative_slope=neg_slope, num_parameters=out_channels)
        self.stochastic_depth = StochasticDepth(p=stochastic_depth_prob, mode='row')
        self.se_block = se_block
        self.stride = stride
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.mid_channels = mid_channels
        self.dilation = dilation
        self.dropout = dropout
        self.stochastic_depth_prob = stochastic_depth_prob
        self.squeeze_channels = squeeze_channels

    def forward(self, x: 'Tensor', *args, **kwargs) ->Tensor:
        out = self.block(x)
        out = self.se_block(out)
        res = self.down_sample(x)
        out = self.stochastic_depth(out)
        out = out + res
        return self.final_act(out)

    def __repr__(self) ->str:
        return '{}(in_channels={}, mid_channels={}, out_channels={}, stride={}, dilation={}, dropout={}, stochastic_depth_prob={}, squeeze_channels={})'.format(self.__class__.__name__, self.in_channels, self.mid_channels, self.out_channels, self.stride, self.dilation, self.dropout, self.stochastic_depth_prob, self.squeeze_channels)


def _patch_merging_pad(x):
    H, W, _ = x.shape[-3:]
    x = F.pad(x, (0, 0, 0, W % 2, 0, H % 2))
    return x


class PatchMerging(BaseModule):
    """Patch Merging Layer.
    Args:
        dim (int): Number of input channels.
        norm_layer (str): Normalization layer name.
        strided (Optional[bool]): Down-sample the input by a factor of 2. Default is True.
    """

    def __init__(self, opts, dim: 'int', norm_layer: 'str', strided: 'Optional[bool]'=True):
        super().__init__()
        self.dim = dim
        self.reduction = LinearLayer(in_features=4 * dim, out_features=2 * dim, bias=False)
        self.norm = get_normalization_layer(opts=opts, norm_type=norm_layer, num_features=4 * dim)
        self.strided = strided

    def forward(self, x: 'Tensor', *args, **kwargs) ->Tensor:
        """
        Args:
            x (Tensor): input tensor with expected layout of [..., H, W, C]
        Returns:
            Tensor with layout of [..., H/2, W/2, 2*C]
        """
        x = _patch_merging_pad(x)
        if self.strided:
            x0 = x[..., 0::2, 0::2, :]
            x1 = x[..., 1::2, 0::2, :]
            x2 = x[..., 0::2, 1::2, :]
            x3 = x[..., 1::2, 1::2, :]
            x = torch.cat([x0, x1, x2, x3], -1)
        else:
            x = torch.cat([x, x, x, x], -1)
        x = self.norm(x)
        x = self.reduction(x)
        return x

    def __repr__(self) ->str:
        s = f'{self.__class__.__name__}(dim={self.dim})'
        return s


class Permute(BaseModule):
    """This module returns a view of the tensor input with its dimensions permuted.
    Args:
        dims (List[int]): The desired ordering of dimensions
    """

    def __init__(self, dims: 'List[int]'):
        super().__init__()
        self.dims = dims

    def forward(self, x: 'Tensor') ->Tensor:
        return torch.permute(x, self.dims)

    def __repr__(self) ->str:
        s = f'{self.__class__.__name__}(dims={self.dims})'
        return s


def shifted_window_attention(input: 'Tensor', qkv_weight: 'Tensor', proj_weight: 'Tensor', relative_position_bias: 'Tensor', window_size: 'List[int]', num_heads: 'int', shift_size: 'List[int]', attention_dropout: 'float'=0.0, dropout: 'float'=0.0, qkv_bias: 'Optional[Tensor]'=None, proj_bias: 'Optional[Tensor]'=None):
    """
    Window based multi-head self attention (W-MSA) module with relative position bias.
    It supports both of shifted and non-shifted window.
    Args:
        input (Tensor[N, H, W, C]): The input tensor or 4-dimensions.
        qkv_weight (Tensor[in_dim, out_dim]): The weight tensor of query, key, value.
        proj_weight (Tensor[out_dim, out_dim]): The weight tensor of projection.
        relative_position_bias (Tensor): The learned relative position bias added to attention.
        window_size (List[int]): Window size.
        num_heads (int): Number of attention heads.
        shift_size (List[int]): Shift size for shifted window attention.
        attention_dropout (float): Dropout ratio of attention weight. Default: 0.0.
        dropout (float): Dropout ratio of output. Default: 0.0.
        qkv_bias (Tensor[out_dim], optional): The bias tensor of query, key, value. Default: None.
        proj_bias (Tensor[out_dim], optional): The bias tensor of projection. Default: None.
    Returns:
        Tensor[N, H, W, C]: The output tensor after shifted window attention.
    """
    B, H, W, C = input.shape
    pad_r = (window_size[1] - W % window_size[1]) % window_size[1]
    pad_b = (window_size[0] - H % window_size[0]) % window_size[0]
    x = F.pad(input, (0, 0, 0, pad_r, 0, pad_b))
    _, pad_H, pad_W, _ = x.shape
    shift_size = shift_size.copy()
    if window_size[0] >= pad_H:
        shift_size[0] = 0
    if window_size[1] >= pad_W:
        shift_size[1] = 0
    if sum(shift_size) > 0:
        x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1]), dims=(1, 2))
    num_windows = pad_H // window_size[0] * (pad_W // window_size[1])
    x = x.view(B, pad_H // window_size[0], window_size[0], pad_W // window_size[1], window_size[1], C)
    x = x.permute(0, 1, 3, 2, 4, 5).reshape(B * num_windows, window_size[0] * window_size[1], C)
    qkv = F.linear(x, qkv_weight, qkv_bias)
    qkv = qkv.reshape(x.size(0), x.size(1), 3, num_heads, C // num_heads).permute(2, 0, 3, 1, 4)
    q, k, v = qkv[0], qkv[1], qkv[2]
    q = q * (C // num_heads) ** -0.5
    attn = q.matmul(k.transpose(-2, -1))
    attn = attn + relative_position_bias
    if sum(shift_size) > 0:
        attn_mask = x.new_zeros((pad_H, pad_W))
        h_slices = (0, -window_size[0]), (-window_size[0], -shift_size[0]), (-shift_size[0], None)
        w_slices = (0, -window_size[1]), (-window_size[1], -shift_size[1]), (-shift_size[1], None)
        count = 0
        for h in h_slices:
            for w in w_slices:
                attn_mask[h[0]:h[1], w[0]:w[1]] = count
                count += 1
        attn_mask = attn_mask.view(pad_H // window_size[0], window_size[0], pad_W // window_size[1], window_size[1])
        attn_mask = attn_mask.permute(0, 2, 1, 3).reshape(num_windows, window_size[0] * window_size[1])
        attn_mask = attn_mask.unsqueeze(1) - attn_mask.unsqueeze(2)
        attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))
        attn = attn.view(x.size(0) // num_windows, num_windows, num_heads, x.size(1), x.size(1))
        attn = attn + attn_mask.unsqueeze(1).unsqueeze(0)
        attn = attn.view(-1, num_heads, x.size(1), x.size(1))
    attn = F.softmax(attn, dim=-1)
    attn = F.dropout(attn, p=attention_dropout)
    x = attn.matmul(v).transpose(1, 2).reshape(x.size(0), x.size(1), C)
    x = F.linear(x, proj_weight, proj_bias)
    x = F.dropout(x, p=dropout)
    x = x.view(B, pad_H // window_size[0], pad_W // window_size[1], window_size[0], window_size[1], C)
    x = x.permute(0, 1, 3, 2, 4, 5).reshape(B, pad_H, pad_W, C)
    if sum(shift_size) > 0:
        x = torch.roll(x, shifts=(shift_size[0], shift_size[1]), dims=(1, 2))
    x = x[:, :H, :W, :].contiguous()
    return x


class ShiftedWindowAttention(BaseModule):
    """
    See :func:`shifted_window_attention`.
    """

    def __init__(self, dim: 'int', window_size: 'List[int]', shift_size: 'List[int]', num_heads: 'int', qkv_bias: 'bool'=True, proj_bias: 'bool'=True, attention_dropout: 'float'=0.0, dropout: 'float'=0.0):
        super().__init__()
        if len(window_size) != 2 or len(shift_size) != 2:
            raise ValueError('window_size and shift_size must be of length 2')
        self.window_size = window_size
        self.shift_size = shift_size
        self.num_heads = num_heads
        self.attention_dropout = attention_dropout
        self.dropout = dropout
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.proj = nn.Linear(dim, dim, bias=proj_bias)
        self.relative_position_bias_table = nn.Parameter(torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads))
        coords_h = torch.arange(self.window_size[0])
        coords_w = torch.arange(self.window_size[1])
        coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing='ij'))
        coords_flatten = torch.flatten(coords, 1)
        relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]
        relative_coords = relative_coords.permute(1, 2, 0).contiguous()
        relative_coords[:, :, 0] += self.window_size[0] - 1
        relative_coords[:, :, 1] += self.window_size[1] - 1
        relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1
        relative_position_index = relative_coords.sum(-1).view(-1)
        self.register_buffer('relative_position_index', relative_position_index)
        nn.init.trunc_normal_(self.relative_position_bias_table, std=0.02)
        self.embed_dim = dim

    def __repr__(self) ->str:
        return '{}(embed_dim={}, window_size={}, shift_size={}, num_heads={}, dropout={}, attn_dropout={}, dropout={})'.format(self.__class__.__name__, self.embed_dim, self.window_size, self.shift_size, self.num_heads, self.attention_dropout, self.dropout)

    def forward(self, x: 'Tensor', *args, **kwargs) ->Tensor:
        """
        Args:
            x (Tensor): Tensor with layout of [B, H, W, C]
        Returns:
            Tensor with same layout as input, i.e. [B, H, W, C]
        """
        N = self.window_size[0] * self.window_size[1]
        relative_position_bias = self.relative_position_bias_table[self.relative_position_index]
        relative_position_bias = relative_position_bias.view(N, N, -1)
        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous().unsqueeze(0)
        return shifted_window_attention(x, self.qkv.weight, self.proj.weight, relative_position_bias, self.window_size, self.num_heads, shift_size=self.shift_size, attention_dropout=self.attention_dropout, dropout=self.dropout, qkv_bias=self.qkv.bias, proj_bias=self.proj.bias)


class SwinTransformerBlock(BaseModule):
    """
    Swin Transformer Block.
    Args:
        dim (int): Number of input channels.
        num_heads (int): Number of attention heads.
        window_size (List[int]): Window size.
        shift_size (List[int]): Shift size for shifted window attention.
        mlp_ratio (float): Ratio of mlp hidden dim to embedding dim. Default: 4.0.
        dropout (float): Dropout rate. Default: 0.0.
        attention_dropout (float): Attention dropout rate. Default: 0.0.
        stochastic_depth_prob: (float): Stochastic depth rate. Default: 0.0.
        norm_layer (nn.Module): Normalization layer.  Default: nn.LayerNorm.
        attn_layer (nn.Module): Attention layer. Default: ShiftedWindowAttention
    """

    def __init__(self, opts, embed_dim: 'int', num_heads: 'int', window_size: 'List[int]', shift_size: 'List[int]', mlp_ratio: 'float'=4.0, dropout: 'float'=0.0, attn_dropout: 'Optional[float]'=0.0, ffn_dropout: 'Optional[float]'=0.0, stochastic_depth_prob: 'float'=0.0, norm_layer: 'Optional[str]'='layer_norm'):
        super().__init__()
        attn_unit = ShiftedWindowAttention(embed_dim, window_size, shift_size, num_heads, attention_dropout=attn_dropout, dropout=dropout)
        self.attn = nn.Sequential(get_normalization_layer(opts=opts, norm_type=norm_layer, num_features=embed_dim), attn_unit, Dropout(p=dropout))
        self.stochastic_depth = StochasticDepth(stochastic_depth_prob, 'row')
        ffn_latent_dim = int(embed_dim * mlp_ratio)
        act_name = build_activation_layer(opts, num_parameters=1)
        self.mlp = nn.Sequential(get_normalization_layer(opts=opts, norm_type=norm_layer, num_features=embed_dim), LinearLayer(in_features=embed_dim, out_features=ffn_latent_dim, bias=True), act_name, Dropout(p=ffn_dropout), LinearLayer(in_features=ffn_latent_dim, out_features=embed_dim, bias=True), Dropout(p=dropout))
        self.embed_dim = embed_dim
        self.ffn_dim = ffn_latent_dim
        self.ffn_dropout = ffn_dropout
        self.std_dropout = dropout
        self.attn_fn_name = attn_unit.__class__.__name__
        self.act_fn_name = act_name.__class__.__name__
        self.norm_type = norm_layer

    def __repr__(self) ->str:
        return '{}(embed_dim={}, ffn_dim={}, dropout={}, ffn_dropout={}, attn_fn={}, act_fn={}, norm_fn={})'.format(self.__class__.__name__, self.embed_dim, self.ffn_dim, self.std_dropout, self.ffn_dropout, self.attn_fn_name, self.act_fn_name, self.norm_type)

    def forward(self, x: 'Tensor', *args, **kwargs) ->Tensor:
        x = x + self.stochastic_depth(self.attn(x))
        x = x + self.stochastic_depth(self.mlp(x))
        return x


def check_feature_map_output_channels(config: 'Dict', layer_name: 'str') ->int:
    enc_ch_l: 'Dict' = config.get(layer_name, None)
    if enc_ch_l is None or not enc_ch_l:
        logger.error('Encoder does not define input-output mapping for {}: Got: {}'.format(layer_name, config))
    enc_ch_l_out = enc_ch_l.get('out', None)
    if enc_ch_l_out is None or not enc_ch_l_out:
        logger.error('Output channels are not defined in {} of the encoder. Got: {}'.format(layer_name, enc_ch_l))
    return enc_ch_l_out


def import_modules_from_folder(folder_name: 'str', extra_roots: 'Sequence[str]'=()) ->None:
    """Automatically import all modules from public library root folder, in addition
    to the @extra_roots directories.

    The @folder_name directory must exist in LIBRARY_ROOT, but existence in @extra_roots
    is optional.

    Args:
        folder_name: Name of the folder to search for its internal and public modules.
        extra_roots: By default, this function only imports from
            `LIBRARY_ROOT/{folder_name}/**/*.py`. For any extra_root provided, it will
            also import `LIBRARY_ROOT/{extra_root}/{folder_name}/**/*.py` modules.
    """
    if not LIBRARY_ROOT.joinpath(folder_name).exists():
        logger.error(f"{folder_name} doesn't exist in the public library root directory.")
    for base_dir in ['.', *extra_roots]:
        for path in LIBRARY_ROOT.glob(os.path.join(base_dir, folder_name, '**/*.py')):
            filename = path.name
            if filename[0] not in ('.', '_'):
                module_name = str(path.relative_to(LIBRARY_ROOT).with_suffix('')).replace(os.sep, '.')
                importlib.import_module(module_name)


def clean_strip(obj: 'Union[str, List[str]]', sep: 'Optional[str]'=',', strip: 'bool'=True) ->List[str]:
    if isinstance(obj, list):
        strings = obj
    else:
        strings = obj.split(sep)
    if strip:
        strings = [x.strip() for x in strings]
    strings = [x for x in strings if x]
    return strings


def freeze_module(module: 'torch.nn.Module', force_eval: 'bool'=True) ->torch.nn.Module:
    """
    Sets requires_grad = False on all the given module parameters, and put the module in eval mode.
    By default, it also overrides the module's `train` method to make sure that it always stays in eval mode
    (ie calling ``module.train(mode=True)`` executes ``module.train(mode=False)``)

    >>> module = nn.Linear(10, 20).train()
    >>> module.training
    True
    >>> module.weight.requires_grad
    True
    >>> freeze_module(module).train().training
    False
    >>> module.weight.requires_grad
    False
    """
    module.eval()
    for parameter in module.parameters():
        parameter.requires_grad = False
    if force_eval:

        def _force_train_in_eval(self: 'torch.nn.Module', mode: 'bool'=True) ->torch.nn.Module:
            return self
        module.train = MethodType(_force_train_in_eval, module)
    return module


def freeze_modules_based_on_opts(opts: 'argparse.Namespace', model: 'torch.nn.Module', verbose: 'bool'=True) ->torch.nn.Module:
    """
    Allows for freezing immediate modules and parameters of the model using --model.freeze-modules.

    --model.freeze-modules should be a list of strings or a comma-separated list of regex expressions.

    Examples of --model.freeze-modules:
        "conv.*"  # see example below: can freeze all (top-level) conv layers
        "^((?!classifier).)*$"   # freezes everything except for "classifier": useful for linear probing
        "conv1,layer1,layer2,layer3"  # freeze all layers up to layer3

    >>> model = nn.Sequential(OrderedDict([
          ('conv1', nn.Conv2d(1, 20, 5)),
          ('relu1', nn.ReLU()),
          ('conv2', nn.Conv2d(20, 64, 5)),
          ('relu2', nn.ReLU())
        ]))
    >>> opts = argparse.Namespace(**{"model.freeze_modules": "conv1"})
    >>> _ = freeze_modules_based_on_opts(opts, model)
    INFO    - Freezing module: conv1
    >>> model.train()
    >>> model.conv1.training
    False
    >>> model.conv2.training
    True
    """
    freeze_patterns = getattr(opts, 'model.freeze_modules', '')
    freeze_patterns = clean_strip(freeze_patterns)
    verbose = verbose and is_master(opts)
    if freeze_patterns:
        for name, module in model.named_children():
            if any([re.match(p, name) for p in freeze_patterns]):
                freeze_module(module)
                if verbose:
                    logger.info('Freezing module: {}'.format(name))
        for name, param in model.named_parameters(recurse=False):
            if any([re.match(p, name) for p in freeze_patterns]):
                param.requires_grad = False
                if verbose:
                    logger.info('Freezing parameter: {}'.format(name))
    if verbose and hasattr(model, 'get_trainable_parameters'):
        param_list, _ = model.get_trainable_parameters()
        for params in param_list:
            if not isinstance(params['param_names'], List) or not isinstance(params['params'], List) or not isinstance(params['weight_decay'], (float, int)):
                param_types = {k: type(v) for k, v in params.items()}
                logger.error('Expected parameter format: {{ params: List, weight_decay: float, param_names: List }}. Got: {}'.format(param_types))
        trainable_param_names = [p for x in param_list for p in x['param_names']]
        logger.info('Trainable parameters: {}'.format(trainable_param_names))
    return model


def is_start_rank_node(opts) ->bool:
    node_rank = getattr(opts, 'ddp.rank', 0)
    def_rank = getattr(opts, 'ddp.start_rank', 0)
    return node_rank == def_rank


def load_pretrained_model(model: 'torch.nn.Module', wt_loc: 'str', opts: 'argparse.Namespace', *args, **kwargs) ->torch.nn.Module:
    """Helper function to load pre-trained weights.
    Args:
        model: Model whose weights will be loaded.
        wt_loc: Path to file to load state_dict from.
        opts: Input arguments.
    Returns:
        The model loaded with the given weights.

    """
    if not os.path.isfile(wt_loc):
        logger.error('Pretrained file is not found here: {}'.format(wt_loc))
    wts = torch.load(wt_loc, map_location='cpu')
    is_master_node = is_start_rank_node(opts)
    exclude_scopes = getattr(opts, 'model.resume_exclude_scopes', '')
    exclude_scopes: 'List[str]' = clean_strip(exclude_scopes)
    missing_scopes = getattr(opts, 'model.ignore_missing_scopes', '')
    missing_scopes: 'List[str]' = clean_strip(missing_scopes)
    rename_scopes_map: 'List[List[str]]' = getattr(opts, 'model.rename_scopes_map', [])
    if rename_scopes_map:
        for entry in rename_scopes_map:
            if len(entry) != 2:
                raise ValueError('Every entry in model.rename_scopes_map must contain exactly two string elements for before and after. Got {}.'.format(str(entry)))
    missing_scopes += exclude_scopes
    if exclude_scopes:
        for key in wts.copy():
            if any([re.match(x, key) for x in exclude_scopes]):
                del wts[key]
    if rename_scopes_map:
        for before, after in rename_scopes_map:
            wts = {re.sub(before, after, key): value for key, value in wts.items()}
    strict = not bool(missing_scopes)
    try:
        module = unwrap_model_fn(model)
        missing_keys, unexpected_keys = module.load_state_dict(wts, strict=strict)
        if unexpected_keys:
            raise Exception('Found unexpected keys: {}.You can ignore these keys using `model.resume_exclude_scopes`.'.format(','.join(unexpected_keys)))
        missing_keys = [key for key in missing_keys if not any([re.match(x, key) for x in missing_scopes])]
        if missing_keys:
            raise Exception('Missing keys detected. Did not find the following keys in pre-trained model: {}. You can ignore the keys using `model.ignore_missing_scopes`.'.format(','.join(missing_keys)))
        if is_master_node:
            logger.log('Pretrained weights are loaded from {}'.format(wt_loc))
    except Exception as e:
        if is_master_node:
            logger.error('Unable to load pretrained weights from {}. Error: {}'.format(wt_loc, e))
    return model


class MaskRCNNEncoder(nn.Module):

    def __init__(self, opts: 'argparse.Namespace', encoder: 'BaseImageEncoder', output_strides: 'List', projection_channels: 'int', encoder_lr_multiplier: 'Optional[float]'=1.0, *args, **kwargs) ->None:
        use_fpn = not getattr(opts, 'model.detection.mask_rcnn.disable_fpn', False)
        super().__init__()
        encoder.conv_1x1_exp = Identity()
        encoder.classifier = Identity()
        backbone_proj_layers = nn.ModuleDict()
        self.backbone_output_strides = sorted(list({4, 8, 16, 32}.intersection(output_strides)))
        model_config = encoder.model_conf_dict
        self.backbone_map = {}
        fpn_proj_layers = nn.ModuleDict() if use_fpn else None
        for os in self.backbone_output_strides:
            if os == 4:
                in_channels = model_config['layer2']['out']
                backbone_os_str = 'out_l2'
            elif os == 8:
                in_channels = model_config['layer3']['out']
                backbone_os_str = 'out_l3'
            elif os == 16:
                in_channels = model_config['layer4']['out']
                backbone_os_str = 'out_l4'
            elif os == 32:
                in_channels = model_config['layer5']['out']
                backbone_os_str = 'out_l5'
            else:
                raise NotImplementedError
            conv_layer = ConvLayer2d(opts=opts, in_channels=in_channels, out_channels=projection_channels, kernel_size=1, use_norm=True, use_act=False)
            backbone_proj_layers.add_module(str(os), conv_layer)
            self.backbone_map[os] = backbone_os_str
            if use_fpn:
                fpn_layer = ConvLayer2d(opts=opts, in_channels=projection_channels, out_channels=projection_channels, kernel_size=3, use_norm=True, use_act=False)
                fpn_proj_layers.add_module(str(os), fpn_layer)
        extra_layers = nn.ModuleDict()
        extra_layer_os = sorted(list(set(self.backbone_output_strides) ^ set(output_strides)))
        for os in extra_layer_os:
            conv_layer = ConvLayer2d(opts=opts, in_channels=projection_channels, out_channels=projection_channels, kernel_size=3, stride=2, use_norm=True, use_act=False)
            extra_layers.add_module(str(os), conv_layer)
        self.encoder = encoder
        self.backbone_proj_layers = backbone_proj_layers
        self.fpn_proj_layers = fpn_proj_layers
        self.use_fpn = use_fpn
        self.extra_layers = extra_layers
        self.out_channels = projection_channels
        self.augmented_tensor = None
        self.encoder_lr_multiplier = encoder_lr_multiplier

    def get_augmented_tensor(self) ->Tensor:
        return self.augmented_tensor

    def forward(self, x: 'Tensor') ->Dict[str, Tensor]:
        enc_end_points: 'Dict' = self.encoder.extract_end_points_all(x)
        self.augmented_tensor = enc_end_points.pop('augmented_tensor', None)
        outputs_backbone: 'Dict' = {}
        for os, enc_key_name in self.backbone_map.items():
            x_proj = self.backbone_proj_layers[str(os)](enc_end_points.pop(enc_key_name))
            outputs_backbone[f'{os}'] = x_proj
        if self.fpn_proj_layers:
            last_os = self.backbone_output_strides[-1]
            prev_fm = outputs_backbone[f'{last_os}']
            prev_fm = self.fpn_proj_layers[f'{last_os}'](prev_fm)
            for os in self.backbone_output_strides[:-1][::-1]:
                curr_fm = outputs_backbone[f'{os}']
                feat_shape = curr_fm.shape[-2:]
                inner_top_down = F.interpolate(prev_fm, size=feat_shape, mode='nearest')
                prev_fm = self.fpn_proj_layers[f'{os}'](curr_fm + inner_top_down)
                outputs_backbone[f'{os}'] = prev_fm
        if self.extra_layers:
            prev_os = self.backbone_output_strides[-1]
            for os, extra_layer in self.extra_layers.items():
                x_proj = extra_layer(outputs_backbone[f'{prev_os}'])
                outputs_backbone[f'{os}'] = x_proj
                prev_os = os
        return outputs_backbone

    def get_trainable_parameters(self, weight_decay: 'float'=0.0, no_decay_bn_filter_bias: 'bool'=False, *args, **kwargs) ->Tuple[List, List]:
        module_name = kwargs.pop('module_name', '')
        """Returns a list of trainable parameters"""
        all_params = []
        all_params_lr = []
        if hasattr(self.encoder, 'enable_layer_wise_lr_decay') and self.encoder.enable_layer_wise_lr_decay:
            backbone_param_list, backbone_lr_list = self.encoder.get_trainable_parameters(*args, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, module_name=module_name + 'encoder.', **kwargs)
            all_params.extend(backbone_param_list)
            if self.encoder_lr_multiplier != 1.0:
                backbone_lr_list = [(lr * self.encoder_lr_multiplier) for lr in backbone_lr_list]
            all_params_lr.extend(backbone_lr_list)
        else:
            backbone_param_list = parameter_list(*args, named_parameters=self.encoder.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, module_name=module_name + 'encoder.', **kwargs)
            all_params.extend(backbone_param_list)
            all_params_lr.extend([self.encoder_lr_multiplier] * len(backbone_param_list))
        if self.backbone_proj_layers:
            projection_param_list = parameter_list(*args, named_parameters=self.backbone_proj_layers.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, module_name=module_name + 'backbone_proj_layers.', **kwargs)
            all_params.extend(projection_param_list)
            all_params_lr.extend([1.0] * len(projection_param_list))
        if self.fpn_proj_layers:
            fpn_projection_param_list = parameter_list(*args, named_parameters=self.fpn_proj_layers.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, module_name=module_name + 'fpn_proj_layers.', **kwargs)
            all_params.extend(fpn_projection_param_list)
            all_params_lr.extend([1.0] * len(fpn_projection_param_list))
        if self.extra_layers:
            extra_layer_param_list = parameter_list(*args, named_parameters=self.extra_layers.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, module_name=module_name + 'extra_layers.', **kwargs)
            all_params.extend(extra_layer_param_list)
            all_params_lr.extend([1.0] * len(extra_layer_param_list))
        return all_params, all_params_lr


def replace_syncbn_with_syncbnfp32(opts, num_features: 'int') ->nn.Module:
    norm_layer = getattr(opts, 'model.normalization.name', None)
    if norm_layer.find('sync') > -1:
        return get_normalization_layer(opts, num_features=num_features, norm_type='sync_batch_norm_fp32')
    else:
        return get_normalization_layer(opts=opts, num_features=num_features)


class FastRCNNConvFCHead(nn.Sequential):

    def __init__(self, opts, input_size: 'Tuple[int, int, int]', conv_layers: 'List[int]', fc_layers: 'List[int]', *args, **kwargs):
        """
        Args:
            input_size (Tuple[int, int, int]): the input size in CHW format.
            conv_layers (list): feature dimensions of each Convolution layer
            fc_layers (list): feature dimensions of each FCN layer
        """
        in_channels, in_height, in_width = input_size
        blocks = []
        previous_channels = in_channels
        for current_channels in conv_layers:
            blocks.extend([ConvLayer2d(opts, in_channels=previous_channels, out_channels=current_channels, kernel_size=3, stride=1, use_norm=False, use_act=False), replace_syncbn_with_syncbnfp32(opts, num_features=current_channels), nn.ReLU(inplace=False)])
            previous_channels = current_channels
        blocks.append(nn.Flatten())
        previous_channels = previous_channels * in_height * in_width
        for current_channels in fc_layers:
            blocks.append(LinearLayer(previous_channels, current_channels, bias=True))
            blocks.append(nn.ReLU(inplace=True))
            previous_channels = current_channels
        super().__init__(*blocks)
        for layer in self.modules():
            if isinstance(layer, nn.Conv2d):
                initialize_conv_layer(module=layer, init_method='kaiming_normal')
            elif isinstance(layer, LinearLayer):
                initialize_fc_layer(module=layer, init_method='kaiming_uniform')


class FastRCNNPredictor(nn.Module):
    """
    Standard classification + bounding box regression layers
    for Fast R-CNN.

    Args:
        in_channels (int): number of input channels
        num_classes (int): number of output classes (including background)
    """

    def __init__(self, in_channels: 'int', num_classes: 'int') ->None:
        super().__init__()
        self.cls_score = LinearLayer(in_channels, num_classes, bias=True)
        self.bbox_pred = LinearLayer(in_channels, num_classes * 4, bias=True)
        for layer in self.modules():
            if isinstance(layer, LinearLayer):
                initialize_fc_layer(module=layer, init_method='kaiming_uniform')

    def forward(self, x: 'Tensor') ->Tuple[Tensor, Tensor]:
        if x.dim() == 4:
            torch._assert(list(x.shape[2:]) == [1, 1], f'x has the wrong shape, expecting the last two dimensions to be [1,1] instead of {list(x.shape[2:])}')
        x = x.flatten(start_dim=1)
        scores = self.cls_score(x)
        bbox_deltas = self.bbox_pred(x)
        return scores, bbox_deltas


class MaskRCNNHeads(nn.Sequential):

    def __init__(self, opts, in_channels: 'int', layers: 'List', dilation: 'int'):
        """
        Args:
            in_channels (int): number of input channels
            layers (list): feature dimensions of each FCN layer
            dilation (int): dilation rate of kernel
            norm_layer (callable, optional): Module specifying the normalization layer to use. Default: None
        """
        blocks = []
        next_feature = in_channels
        for layer_features in layers:
            blocks.extend([ConvLayer2d(opts=opts, in_channels=next_feature, out_channels=layer_features, kernel_size=3, stride=1, dilation=dilation, use_norm=False, use_act=False, bias=False), replace_syncbn_with_syncbnfp32(opts=opts, num_features=layer_features), nn.ReLU(inplace=False)])
            next_feature = layer_features
        super().__init__(*blocks)
        for layer in self.modules():
            if isinstance(layer, nn.Conv2d):
                initialize_conv_layer(module=layer, init_method='kaiming_normal')


class MaskRCNNPredictor(nn.Sequential):

    def __init__(self, opts, in_channels: 'int', dim_reduced: 'int', num_classes: 'int') ->None:
        super().__init__(*[TransposeConvLayer2d(opts, in_channels=in_channels, out_channels=dim_reduced, kernel_size=2, stride=2, padding=0, output_padding=0, use_norm=False, use_act=False, bias=False, groups=1), replace_syncbn_with_syncbnfp32(opts, num_features=dim_reduced), nn.ReLU(inplace=False), ConvLayer2d(opts, in_channels=dim_reduced, out_channels=num_classes, kernel_size=1, stride=1, bias=True, use_norm=False, use_act=False)])
        for layer in self.modules():
            if isinstance(layer, (nn.Conv2d, nn.ConvTranspose2d)):
                initialize_conv_layer(module=layer, init_method='kaiming_normal')


class RPNHead(nn.Module):
    """
    Adds a simple RPN Head with classification and regression heads

    Args:
        in_channels (int): number of channels of the input feature
        num_anchors (int): number of anchors to be predicted
        conv_depth (int, optional): number of convolutions
    """

    def __init__(self, opts, in_channels: 'int', num_anchors: 'int', conv_depth=1) ->None:
        super().__init__()
        convs = []
        for _ in range(conv_depth):
            convs.extend([ConvLayer2d(opts, in_channels=in_channels, out_channels=in_channels, kernel_size=3, stride=1, use_norm=False, use_act=False, bias=False), replace_syncbn_with_syncbnfp32(opts, num_features=in_channels), nn.ReLU(inplace=False)])
        self.conv = nn.Sequential(*convs)
        self.cls_logits = ConvLayer2d(opts, in_channels=in_channels, out_channels=num_anchors, kernel_size=1, stride=1, use_norm=False, use_act=False, bias=True)
        self.bbox_pred = ConvLayer2d(opts, in_channels=in_channels, out_channels=num_anchors * 4, kernel_size=1, stride=1, use_act=False, use_norm=False, bias=True)
        for layer in self.modules():
            if isinstance(layer, nn.Conv2d):
                initialize_conv_layer(module=layer, init_method='normal', std_val=0.01)

    def forward(self, x: 'List[Tensor]') ->Tuple[List[Tensor], List[Tensor]]:
        logits = []
        bbox_reg = []
        for feature in x:
            t = self.conv(feature)
            logits.append(self.cls_logits(t))
            bbox_reg.append(self.bbox_pred(t))
        return logits, bbox_reg


class FeaturePyramidNetwork(BaseModule):
    """
    This class implements the `Feature Pyramid Network <https://arxiv.org/abs/1612.03144>`_ module for object detection.

    Args:
        opts: command-line arguments
        in_channels (List[int]): List of channels at different output strides
        output_strides (List[int]): Feature maps from these output strides will be used in FPN
        out_channels (int): Output channels

    """

    def __init__(self, opts, in_channels: 'List[int]', output_strides: 'List[str]', out_channels: 'int', *args, **kwargs) ->None:
        if isinstance(in_channels, int):
            in_channels = [in_channels]
        if isinstance(output_strides, int):
            output_strides = [output_strides]
        if len(in_channels) != len(output_strides):
            logger.error('For {}, we need the length of input_channels to be the same as the length of output stride. Got: {} and {}'.format(self.__class__.__name__, len(in_channels), len(output_strides)))
        assert len(in_channels) == len(output_strides)
        super().__init__(*args, **kwargs)
        self.proj_layers = nn.ModuleDict()
        self.nxn_convs = nn.ModuleDict()
        for os, in_channel in zip(output_strides, in_channels):
            proj_layer = ConvLayer2d(opts=opts, in_channels=in_channel, out_channels=out_channels, kernel_size=1, bias=False, use_norm=True, use_act=False)
            nxn_conv = ConvLayer2d(opts=opts, in_channels=out_channels, out_channels=out_channels, kernel_size=3, bias=False, use_norm=True, use_act=False)
            self.proj_layers.add_module(name='os_{}'.format(os), module=proj_layer)
            self.nxn_convs.add_module(name='os_{}'.format(os), module=nxn_conv)
        self.num_fpn_layers = len(in_channels)
        self.out_channels = out_channels
        self.in_channels = in_channels
        self.output_strides = output_strides
        self.reset_weights()

    def reset_weights(self) ->None:
        """Resets the weights of FPN layers"""
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                initialize_conv_layer(m, init_method='xavier_uniform')
            elif isinstance(m, norm_layers_tuple):
                initialize_norm_layers(m)

    def forward(self, x: 'Dict[str, Tensor]', *args, **kwargs) ->Dict[str, Tensor]:
        assert len(x) == self.num_fpn_layers
        fpn_out_dict = {'os_'.format(os): None for os in self.output_strides}
        os_key = 'os_{}'.format(self.output_strides[-1])
        prev_x = self.proj_layers[os_key](x[os_key])
        prev_x = self.nxn_convs[os_key](prev_x)
        fpn_out_dict[os_key] = prev_x
        remaining_output_strides = self.output_strides[:-1]
        for os in remaining_output_strides[::-1]:
            os_key = 'os_{}'.format(os)
            curr_x = self.proj_layers[os_key](x[os_key])
            prev_x = F.interpolate(prev_x, size=curr_x.shape[-2:], mode='nearest')
            prev_x = curr_x + prev_x
            prev_x = self.nxn_convs[os_key](prev_x)
            fpn_out_dict[os_key] = prev_x
        return fpn_out_dict

    def __repr__(self):
        return '{}(in_channels={}, output_strides={} out_channels={})'.format(self.__class__.__name__, self.in_channels, self.output_strides, self.out_channels)


class SSDHead(BaseModule):
    """
    This class defines the `SSD object detection Head <https://arxiv.org/abs/1512.02325>`_

    Args:
        opts: command-line arguments
        in_channels (int): :math:`C` from an expected input of size :math:`(N, C, H, W)`
        n_anchors (int): Number of anchors
        n_classes (int): Number of classes in the dataset
        n_coordinates (Optional[int]): Number of coordinates. Default: 4 (x, y, w, h)
        proj_channels (Optional[int]): Number of projected channels. If `-1`, then projection layer is not used
        kernel_size (Optional[int]): Kernel size in convolutional layer. If kernel_size=1, then standard
            point-wise convolution is used. Otherwise, separable convolution is used
        stride (Optional[int]): stride for feature map. If stride > 1, then feature map is sampled at this rate
            and predictions are made on fewer pixels as compared to the input tensor. Default: 1
    """

    def __init__(self, opts, in_channels: 'int', n_anchors: 'int', n_classes: 'int', n_coordinates: 'Optional[int]'=4, proj_channels: 'Optional[int]'=-1, kernel_size: 'Optional[int]'=3, stride: 'Optional[int]'=1, *args, **kwargs) ->None:
        super().__init__()
        proj_layer = None
        self.proj_channels = None
        if proj_channels != -1 and proj_channels != in_channels and kernel_size > 1:
            proj_layer = ConvLayer2d(opts=opts, in_channels=in_channels, out_channels=proj_channels, kernel_size=1, stride=1, groups=1, bias=False, use_norm=True, use_act=True)
            in_channels = proj_channels
            self.proj_channels = proj_channels
        self.proj_layer = proj_layer
        conv_fn = ConvLayer2d if kernel_size == 1 else SeparableConv2d
        if kernel_size > 1 and stride > 1:
            kernel_size = max(kernel_size, stride if stride % 2 != 0 else stride + 1)
        self.loc_cls_layer = conv_fn(opts=opts, in_channels=in_channels, out_channels=n_anchors * (n_coordinates + n_classes), kernel_size=kernel_size, stride=1, groups=1, bias=True, use_norm=False, use_act=False)
        self.n_coordinates = n_coordinates
        self.n_classes = n_classes
        self.n_anchors = n_anchors
        self.k_size = kernel_size
        self.stride = stride
        self.in_channel = in_channels
        self.reset_parameters()

    def __repr__(self) ->str:
        repr_str = '{}(in_channels={}, n_anchors={}, n_classes={}, n_coordinates={}, kernel_size={}, stride={}'.format(self.__class__.__name__, self.in_channel, self.n_anchors, self.n_classes, self.n_coordinates, self.k_size, self.stride)
        if self.proj_layer is not None:
            repr_str += ', proj=True, proj_channels={}'.format(self.proj_channels)
        repr_str += ')'
        return repr_str

    def reset_parameters(self) ->None:
        for layer in self.modules():
            if isinstance(layer, nn.Conv2d):
                initialize_conv_layer(module=layer, init_method='xavier_uniform')

    def _sample_fm(self, x: 'Tensor') ->Tensor:
        height, width = x.shape[-2:]
        device = x.device
        start_step = max(0, self.stride // 2)
        indices_h = torch.arange(start=start_step, end=height, step=self.stride, dtype=torch.int64, device=device)
        indices_w = torch.arange(start=start_step, end=width, step=self.stride, dtype=torch.int64, device=device)
        x_sampled = torch.index_select(x, dim=-1, index=indices_w)
        x_sampled = torch.index_select(x_sampled, dim=-2, index=indices_h)
        return x_sampled

    def forward(self, x: 'Tensor', *args, **kwargs) ->Tuple[Tensor, Tensor]:
        batch_size = x.shape[0]
        if self.proj_layer is not None:
            x = self.proj_layer(x)
        x = self.loc_cls_layer(x)
        if self.stride > 1:
            x = self._sample_fm(x)
        x = x.permute(0, 2, 3, 1)
        x = x.contiguous().view(batch_size, -1, self.n_coordinates + self.n_classes)
        box_locations, box_classes = torch.split(x, [self.n_coordinates, self.n_classes], dim=-1)
        return box_locations, box_classes


def build_anchor_generator(opts, *args, **kwargs):
    """Build anchor generator for object detection"""
    anchor_gen_name = getattr(opts, 'anchor_generator.name')
    if anchor_gen_name == '__base__':
        logger.error("__base__ can't be used as a projection name. Please check.")
    anchor_gen = ANCHOR_GEN_REGISTRY[anchor_gen_name](opts, *args, **kwargs)
    return anchor_gen


class BaseMatcher(object):
    """
    Base class for matching anchor boxes and labels for the task of object detection
    """

    def __init__(self, opts, *args, **kwargs) ->None:
        super(BaseMatcher, self).__init__()
        self.opts = opts

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser'):
        """Add class-specific arguments"""
        return parser

    def __call__(self, *args, **kwargs):
        raise NotImplementedError


def build_matcher(opts, *args, **kwargs):
    matcher_name = getattr(opts, 'matcher.name', None)
    if matcher_name == '__base__':
        logger.error("__base__ can't be used as a projection name. Please check.")
    matcher = MATCHER_REGISTRY[matcher_name](opts, *args, **kwargs)
    return matcher


def is_coreml_conversion(opts) ->bool:
    if getattr(opts, 'common.enable_coreml_compatible_module', False):
        return True
    return False


def _check_out_channels(config: 'dict', layer_name: 'str') ->int:
    enc_ch_l: 'dict' = config.get(layer_name, None)
    if enc_ch_l is None or not enc_ch_l:
        logger.error('Encoder does not define input-output mapping for {}: Got: {}'.format(layer_name, config))
    enc_ch_l_out = enc_ch_l.get('out', None)
    if enc_ch_l_out is None or not enc_ch_l_out:
        logger.error('Output channels are not defined in {} of the encoder. Got: {}'.format(layer_name, enc_ch_l))
    return enc_ch_l_out


class ASPPConv2d(ConvLayer2d):
    """
    Convolution with a dilation  for the ASPP module
    Args:
        opts: command-line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H, W)`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H, W)`
        dilation (int): Dilation rate

    Shape:
        - Input: :math:`(N, C_{in}, H, W)`
        - Output: :math:`(N, C_{out}, H, W)`
    """

    def __init__(self, opts, in_channels: 'int', out_channels: 'int', dilation: 'int', *args, **kwargs) ->None:
        super().__init__(opts=opts, in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, use_norm=True, use_act=True, dilation=dilation)

    def adjust_atrous_rate(self, rate: 'int') ->None:
        """This function allows to adjust the dilation rate"""
        self.block.conv.dilation = rate
        self.block.conv.padding = rate


class ASPPPooling(BaseLayer):
    """
    ASPP pooling layer
    Args:
        opts: command-line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H, W)`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H, W)`

    Shape:
        - Input: :math:`(N, C_{in}, H, W)`
        - Output: :math:`(N, C_{out}, H, W)`
    """

    def __init__(self, opts, in_channels: 'int', out_channels: 'int', *args, **kwargs) ->None:
        super().__init__()
        self.aspp_pool = nn.Sequential()
        self.aspp_pool.add_module(name='global_pool', module=AdaptiveAvgPool2d(output_size=1))
        self.aspp_pool.add_module(name='conv_1x1', module=ConvLayer2d(opts=opts, in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, use_norm=True, use_act=True))
        self.in_channels = in_channels
        self.out_channels = out_channels

    def forward(self, x: 'Tensor') ->Tensor:
        x_size = x.shape[-2:]
        x = self.aspp_pool(x)
        x = F.interpolate(x, size=x_size, mode='bilinear', align_corners=False)
        return x

    def __repr__(self):
        return '{}(in_channels={}, out_channels={})'.format(self.__class__.__name__, self.in_channels, self.out_channels)


class ASPPSeparableConv2d(SeparableConv2d):
    """
    Separable Convolution with a dilation for the ASPP module
    Args:
        opts: command-line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H, W)`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H, W)`
        dilation (int): Dilation rate

    Shape:
        - Input: :math:`(N, C_{in}, H, W)`
        - Output: :math:`(N, C_{out}, H, W)`
    """

    def __init__(self, opts, in_channels: 'int', out_channels: 'int', dilation: 'int', *args, **kwargs) ->None:
        super().__init__(opts=opts, in_channels=in_channels, out_channels=out_channels, kernel_size=3, stride=1, dilation=dilation, use_norm=True, use_act=True)

    def adjust_atrous_rate(self, rate: 'int') ->None:
        """This function allows to adjust the dilation rate"""
        self.dw_conv.block.conv.dilation = rate
        self.dw_conv.block.conv.padding = rate


class ASPP(BaseModule):
    """
    ASPP module defined in DeepLab papers, `here <https://arxiv.org/abs/1606.00915>`_ and `here <https://arxiv.org/abs/1706.05587>`_

    Args:
        opts: command-line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H, W)`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H, W)`
        atrous_rates (Tuple[int]): atrous rates for different branches.
        is_sep_conv (Optional[bool]): Use separable convolution instead of standaard conv. Default: False
        dropout (Optional[float]): Apply dropout. Default is 0.0

    Shape:
        - Input: :math:`(N, C_{in}, H, W)`
        - Output: :math:`(N, C_{out}, H, W)`
    """

    def __init__(self, opts, in_channels: 'int', out_channels: 'int', atrous_rates: 'Tuple[int]', is_sep_conv: 'Optional[bool]'=False, dropout: 'Optional[float]'=0.0, *args, **kwargs) ->None:
        in_proj = ConvLayer2d(opts=opts, in_channels=in_channels, out_channels=out_channels, kernel_size=1, stride=1, use_norm=True, use_act=True)
        out_proj = ConvLayer2d(opts=opts, in_channels=5 * out_channels, out_channels=out_channels, kernel_size=1, stride=1, use_norm=True, use_act=True)
        aspp_layer = ASPPSeparableConv2d if is_sep_conv else ASPPConv2d
        assert len(atrous_rates) == 3
        modules = [in_proj]
        modules.extend([aspp_layer(opts=opts, in_channels=in_channels, out_channels=out_channels, dilation=rate) for rate in atrous_rates])
        modules.append(ASPPPooling(opts=opts, in_channels=in_channels, out_channels=out_channels))
        if not 0.0 <= dropout < 1.0:
            if is_master(opts):
                logger.warning('Dropout value in {} should be between 0 and 1. Got: {}. Setting it to 0.0'.format(self.__class__.__name__, dropout))
            dropout = 0.0
        super().__init__()
        self.convs = nn.ModuleList(modules)
        self.project = out_proj
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.atrous_rates = atrous_rates
        self.is_sep_conv_layer = is_sep_conv
        self.n_atrous_branches = len(atrous_rates)
        self.dropout_layer = Dropout2d(p=dropout)

    def forward(self, x: 'Tensor', *args, **kwargs) ->Tensor:
        out = []
        for conv in self.convs:
            out.append(conv(x))
        out = torch.cat(out, dim=1)
        out = self.project(out)
        out = self.dropout_layer(out)
        return out

    def __repr__(self):
        return '{}(in_channels={}, out_channels={}, atrous_rates={}, is_aspp_sep={}, dropout={})'.format(self.__class__.__name__, self.in_channels, self.out_channels, self.atrous_rates, self.is_sep_conv_layer, self.dropout_layer.p)


class PSP(BaseModule):
    """
    This class defines the Pyramid Scene Parsing module in the `PSPNet paper <https://arxiv.org/abs/1612.01105>`_

    Args:
        opts: command-line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H, W)`
        out_channels (int): :math:`C_{out}` from an expected output of size :math:`(N, C_{out}, H, W)`
        pool_sizes Optional[Tuple[int, ...]]: List or Tuple of pool sizes. Default: (1, 2, 3, 6)
        dropout (Optional[float]): Apply dropout. Default is 0.0
    """

    def __init__(self, opts, in_channels: 'int', out_channels: 'int', pool_sizes: 'Optional[Tuple[int, ...]]'=(1, 2, 3, 6), dropout: 'Optional[float]'=0.0, *args, **kwargs) ->None:
        if not 0.0 <= dropout < 1.0:
            logger.error('Dropout value in {} should be between 0 and 1. Got: {}'.format(self.__class__.__name__, dropout))
        reduction_dim = in_channels // len(pool_sizes)
        reduction_dim = reduction_dim // 16 * 16
        channels_after_concat = reduction_dim * len(pool_sizes) + in_channels
        super().__init__()
        self.psp_branches = nn.ModuleList([self._make_psp_layer(opts, o_size=ps, in_channels=in_channels, out_channels=reduction_dim) for ps in pool_sizes])
        self.fusion = nn.Sequential(ConvLayer2d(opts=opts, in_channels=channels_after_concat, out_channels=out_channels, kernel_size=3, stride=1, use_norm=True, use_act=True), Dropout2d(p=dropout))
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.pool_sizes = pool_sizes
        self.inner_channels = reduction_dim
        self.dropout = dropout

    @staticmethod
    def _make_psp_layer(opts, o_size: 'int', in_channels: 'int', out_channels: 'int') ->nn.Module:
        return nn.Sequential(AdaptiveAvgPool2d(output_size=(o_size, o_size)), ConvLayer2d(opts, in_channels=in_channels, out_channels=out_channels, kernel_size=1, bias=False, use_norm=True, use_act=True))

    def forward(self, x: 'Tensor', *args, **kwargs) ->Tensor:
        x_size = x.shape[2:]
        out = [x] + [F.interpolate(input=psp_branch(x), size=x_size, mode='bilinear', align_corners=True) for psp_branch in self.psp_branches]
        out = torch.cat(out, dim=1)
        out = self.fusion(out)
        return out

    def __repr__(self):
        return '{}(in_channels={}, out_channels={}, pool_sizes={}, inner_channels={}, dropout_2d={})'.format(self.__class__.__name__, self.in_channels, self.out_channels, self.pool_sizes, self.inner_channels, self.dropout)


class RepCPE(BaseModule):
    """
    Implementation of reparameterizable conditional positional encoding.
    For more details refer to paper:
    `Conditional Positional Encodings for Vision Transformers <https://arxiv.org/pdf/2102.10882.pdf>`_

    Args:
        opts: Command line arguments.
        in_channels: Number of input channels.
        embed_dim: Number of embedding dimensions. Default: 768
        spatial_shape: Spatial shape of kernel for positional encoding. Default: (7, 7)
        inference_mode: Flag to instantiate block in inference mode. Default: ``False``
    """

    def __init__(self, opts: 'argparse.Namespace', in_channels: 'int', embed_dim: 'int'=768, spatial_shape: 'Union[int, Tuple[int, int]]'=(7, 7), inference_mode: 'bool'=False):
        super(RepCPE, self).__init__()
        if isinstance(spatial_shape, int):
            spatial_shape = tuple([spatial_shape] * 2)
        assert isinstance(spatial_shape, Tuple), f'"spatial_shape" must by a sequence or int, get {type(spatial_shape)} instead.'
        assert len(spatial_shape) == 2, f'Length of "spatial_shape" should be 2, got {len(spatial_shape)} instead.'
        self.spatial_shape = spatial_shape
        self.embed_dim = embed_dim
        self.in_channels = in_channels
        self.groups = embed_dim
        if inference_mode:
            self.reparam_conv = nn.Conv2d(in_channels=self.in_channels, out_channels=self.embed_dim, kernel_size=self.spatial_shape, stride=1, padding=int(self.spatial_shape[0] // 2), groups=self.embed_dim, bias=True)
        else:
            self.pe = ConvLayer2d(opts, in_channels=in_channels, out_channels=embed_dim, kernel_size=spatial_shape, stride=1, padding=int(spatial_shape[0] // 2), use_norm=False, use_act=False, bias=True, groups=embed_dim)

    def forward(self, x: 'torch.Tensor', *args, **kwargs) ->torch.Tensor:
        """
        Forward pass implements inference logic for module
        before and after reparameterization.

        Args:
            x: Input tensor of shape :math:`(B, C, H, W)`.

        Returns:
            torch.Tensor of shape :math:`(B, C, H, W)`.
        """
        if hasattr(self, 'reparam_conv'):
            x = self.reparam_conv(x)
            return x
        else:
            x = self.pe(x) + x
            return x

    def reparameterize(self) ->None:
        """Reparameterize linear branches."""
        input_dim = self.in_channels // self.groups
        kernel_value = torch.zeros((self.in_channels, input_dim, self.spatial_shape[0], self.spatial_shape[1]), dtype=self.pe.block.conv.weight.dtype, device=self.pe.block.conv.weight.device)
        for i in range(self.in_channels):
            kernel_value[i, i % input_dim, self.spatial_shape[0] // 2, self.spatial_shape[1] // 2] = 1
        id_tensor = kernel_value
        w_final = id_tensor + self.pe.block.conv.weight
        b_final = self.pe.block.conv.bias
        self.reparam_conv = nn.Conv2d(in_channels=self.in_channels, out_channels=self.embed_dim, kernel_size=self.spatial_shape, stride=1, padding=int(self.spatial_shape[0] // 2), groups=self.embed_dim, bias=True)
        self.reparam_conv.weight.data = w_final
        self.reparam_conv.bias.data = b_final
        for para in self.parameters():
            para.detach_()
        self.__delattr__('pe')


class LinearAttnFFN(BaseModule):
    """
    This class defines the pre-norm transformer encoder with linear self-attention in `MobileViTv2 <https://arxiv.org/abs/2206.02680>`_ paper
    Args:
        opts: command line arguments
        embed_dim (int): :math:`C_{in}` from an expected input of size :math:`(B, C_{in}, P, N)`
        ffn_latent_dim (int): Inner dimension of the FFN
        attn_dropout (Optional[float]): Dropout rate for attention in multi-head attention. Default: 0.0
        dropout (Optional[float]): Dropout rate. Default: 0.0
        ffn_dropout (Optional[float]): Dropout between FFN layers. Default: 0.0
        norm_layer (Optional[str]): Normalization layer. Default: layer_norm_2d

    Shape:
        - Input: :math:`(B, C_{in}, P, N)` where :math:`B` is batch size, :math:`C_{in}` is input embedding dim,
            :math:`P` is number of pixels in a patch, and :math:`N` is number of patches,
        - Output: same shape as the input
    """

    def __init__(self, opts, embed_dim: 'int', ffn_latent_dim: 'int', attn_dropout: 'Optional[float]'=0.0, dropout: 'Optional[float]'=0.1, ffn_dropout: 'Optional[float]'=0.0, norm_layer: 'Optional[str]'='layer_norm_2d', *args, **kwargs) ->None:
        super().__init__()
        attn_unit = LinearSelfAttention(opts, embed_dim=embed_dim, attn_dropout=attn_dropout, bias=True)
        self.pre_norm_attn = nn.Sequential(get_normalization_layer(opts=opts, norm_type=norm_layer, num_features=embed_dim), attn_unit, Dropout(p=dropout))
        self.pre_norm_ffn = nn.Sequential(get_normalization_layer(opts=opts, norm_type=norm_layer, num_features=embed_dim), ConvLayer2d(opts=opts, in_channels=embed_dim, out_channels=ffn_latent_dim, kernel_size=1, stride=1, bias=True, use_norm=False, use_act=True), Dropout(p=ffn_dropout), ConvLayer2d(opts=opts, in_channels=ffn_latent_dim, out_channels=embed_dim, kernel_size=1, stride=1, bias=True, use_norm=False, use_act=False), Dropout(p=dropout))
        self.embed_dim = embed_dim
        self.ffn_dim = ffn_latent_dim
        self.ffn_dropout = ffn_dropout
        self.std_dropout = dropout
        self.attn_fn_name = attn_unit.__repr__()
        self.norm_name = norm_layer

    def __repr__(self) ->str:
        return '{}(embed_dim={}, ffn_dim={}, dropout={}, ffn_dropout={}, attn_fn={}, norm_layer={})'.format(self.__class__.__name__, self.embed_dim, self.ffn_dim, self.std_dropout, self.ffn_dropout, self.attn_fn_name, self.norm_name)

    def forward(self, x: 'Tensor', x_prev: 'Optional[Tensor]'=None, *args, **kwargs) ->Tensor:
        if x_prev is None:
            x = x + self.pre_norm_attn(x)
        else:
            res = x
            x = self.pre_norm_attn[0](x)
            x = self.pre_norm_attn[1](x, x_prev)
            x = self.pre_norm_attn[2](x)
            x = x + res
        x = x + self.pre_norm_ffn(x)
        return x


class MobileViTBlockv2(BaseModule):
    """
    This class defines the `MobileViTv2 <https://arxiv.org/abs/2206.02680>`_ block

    Args:
        opts: command line arguments
        in_channels (int): :math:`C_{in}` from an expected input of size :math:`(N, C_{in}, H, W)`
        attn_unit_dim (int): Input dimension to the attention unit
        ffn_multiplier (int): Expand the input dimensions by this factor in FFN. Default is 2.
        n_attn_blocks (Optional[int]): Number of attention units. Default: 2
        attn_dropout (Optional[float]): Dropout in multi-head attention. Default: 0.0
        dropout (Optional[float]): Dropout rate. Default: 0.0
        ffn_dropout (Optional[float]): Dropout between FFN layers in transformer. Default: 0.0
        patch_h (Optional[int]): Patch height for unfolding operation. Default: 8
        patch_w (Optional[int]): Patch width for unfolding operation. Default: 8
        conv_ksize (Optional[int]): Kernel size to learn local representations in MobileViT block. Default: 3
        dilation (Optional[int]): Dilation rate in convolutions. Default: 1
        attn_norm_layer (Optional[str]): Normalization layer in the attention block. Default: layer_norm_2d
    """

    def __init__(self, opts, in_channels: 'int', attn_unit_dim: 'int', ffn_multiplier: 'Optional[Union[Sequence[Union[int, float]], int, float]]'=2.0, n_attn_blocks: 'Optional[int]'=2, attn_dropout: 'Optional[float]'=0.0, dropout: 'Optional[float]'=0.0, ffn_dropout: 'Optional[float]'=0.0, patch_h: 'Optional[int]'=8, patch_w: 'Optional[int]'=8, conv_ksize: 'Optional[int]'=3, dilation: 'Optional[int]'=1, attn_norm_layer: 'Optional[str]'='layer_norm_2d', *args, **kwargs) ->None:
        cnn_out_dim = attn_unit_dim
        conv_3x3_in = ConvLayer2d(opts=opts, in_channels=in_channels, out_channels=in_channels, kernel_size=conv_ksize, stride=1, use_norm=True, use_act=True, dilation=dilation, groups=in_channels)
        conv_1x1_in = ConvLayer2d(opts=opts, in_channels=in_channels, out_channels=cnn_out_dim, kernel_size=1, stride=1, use_norm=False, use_act=False)
        super(MobileViTBlockv2, self).__init__()
        self.local_rep = nn.Sequential(conv_3x3_in, conv_1x1_in)
        self.global_rep, attn_unit_dim = self._build_attn_layer(opts=opts, d_model=attn_unit_dim, ffn_mult=ffn_multiplier, n_layers=n_attn_blocks, attn_dropout=attn_dropout, dropout=dropout, ffn_dropout=ffn_dropout, attn_norm_layer=attn_norm_layer)
        self.conv_proj = ConvLayer2d(opts=opts, in_channels=cnn_out_dim, out_channels=in_channels, kernel_size=1, stride=1, use_norm=True, use_act=False)
        self.patch_h = patch_h
        self.patch_w = patch_w
        self.patch_area = self.patch_w * self.patch_h
        self.cnn_in_dim = in_channels
        self.cnn_out_dim = cnn_out_dim
        self.transformer_in_dim = attn_unit_dim
        self.dropout = dropout
        self.attn_dropout = attn_dropout
        self.ffn_dropout = ffn_dropout
        self.n_blocks = n_attn_blocks
        self.conv_ksize = conv_ksize
        self.enable_coreml_compatible_fn = getattr(opts, 'common.enable_coreml_compatible_module', False)
        if self.enable_coreml_compatible_fn:
            self.register_buffer(name='unfolding_weights', tensor=self._compute_unfolding_weights(), persistent=False)

    def _compute_unfolding_weights(self) ->Tensor:
        weights = torch.eye(self.patch_h * self.patch_w, dtype=torch.float)
        weights = weights.reshape((self.patch_h * self.patch_w, 1, self.patch_h, self.patch_w))
        weights = weights.repeat(self.cnn_out_dim, 1, 1, 1)
        return weights

    def _build_attn_layer(self, opts, d_model: 'int', ffn_mult: 'Union[Sequence, int, float]', n_layers: 'int', attn_dropout: 'float', dropout: 'float', ffn_dropout: 'float', attn_norm_layer: 'str', *args, **kwargs) ->Tuple[nn.Module, int]:
        if isinstance(ffn_mult, Sequence) and len(ffn_mult) == 2:
            ffn_dims = np.linspace(ffn_mult[0], ffn_mult[1], n_layers, dtype=float) * d_model
        elif isinstance(ffn_mult, Sequence) and len(ffn_mult) == 1:
            ffn_dims = [ffn_mult[0] * d_model] * n_layers
        elif isinstance(ffn_mult, (int, float)):
            ffn_dims = [ffn_mult * d_model] * n_layers
        else:
            raise NotImplementedError
        ffn_dims = [int(d // 16 * 16) for d in ffn_dims]
        global_rep = [LinearAttnFFN(opts=opts, embed_dim=d_model, ffn_latent_dim=ffn_dims[block_idx], attn_dropout=attn_dropout, dropout=dropout, ffn_dropout=ffn_dropout, norm_layer=attn_norm_layer) for block_idx in range(n_layers)]
        global_rep.append(get_normalization_layer(opts=opts, norm_type=attn_norm_layer, num_features=d_model))
        return nn.Sequential(*global_rep), d_model

    def __repr__(self) ->str:
        repr_str = '{}('.format(self.__class__.__name__)
        repr_str += '\n\t Local representations'
        if isinstance(self.local_rep, nn.Sequential):
            for m in self.local_rep:
                repr_str += '\n\t\t {}'.format(m)
        else:
            repr_str += '\n\t\t {}'.format(self.local_rep)
        repr_str += '\n\t Global representations with patch size of {}x{}'.format(self.patch_h, self.patch_w)
        if isinstance(self.global_rep, nn.Sequential):
            for m in self.global_rep:
                repr_str += '\n\t\t {}'.format(m)
        else:
            repr_str += '\n\t\t {}'.format(self.global_rep)
        if isinstance(self.conv_proj, nn.Sequential):
            for m in self.conv_proj:
                repr_str += '\n\t\t {}'.format(m)
        else:
            repr_str += '\n\t\t {}'.format(self.conv_proj)
        repr_str += '\n)'
        return repr_str

    def unfolding_pytorch(self, feature_map: 'Tensor') ->Tuple[Tensor, Tuple[int, int]]:
        batch_size, in_channels, img_h, img_w = feature_map.shape
        patches = F.unfold(feature_map, kernel_size=(self.patch_h, self.patch_w), stride=(self.patch_h, self.patch_w))
        patches = patches.reshape(batch_size, in_channels, self.patch_h * self.patch_w, -1)
        return patches, (img_h, img_w)

    def folding_pytorch(self, patches: 'Tensor', output_size: 'Tuple[int, int]') ->Tensor:
        batch_size, in_dim, patch_size, n_patches = patches.shape
        patches = patches.reshape(batch_size, in_dim * patch_size, n_patches)
        feature_map = F.fold(patches, output_size=output_size, kernel_size=(self.patch_h, self.patch_w), stride=(self.patch_h, self.patch_w))
        return feature_map

    def unfolding_coreml(self, feature_map: 'Tensor') ->Tuple[Tensor, Tuple[int, int]]:
        batch_size, in_channels, img_h, img_w = feature_map.shape
        patches = F.conv2d(feature_map, self.unfolding_weights, bias=None, stride=(self.patch_h, self.patch_w), padding=0, dilation=1, groups=in_channels)
        patches = patches.reshape(batch_size, in_channels, self.patch_h * self.patch_w, -1)
        return patches, (img_h, img_w)

    def folding_coreml(self, patches: 'Tensor', output_size: 'Tuple[int, int]') ->Tensor:
        batch_size, in_dim, patch_size, n_patches = patches.shape
        n_patches_h = output_size[0] // self.patch_h
        n_patches_w = output_size[1] // self.patch_w
        feature_map = patches.reshape(batch_size, in_dim * self.patch_h * self.patch_w, n_patches_h, n_patches_w)
        assert self.patch_h == self.patch_w, 'For Coreml, we need patch_h and patch_w are the same'
        feature_map = F.pixel_shuffle(feature_map, upscale_factor=self.patch_h)
        return feature_map

    def resize_input_if_needed(self, x):
        batch_size, in_channels, orig_h, orig_w = x.shape
        if orig_h % self.patch_h != 0 or orig_w % self.patch_w != 0:
            new_h = int(math.ceil(orig_h / self.patch_h) * self.patch_h)
            new_w = int(math.ceil(orig_w / self.patch_w) * self.patch_w)
            x = F.interpolate(x, size=(new_h, new_w), mode='bilinear', align_corners=True)
        return x

    def forward_spatial(self, x: 'Tensor', *args, **kwargs) ->Tensor:
        x = self.resize_input_if_needed(x)
        fm = self.local_rep(x)
        if self.enable_coreml_compatible_fn:
            patches, output_size = self.unfolding_coreml(fm)
        else:
            patches, output_size = self.unfolding_pytorch(fm)
        patches = self.global_rep(patches)
        if self.enable_coreml_compatible_fn:
            fm = self.folding_coreml(patches=patches, output_size=output_size)
        else:
            fm = self.folding_pytorch(patches=patches, output_size=output_size)
        fm = self.conv_proj(fm)
        return fm

    def forward_temporal(self, x: 'Tensor', x_prev: 'Tensor', *args, **kwargs) ->Union[Tensor, Tuple[Tensor, Tensor]]:
        x = self.resize_input_if_needed(x)
        fm = self.local_rep(x)
        if self.enable_coreml_compatible_fn:
            patches, output_size = self.unfolding_coreml(fm)
        else:
            patches, output_size = self.unfolding_pytorch(fm)
        for global_layer in self.global_rep:
            if isinstance(global_layer, LinearAttnFFN):
                patches = global_layer(x=patches, x_prev=x_prev)
            else:
                patches = global_layer(patches)
        if self.enable_coreml_compatible_fn:
            fm = self.folding_coreml(patches=patches, output_size=output_size)
        else:
            fm = self.folding_pytorch(patches=patches, output_size=output_size)
        fm = self.conv_proj(fm)
        return fm, patches

    def forward(self, x: 'Union[Tensor, Tuple[Tensor]]', *args, **kwargs) ->Union[Tensor, Tuple[Tensor, Tensor]]:
        if isinstance(x, Tuple) and len(x) == 2:
            return self.forward_temporal(x=x[0], x_prev=x[1])
        elif isinstance(x, Tensor):
            return self.forward_spatial(x)
        else:
            raise NotImplementedError


class SSDInstanceHead(BaseModule):
    """
    Instance segmentation head for SSD model.
    """

    def __init__(self, opts, in_channels: 'int', n_classes: 'Optional[int]'=1, inner_dim: 'Optional[int]'=256, output_stride: 'Optional[int]'=1, output_size: 'Optional[int]'=8, *args, **kwargs) ->None:
        """

        Args:
            opts: command-line arguments
            in_channels (int): :math:`C` from an expected input of size :math:`(N, C, H, W)`
            n_classes (Optional[int]): Number of classes. Default: 1
            inner_dim: (Optional[int]): Inner dimension of the instance head. Default: 256
            output_stride (Optional[int]): Output stride of the feature map. Output stride is the ratio of input to
                the feature map size. Default: 1
            output_size (Optional[int]): Output size of the instances extracted from RoIAlign layer. Default: 8
        """
        super().__init__()
        self.roi_align = RoIAlign(output_size=output_size, spatial_scale=1.0 / output_stride, sampling_ratio=2, aligned=True)
        self.seg_head = nn.Sequential(TransposeConvLayer2d(opts=opts, in_channels=in_channels, out_channels=inner_dim, kernel_size=2, stride=2, bias=True, use_norm=False, use_act=True, auto_padding=False, padding=0, output_padding=0), ConvLayer2d(opts=opts, in_channels=inner_dim, out_channels=n_classes, kernel_size=1, stride=1, use_norm=False, use_act=False, bias=True))
        self.inner_channels = inner_dim
        self.in_channels = in_channels
        self.mask_classes = n_classes
        self.reset_parameters()

    def __repr__(self) ->str:
        return '{}(in_channels={}, up_out_channels={}, n_classes={})'.format(self.__class__.__name__, self.in_channels, self.inner_channels, self.mask_classes)

    def reset_parameters(self) ->None:
        for layer in self.modules():
            if isinstance(layer, (nn.Conv2d, nn.ConvTranspose2d)):
                initialize_conv_layer(module=layer, init_method='kaiming_normal')

    def forward(self, x: 'Tensor', boxes: 'Tensor', *args, **kwargs) ->Tensor:
        rois = self.roi_align(x, boxes)
        rois = self.seg_head(rois)
        return rois


class WindowedTransformerEncoder(transformer.TransformerEncoder):
    """
    This class defines the pre-norm `Transformer encoder <https://arxiv.org/abs/1706.03762>`_
    with the addition of windowed attention.

    This class first partitions the input sequence into a series of windows (with
    an optional offset to use when defining windows). Then, it calls a
    TransformerEncoder module. Then, it undoes windowing.

    Args:
        opts: Command line arguments.
        embed_dim: :math:`C_{in}` from an expected input of size :math:`(N, P, C_{in})`.
        ffn_latent_dim: Inner dimension of the FFN.
        num_heads: Number of heads in multi-head attention. Default: 8.
        attn_dropout: Dropout rate for attention in multi-head attention. Default: 0.0.
        dropout: Dropout rate. Default: 0.0.
        ffn_dropout: Dropout between FFN layers. Default: 0.0.
        transformer_norm_layer: Normalization layer. Default: layer_norm.
        stochastic_dropout: Stochastic dropout setting. Default: 0.0.
        window_size: The size of the window, if using windowed attention. Default: None.
        window_shift: The size of the shift, if using shifted windowed attention. Default: None.

    Shape:
        - Input: :math:`(N, P, C_{in})` where :math:`N` is batch size, :math:`P` is number of patches,
        and :math:`C_{in}` is input embedding dim
        - Output: same shape as the input
    """

    def __init__(self, opts: 'argparse.Namespace', embed_dim: 'int', ffn_latent_dim: 'int', num_heads: 'Optional[int]'=8, attn_dropout: 'Optional[float]'=0.0, dropout: 'Optional[float]'=0.0, ffn_dropout: 'Optional[float]'=0.0, transformer_norm_layer: 'Optional[str]'='layer_norm', stochastic_dropout: 'Optional[float]'=0.0, window_size: 'Optional[int]'=None, window_shift: 'Optional[int]'=None, *args, **kwargs) ->None:
        super().__init__(opts=opts, embed_dim=embed_dim, ffn_latent_dim=ffn_latent_dim, num_heads=num_heads, attn_dropout=attn_dropout, dropout=dropout, ffn_dropout=ffn_dropout, transformer_norm_layer=transformer_norm_layer, stochastic_dropout=stochastic_dropout)
        if window_size is None:
            raise ValueError('Please specify window_size')
        if window_shift is None:
            raise ValueError('Please specify window_shift')
        self.window_size: 'int' = window_size
        self.window_shift: 'int' = window_shift

    def forward(self, x: 'Tensor', x_prev: 'Optional[Tensor]'=None, key_padding_mask: 'Optional[Tensor]'=None, attn_mask: 'Optional[Tensor]'=None, *args, **kwargs) ->Tensor:
        """
        Compute the outputs of the WindowedTransformerEncoder on an input.

        Args:
            x: The input tensor, of shape [batch_size, sequence_length, embed_dim].
            x_prev: The context input, if using cross-attention. Its shape is
                [batch_size, sequence_length_2, embed_dim].
            key_padding_mask: An optional tensor of masks to be applied to the
                inputs @x. Its shape is [batch_size, sequence_length].
            attn_mask: An optional attention mask. Its shape is [batch_size,
                sequence_length, sequence_length_2]. (If using self-attention,
                the sequence lengths will be equal.)

        Returns:
            The WindowedTransformerEncoder output.
        """
        B, N, C = x.shape
        x, windowed_key_padding_mask, windows_mask = window_x_and_key_padding_mask(x, key_padding_mask, self.window_size, self.window_shift)
        total_mask = windowed_key_padding_mask.unsqueeze(1) + windows_mask
        if attn_mask is not None:
            total_mask += attn_mask
        fully_masked_windows = total_mask.max(dim=-1).values == float('-inf')
        total_mask[fully_masked_windows] = 0
        x = super().forward(x, x_prev, attn_mask=attn_mask)
        x = unwindow_x(x, B, N, C, self.window_shift)
        return x

    def __repr__(self) ->str:
        ret = super().__repr__()[:-1]
        return f'{ret}, {self.window_size}, {self.window_shift})'


class BaseTextEncoder(nn.Module):
    """Base class for text encoder"""

    def __init__(self, opts, projection_dim: 'int', *args, **kwargs) ->None:
        is_master_node = is_master(opts)
        vocab_size = getattr(opts, 'dataset.text_vocab_size')
        if getattr(opts, 'common.debug_mode', False):
            vocab_size = 100
        if vocab_size is None and is_master_node:
            logger.error("Vocabulary size can't be None or -1 in {}. Got: {}".format(self.__class__.__name__, vocab_size))
        super(BaseTextEncoder, self).__init__()
        self.opts = opts
        self.projection_dim = projection_dim
        self.is_master_node = is_master_node
        self.vocab_size = vocab_size

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser') ->argparse.ArgumentParser:
        """Add model specific arguments"""
        group = parser.add_argument_group(title=cls.__name__)
        group.add_argument('--model.text.name', type=str, default=None, help='Name of the text encoder')
        return parser

    def reset_parameters(self):
        """Initialize model weights"""
        initialize_weights(opts=self.opts, modules=self.modules())

    def get_trainable_parameters(self, weight_decay: 'Optional[float]'=0.0, no_decay_bn_filter_bias: 'Optional[bool]'=False, *args, **kwargs):
        param_list = parameter_list(*args, named_parameters=self.named_parameters, weight_decay=weight_decay, no_decay_bn_filter_bias=no_decay_bn_filter_bias, **kwargs)
        return param_list, [1.0] * len(param_list)

    def freeze_norm_layers(self) ->None:
        for m in self.modules():
            if isinstance(m, norm_layers_tuple):
                m.eval()
                m.weight.requires_grad = False
                m.bias.requires_grad = False
                m.training = False

    def forward(self, text_tokens: 'Tensor', key_padding_mask: 'Optional[Tensor]'=None, attn_mask: 'Optional[Tensor]'=None, *args, **kwargs) ->Any:
        raise NotImplementedError

    def dummy_input_and_label(self, batch_size: 'int') ->Dict:
        """Create dummy input and labels for CI/CD purposes. Child classes must override it
        if functionality is different.
        """
        seq_length = 77
        vocab_size = 10
        text_tensor = torch.randint(low=0, high=vocab_size, size=(batch_size, seq_length)).long()
        return {'text': text_tensor}


class TextTransformer(BaseTextEncoder):

    def __init__(self, opts, projection_dim: 'int', *args, **kwargs) ->None:
        model_dim = getattr(opts, 'model.text.transformer.model_dim', 512)
        no_scale_embedding = getattr(opts, 'model.text.transformer.no_scale_embedding', False)
        no_pos_embedding = getattr(opts, 'model.text.transformer.no_pos_embedding', False)
        embed_dropout = getattr(opts, 'model.text.transformer.embed_dropout', 0.0)
        dropout = getattr(opts, 'model.text.transformer.dropout', 0.0)
        attn_dropout = getattr(opts, 'model.text.transformer.attn_dropout', 0.0)
        ffn_dropout = getattr(opts, 'model.text.transformer.ffn_dropout', 0.0)
        norm_layer = getattr(opts, 'model.text.transformer.norm_layer', None)
        gradient_ckpt = getattr(opts, 'model.text.transformer.gradient_checkpoint', False)
        if norm_layer is None:
            logger.error('Normalization layer can not be None in {}'.format(self.__class__.__name__))
        super().__init__(*args, opts=opts, projection_dim=projection_dim, **kwargs)
        padding_index = getattr(opts, 'dataset.padding_index', None)
        self.embedding_layer = Embedding(opts=opts, embedding_dim=model_dim, padding_idx=padding_index, num_embeddings=self.vocab_size)
        self.embed_scale = 1.0 if no_scale_embedding else model_dim ** -0.5
        context_length = getattr(opts, 'dataset.text_context_length')
        if getattr(opts, 'common.debug_mode', False):
            context_length = 77
        assert context_length is not None, "Context length can't be None. Please set dataset.text_context_length argument in your dataset class"
        self.positional_embedding = None if no_pos_embedding else PositionalEmbedding(opts=opts, num_embeddings=context_length, embedding_dim=model_dim, padding_idx=getattr(opts, 'dataset.padding_index', None), is_learnable=not getattr(opts, 'model.text.transformer.sinusoidal_pos_emb', False))
        self.embedding_dropout = Dropout(p=embed_dropout)
        n_transformer_layers = getattr(opts, 'model.text.transformer.n_transformer_layers', 6)
        ffn_multipliers = getattr(opts, 'model.text.transformer.ffn_multiplier_per_layer', 4.0)
        if isinstance(ffn_multipliers, (float, int)):
            ffn_multipliers = [ffn_multipliers] * n_transformer_layers
        if not isinstance(ffn_multipliers, Sequence):
            logger.error('{} expects FFN multipliers as a list, whose length is the same as number of transformer layers. Got: {}'.format(self.__class__.__name__, type(ffn_multipliers)))
        elif isinstance(ffn_multipliers, Sequence) and len(ffn_multipliers) != n_transformer_layers:
            logger.error('We need FFN multiplier for each transformer layer. Got {} ffn multipliers while number of transformer layers = {}'.format(len(ffn_multipliers), n_transformer_layers))
        ffn_dims = [int(math.ceil(model_dim * ffn_mult / 16.0) * 16.0) for ffn_mult in ffn_multipliers]
        mha_heads = getattr(opts, 'model.text.transformer.n_heads_per_layer', 8)
        if isinstance(mha_heads, int):
            mha_heads = [mha_heads] * n_transformer_layers
        if not isinstance(mha_heads, Sequence):
            logger.error('{} expects MHA heads as a list, whose length is the same as number of transformer layers. Got: {}'.format(self.__class__.__name__, type(mha_heads)))
        elif isinstance(mha_heads, Sequence) and len(mha_heads) != n_transformer_layers:
            logger.error('{} needs MHA heads for each transformer layer. Got {} mha heads while number of transformer layers = {}'.format(self.__class__.__name__, len(mha_heads), n_transformer_layers))
        self.transformer = nn.ModuleList([TransformerEncoder(opts=opts, embed_dim=model_dim, num_heads=mha_heads[layer_idx], ffn_latent_dim=ffn_dims[layer_idx], attn_dropout=attn_dropout, ffn_dropout=ffn_dropout, dropout=dropout, transformer_norm_layer=norm_layer) for layer_idx in range(n_transformer_layers)])
        self.final_layer_norm = get_normalization_layer(opts, num_features=model_dim, norm_type=norm_layer)
        self.projection_layer = nn.Parameter(torch.empty(model_dim, self.projection_dim))
        self.model_dim = model_dim
        self.reset_parameters_clip_style()
        self.gradient_ckpt = gradient_ckpt
        self.use_pytorch_mha = False
        self.causal_masking = getattr(opts, 'model.text.transformer.causal_masking', False)
        self.classes_per_split_zero_shot = max(1, int(getattr(opts, 'model.text.transformer.classes_per_split_zero_shot', 1)))

    def reset_parameters_clip_style(self):
        """This function resets the weights of Transformer model as done in the CLIP paper"""
        nn.init.normal_(self.embedding_layer.weight, mean=0.0, std=0.02)
        attn_std = self.model_dim ** -0.5
        proj_std = attn_std * (2 * len(self.transformer)) ** -0.5
        fc_std = (2 * self.model_dim) ** -0.5
        for block in self.transformer:
            nn.init.normal_(block.pre_norm_mha[1].qkv_proj.weight, mean=0.0, std=attn_std)
            nn.init.normal_(block.pre_norm_mha[1].out_proj.weight, mean=0.0, std=proj_std)
            nn.init.normal_(block.pre_norm_ffn[1].weight, mean=0.0, std=fc_std)
            nn.init.normal_(block.pre_norm_ffn[4].weight, mean=0.0, std=proj_std)
        nn.init.normal_(self.projection_layer, mean=0.0, std=attn_std)

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser') ->argparse.ArgumentParser:
        if cls != TextTransformer:
            return parser
        group = parser.add_argument_group(title=cls.__name__)
        group.add_argument('--model.text.transformer.model-dim', type=int, default=512, help='Model dimension of the transformer model')
        group.add_argument('--model.text.transformer.no-scale-embedding', action='store_true', help='Do not scale the output of embedding layer in {}'.format(cls.__name__))
        group.add_argument('--model.text.transformer.no-pos-embedding', action='store_true', help='Do not add positional embeddings to the output of embedding layer in {}'.format(cls.__name__))
        group.add_argument('--model.text.transformer.embed-dropout', type=float, default=0.0, help='Dropout in embedding layer')
        default_layers = 6
        group.add_argument('--model.text.transformer.n-transformer-layers', type=int, default=default_layers, help='Number of transformer layers in {}'.format(cls.__name__))
        group.add_argument('--model.text.transformer.n-heads-per-layer', type=int, default=[8] * default_layers, nargs='+', help='Number of transformer heads per transformer layer')
        group.add_argument('--model.text.transformer.ffn-multiplier-per-layer', type=float, default=[4.0] * default_layers, nargs='+', help='FFN multiplier for each transformer layer')
        group.add_argument('--model.text.transformer.attn-dropout', type=float, default=0.0, help='Dropout in multi-head attention')
        group.add_argument('--model.text.transformer.ffn-dropout', type=float, default=0.0, help='Dropout between linear layers in FFN')
        group.add_argument('--model.text.transformer.dropout', type=float, default=0.0, help='Dropout in transformer')
        group.add_argument('--model.text.transformer.norm-layer', type=str, default='layer_norm', help='Normalization layer')
        group.add_argument('--model.text.transformer.sinusoidal-pos-emb', action='store_true', help='Use sinusoidal positional embedding')
        group.add_argument('--model.text.transformer.gradient-checkpoint', action='store_true', help='Use gradient checkpointing')
        group.add_argument('--model.text.transformer.num-checkpoint-segments', type=int, default=1, help='Number of gradient checkpoint segments')
        group.add_argument('--model.text.transformer.causal-masking', action='store_true', help='Use causal masking')
        group.add_argument('--model.text.transformer.classes-per-split-zero-shot', type=int, default=20, help='Divide zero-shot classes into these many chunks, for faster processing')
        return parser

    def forward_embedding(self, text_tokens: 'Tensor'):
        token_emb = self.embedding_layer(text_tokens)
        seq_len = token_emb.shape[1]
        if self.positional_embedding is not None:
            token_emb = token_emb + self.positional_embedding(seq_len)
        token_emb = self.embedding_dropout(token_emb)
        return token_emb

    def build_attention_mask(self, context_length: 'int', batch_size: 'int'):
        mask = torch.empty(context_length, context_length)
        mask.fill_(float('-inf'))
        mask.triu_(1)
        if not self.use_pytorch_mha:
            mask = mask.unsqueeze(0)
            mask = mask.expand(batch_size, -1, -1)
        return mask

    def encode_text(self, text_tokens: 'Tensor', key_padding_mask: 'Optional[Tensor]'=None, return_all_tokens: 'bool'=False, *args, **kwargs) ->Tensor:
        """
        Returns token embeddings.

        :param text_tokens: a tensor of token indices. ([Batch, Seq_len])
        :param key_padding_mask: a tensor of boolean values as the padding mask.
        :param return_all_tokens: a boolean flag to return all tokens, defaults to False
            to return only EOT token embedding.
        :return: a tensor of [Batch, Seq_len, hidden_dim] if return_all_tokens is True,
            otherwise a tensor of [Batch, hidden_dim].
        """
        token_emb = self.forward_embedding(text_tokens)
        attn_mask = None
        if self.causal_masking:
            attn_mask = self.build_attention_mask(context_length=text_tokens.shape[1], batch_size=text_tokens.shape[0])
            attn_mask = attn_mask
            key_padding_mask = None
        if self.use_pytorch_mha:
            token_emb = token_emb.transpose(0, 1)
        for layer in self.transformer:
            if self.gradient_ckpt:
                token_emb = gradient_checkpoint_fn(layer, token_emb, None, key_padding_mask, attn_mask)
            else:
                token_emb = layer(token_emb, key_padding_mask=key_padding_mask, attn_mask=attn_mask, use_pytorch_mha=self.use_pytorch_mha)
        token_emb = self.final_layer_norm(token_emb)
        if return_all_tokens:
            if self.use_pytorch_mha:
                token_emb = token_emb.transpose(0, 1)
            return token_emb
        if self.use_pytorch_mha:
            token_emb = token_emb[text_tokens.argmax(dim=-1), torch.arange(text_tokens.shape[0])]
        else:
            token_emb = token_emb[torch.arange(text_tokens.shape[0]), text_tokens.argmax(dim=-1)]
        token_emb = token_emb @ self.projection_layer
        token_emb = F.normalize(token_emb, dim=-1)
        return token_emb

    def forward_zero_shot(self, text_tokens: 'Tensor', key_padding_mask: 'Optional[Tensor]'=None, *args, **kwargs) ->Tensor:
        if self.training:
            raise NotImplementedError('Zero-shot evaluation is only supported with eval mode')
        if text_tokens.ndim != 4:
            logger.error('For zero-shot evaluation, expected size of text is [Batch, Num_classes, num_captions, context_len]')
        batch_size, num_classes, num_captions, context_len = text_tokens.shape
        if batch_size > 1:
            text_tokens = text_tokens[0:1]
            batch_size = 1
            logger.warning('For zero-shot evaluation, text templates are the same across all images in the batch.Got: {}. Please consider adjusting collate function.'.format(batch_size))
        text_features = []
        for start_idx in range(0, num_classes, self.classes_per_split_zero_shot):
            end_idx = min(start_idx + self.classes_per_split_zero_shot, num_classes)
            text_tokens_split = text_tokens[0, start_idx:end_idx, ...]
            num_classes_split = text_tokens_split.shape[0]
            text_tokens_split = text_tokens_split.reshape(num_classes_split * num_captions, context_len)
            key_padding_mask_split = None
            if key_padding_mask is not None:
                key_padding_mask_split = key_padding_mask[0, start_idx:end_idx, ...]
                key_padding_mask_split = key_padding_mask_split.reshape(num_classes_split * num_captions, context_len)
            class_embedding_split = self.encode_text(text_tokens=text_tokens_split, key_padding_mask=key_padding_mask_split)
            class_embedding_split = class_embedding_split.reshape(num_classes_split, num_captions, class_embedding_split.shape[-1])
            mean_class_embedding_split = class_embedding_split.mean(dim=1)
            mean_class_embedding_split = F.normalize(mean_class_embedding_split, dim=-1)
            text_features.append(mean_class_embedding_split)
        text_features = torch.cat(text_features, dim=0)
        text_features = text_features.transpose(0, 1)
        return text_features.contiguous()

    def forward(self, text_tokens: 'Tensor', key_padding_mask: 'Optional[Tensor]'=None, *args, **kwargs) ->Tensor:
        if text_tokens.dim() == 4:
            return self.forward_zero_shot(*args, text_tokens=text_tokens, key_padding_mask=key_padding_mask, **kwargs)
        elif text_tokens.dim() == 2:
            text_tokens = self.encode_text(*args, text_tokens=text_tokens, key_padding_mask=key_padding_mask, **kwargs)
            return text_tokens
        elif text_tokens.dim() == 3:
            b, n, _ = text_tokens.shape
            text_tokens = text_tokens.reshape(b * n, -1)
            if key_padding_mask:
                key_padding_mask = key_padding_mask.reshape(b * n, -1)
            text_tokens = self.encode_text(*args, text_tokens=text_tokens, key_padding_mask=key_padding_mask, **kwargs)
            text_tokens = text_tokens.reshape(b, n, -1)
            return text_tokens
        else:
            raise NotImplementedError


class BaseTokenizer(nn.Module):

    def __init__(self, opts, *args, **kwargs):
        super().__init__()
        self.opts = opts

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser') ->argparse.ArgumentParser:
        group = parser.add_argument_group(title=cls.__name__)
        group.add_argument('--text-tokenizer.name', type=str, default=None, help='Name of the text tokenizer.')
        return parser

    def get_vocab_size(self):
        raise NotImplementedError

    def get_eot_token(self):
        raise NotImplementedError

    def get_sot_token(self):
        raise NotImplementedError

    def get_encodings(self):
        raise NotImplementedError

    def forward(self, input_sentence: 'Any', *args, **kwargs) ->Any:
        raise NotImplementedError


class ClipTokenizer(BaseTokenizer):

    def __init__(self, opts, *args, **kwargs):
        merges_path = getattr(opts, 'text_tokenizer.clip.merges_path', None)
        if merges_path is None:
            logger.error('Please specify BPE merge file using --text-tokenizer.clip.merges-path argument')
        merges_path = get_local_path(opts, path=merges_path)
        encoder_json_path = getattr(opts, 'text_tokenizer.clip.encoder_json_path', None)
        if encoder_json_path is None:
            logger.error('Please specify Encoder JSON file using --text-tokenizer.clip.encoder-json-path argument')
        encoder_json_path = get_local_path(opts, path=encoder_json_path)
        super().__init__(opts, *args, **kwargs)
        self.tokenizer = CLIPTokenizer(merges_path=merges_path, encoder_json_path=encoder_json_path)
        self.bpe_encodings = self.tokenizer.bpe.bpe_encoder_

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser') ->argparse.ArgumentParser:
        group = parser.add_argument_group(title=cls.__name__)
        group.add_argument('--text-tokenizer.clip.merges-path', type=str, default=None, help='Path to bpe merges file.')
        group.add_argument('--text-tokenizer.clip.encoder-json-path', type=str, default=None, help='Optional, path to BPE encoder json file. When specified, this is used to infer num_merges.')
        return parser

    def get_vocab_size(self):
        return len(self.bpe_encodings)

    def get_encodings(self):
        return self.bpe_encodings

    def get_eot_token(self):
        return int(self.tokenizer('<|endoftext|>')[0])

    def get_sot_token(self):
        return int(self.tokenizer('<|startoftext|>')[0])

    def forward(self, input_sentence: 'str', *args, **kwargs) ->Tensor:
        input_sentence = '<|startoftext|> ' + input_sentence + ' <|endoftext|>'
        tokenized_sentence = self.tokenizer(input_sentence)
        tokenized_sentence = torch.tensor([int(cap) for cap in tokenized_sentence], dtype=torch.long)
        return tokenized_sentence


class BaseCriteria(nn.Module, abc.ABC):
    """Base class for defining loss functions. Sub-classes must implement compute_loss function.

    Args:
        opts: command line arguments
    """

    def __init__(self, opts: 'argparse.Namespace', *args, **kwargs) ->None:
        super(BaseCriteria, self).__init__()
        self.opts = opts
        self.eps = 1e-07

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser') ->argparse.ArgumentParser:
        """Add criterion-specific arguments to the parser."""
        if cls != BaseCriteria:
            return parser
        group = parser.add_argument_group(cls.__name__)
        group.add_argument('--loss.category', type=str, default=None, help='Loss function category (e.g., classification). Defaults to None.')
        return parser

    @abc.abstractmethod
    def forward(self, input_sample: 'Any', prediction: 'Any', target: 'Any', *args, **kwargs) ->Any:
        """Compute the loss.

        Args:
            input_sample: Input to the model.
            prediction: Model's output
            target: Ground truth labels
        """
        raise NotImplementedError

    def extra_repr(self) ->str:
        return ''

    def __repr__(self) ->str:
        return '{}({}\n)'.format(self.__class__.__name__, self.extra_repr())


class BaseClassificationCriteria(BaseCriteria):
    """Base class for defining classification loss functions. Sub-classes must implement forward function.

    Args:
        opts: command line arguments
    """

    def __init__(self, opts: 'argparse.Namespace', *args, **kwargs) ->None:
        super().__init__(opts, *args, **kwargs)

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser') ->argparse.ArgumentParser:
        """Add criterion-specific arguments to the parser."""
        if cls != BaseClassificationCriteria:
            return parser
        group = parser.add_argument_group(cls.__name__)
        group.add_argument('--loss.classification.name', type=str, default=None, help=f'Name of the loss function in {cls.__name__}. Defaults to None.')
        return parser

    def _compute_loss(self, prediction: 'Tensor', target: 'Tensor', *args, **kwargs) ->Tensor:
        """Sub-classes must override this function to compute loss

        Args:
            prediction: Output of the model
            target: ground truth

        Returns:
            Expected to return a scalar value of loss
        """
        raise NotImplementedError

    def forward(self, input_sample: 'Any', prediction: 'Union[Dict[str, Tensor], Tensor]', target: 'Tensor', *args, **kwargs) ->Tensor:
        """Computes the cross entropy loss.

        Args:
            input_sample: Input image tensor to model.
            prediction: Output of model. It can be a tensor or mapping of (string: Tensor). In case of a dictionary,
            `logits` is a required key.
            target: Target label tensor containing values in the range `[0, C)`, where :math:`C` is the number of classes

        Shapes:
            input_sample: This loss function does not care about this argument.
            prediction:
                * When prediction is a tensor, then shape is [N, C]
                * When prediction is a dictionary, then the shape of prediction["logits"] is [N, C]
            target: The shape of target tensor is [N]

        Returns:
            Scalar loss value is returned.
        """
        if isinstance(prediction, Tensor):
            return self._compute_loss(*args, prediction=prediction, target=target, **kwargs)
        elif isinstance(prediction, Dict):
            if 'logits' not in prediction:
                logger.error(f'logits is a required key in {self.__class__.__name__} when prediction typeis dictionary. Got keys: {prediction.keys()}')
            predicted_logits = prediction['logits']
            if predicted_logits is None:
                logger.error('Predicted logits can not be None.')
            ce_loss = self._compute_loss(*args, prediction=predicted_logits, target=target, **kwargs)
            return ce_loss
        else:
            logger.error(f'Prediction should be either a Tensor or Dictionary[str, Tensor]. Got: {type(prediction)}')


class BinaryCrossEntropy(BaseClassificationCriteria):
    """Binary cross-entropy loss for classification tasks

    Args:
        opts: command-line arguments
    """

    def __init__(self, opts: 'argparse.Namespace', *args, **kwargs) ->None:
        super().__init__(opts, *args, **kwargs)
        self.reduction = getattr(opts, 'loss.classification.binary_cross_entropy.reduction')

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser') ->argparse.ArgumentParser:
        if cls != BinaryCrossEntropy:
            return parser
        group = parser.add_argument_group(cls.__name__)
        group.add_argument('--loss.classification.binary-cross-entropy.reduction', type=str, default='mean', choices=['sum', 'mean', 'none', 'batch_mean'], help="Specifies the reduction to apply to the output (default='mean'). 'batch_mean' divides the sum of the loss only by the first dimension.")
        return parser

    def _compute_loss(self, prediction: 'Tensor', target: 'Tensor', *args, **kwargs) ->Tensor:
        """The binary cross-entropy loss with logits for binary classification.
        The probability for class one is the Sigmoid on the logit.
        For multi-class problems with multiple valid labels, the loss penalizes by
        the given target probability of the same shape as predictions.

        Args:
            prediction: Logits of class 1
            target: Ground-truth class index or probability.

        Shapes:
            prediction: [Batch size, ...]
            target: A tensor of similar shape to prediction if the target
                probability for each output is known. Or a tensor of ground-truth labels
                missing the last dimension of size `num_classes`.

        Returns:
            If reduction is none, then tensor of the same shape as prediction is returned.
            Otherwise, a scalar loss value is returned.
        """
        if target.dim() == prediction.dim() - 1:
            target = F.one_hot(target, num_classes=prediction.shape[-1])
        div_by = 1.0
        if self.reduction == 'batch_mean':
            div_by = target.shape[0]
        reduction = self.reduction if self.reduction != 'batch_mean' else 'sum'
        bce_loss = F.binary_cross_entropy_with_logits(input=prediction, target=target, reduction=reduction)
        return bce_loss / div_by

    def extra_repr(self) ->str:
        return f'\n\t reduction={self.reduction}'


def compute_class_weights(target: 'Tensor', n_classes: 'int', norm_val: 'float'=1.1) ->Tensor:
    """Implementation of a class-weighting scheme, as defined in Section 5.2
    of `ENet <https://arxiv.org/pdf/1606.02147.pdf>`_ paper.

    Args:
        target: Tensor of shape [Batch_size, *] containing values in the range `[0, C)`.
        n_classes: Integer specifying the number of classes :math:`C`
        norm_val: Normalization value. Defaults to 1.1. This value is decided based on the
        `ESPNetv2 paper <https://arxiv.org/abs/1811.11431>`_.
        Link: https://github.com/sacmehta/ESPNetv2/blob/b78e323039908f31347d8ca17f49d5502ef1a594/segmentation/loadData.py#L16

    Returns:
        A :math:`C`-dimensional tensor containing class weights
    """
    class_hist = torch.histc(target.float(), bins=n_classes, min=0, max=n_classes - 1)
    None
    mask_indices = class_hist == 0
    norm_hist = torch.div(class_hist, class_hist.sum())
    None
    norm_hist = torch.add(norm_hist, norm_val)
    class_wts = torch.div(torch.ones_like(class_hist), torch.log(norm_hist))
    class_wts[mask_indices] = 0.0
    return class_wts


class CrossEntropy(BaseClassificationCriteria):
    """Cross entropy loss function for image classification tasks

    Args:
        opts: command-line arguments
    """

    def __init__(self, opts: 'argparse.Namespace', *args, **kwargs) ->None:
        super().__init__(opts, *args, **kwargs)
        self.ignore_idx = getattr(opts, 'loss.classification.cross_entropy.ignore_index')
        self.use_class_wts = getattr(opts, 'loss.classification.cross_entropy.class_weights')
        self.label_smoothing = getattr(opts, 'loss.classification.cross_entropy.label_smoothing')

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser') ->argparse.ArgumentParser:
        """Add cross-entropy criterion-specific arguments to the parser."""
        if cls != CrossEntropy:
            return parser
        group = parser.add_argument_group(title=cls.__name__)
        group.add_argument('--loss.classification.cross-entropy.class-weights', action='store_true', default=False, help=f'Use class weights in {cls.__name__}. Defaults to False.')
        group.add_argument('--loss.classification.cross-entropy.ignore-index', type=int, default=-1, help=f'Target value that is ignored and does not contribute to the input gradient in {cls.__name__}. Defaults to -1.')
        group.add_argument('--loss.classification.cross-entropy.label-smoothing', type=float, default=0.0, help=f'Specifies the amount of smoothing when computing the loss in {cls.__name__}, where 0.0 means no smoothing. Defaults to 0.0.')
        return parser

    def _compute_loss(self, prediction: 'Tensor', target: 'Tensor', *args, **kwargs) ->Tensor:
        """Computes cross-entropy loss between prediction and target tensors.

        Args:
            prediction: Predicted tensor of shape [N, C]
            target: Target label tensor of shape [N] containing values between [0, C),

            Here, :math:`C` is the number of classes and :math:`N` is the batch size

        Returns:
            A scalar loss value
        """
        weight = None
        if self.use_class_wts and self.training:
            n_classes = prediction.shape[1]
            weight = compute_class_weights(target=target, n_classes=n_classes)
        label_smoothing_val = self.label_smoothing if self.training else 0.0
        return F.cross_entropy(input=prediction, target=target, weight=weight, ignore_index=self.ignore_idx, label_smoothing=label_smoothing_val)

    def extra_repr(self) ->str:
        return f'\n\t ignore_idx={self.ignore_idx}\n\t class_weighting={self.use_class_wts}\n\t label_smoothing={self.label_smoothing}'


def build_loss_fn(opts: 'argparse.Namespace', category: 'Optional[str]'='', *args, **kwargs) ->BaseCriteria:
    """Helper function to build loss function from command-line arguments.

    Args:
        opts: command-line arguments
        category: Optional task category (e.g., classification). Specifying category may be useful for
            building composite loss functions. See `loss_fns.composite_loss.CompositeLoss.build_composite_loss_fn`
            function for an example

    Returns:
        Loss function module
    """
    if not category:
        category = getattr(opts, 'loss.category')
    if category is None:
        logger.error('Please specify loss name using --loss.category. For composite loss function, see configurationexample in `loss_fns.composite_loss.CompositeLoss`. Got None')
    if hasattr(opts, f'loss.{category}.name'):
        loss_fn_name = getattr(opts, f'loss.{category}.name')
    else:
        loss_fn_name = category
    if loss_fn_name == '__base__':
        logger.error("__base__ can't be used as a loss function name. Please check.")
    loss_fn = LOSS_REGISTRY[loss_fn_name, category](opts, *args, **kwargs)
    return loss_fn


def flatten_yaml_as_dict(d, parent_key='', sep='.'):
    items = []
    for k, v in d.items():
        new_key = parent_key + sep + k if parent_key else k
        if isinstance(v, collections_abc.MutableMapping):
            items.extend(flatten_yaml_as_dict(v, new_key, sep=sep).items())
        else:
            items.append((new_key, v))
    return dict(items)


class CompositeLoss(BaseCriteria):
    """Combines different loss functions and returns the weighted sum of these losses.
    `loss_category` and `loss_weight` are two mandatory keys that allows us to combine
    different losses and compute their weighted sum. The `loss_category` specifies the category
    of a loss function and is a string (e.g., classification). The `loss_weight` specifies
    the contribution of a loss function and is a float value (e.g., 1.0). The sum of `loss_weight`s
    corresponding to different loss functions is not required to be 1.

    Args:
        opts: command-line arguments

    Example::
    # Example yaml config for combining classification and neural_augmentation loss function is given below.
    # Please note that configuration for each loss function should start with `-` in `composite_loss`.

    loss:
      category: "composite_loss"
      composite_loss:
        - loss_category: "classification"
          loss_weight: 1.0
          classification:
            name: "cross_entropy"
            cross_entropy:
              label_smoothing: 0.1
        - loss_category: "neural_augmentation"
          loss_weight: 1.0
          neural_augmentation:
            perceptual_metric: "psnr"
            target_value: [ 40, 10 ]
            curriculum_method: "cosine"
    """

    def __init__(self, opts: 'argparse.Namespace', *args, **kwargs) ->None:
        task_loss_fn_mapping, task_loss_wts_mapping = CompositeLoss.build_composite_loss_fn(opts, *args, **kwargs)
        super().__init__(opts, *args, **kwargs)
        self.loss_fns = task_loss_fn_mapping
        self.loss_weights = task_loss_wts_mapping

    @classmethod
    def build_composite_loss_fn(cls, opts: 'argparse.Namespace', *args, **kwargs) ->Tuple[Mapping[str, BaseCriteria], Mapping[str, float]]:
        """Build loss functions from command line arguments and loss registry

        Args:
            opts: command-line arguments

        Returns:
            A tuple of two dictionaries. The first dictionary, task_loss_fn_mapping, contains
            information about loss function category and module. The second dictionary, `task_loss_wts_mapping`
            contains the information about loss function category and weight.
        """
        composite_loss_opts = getattr(opts, 'loss.composite_loss')
        if composite_loss_opts is None:
            logger.error(f"{cls.__name__} can't be None. Please specify --loss.composite-loss using yaml file")
        if not isinstance(composite_loss_opts, List):
            logger.error(f'{cls.__name__} options are expected as a List. Got type: {type(composite_loss_opts)} and values: {composite_loss_opts}')
        num_loss_fns = len(composite_loss_opts)
        if num_loss_fns < 1:
            logger.error(f'We need at least one loss function if using {cls.__name__}')
        task_loss_fn_mapping = {}
        task_loss_wts_mapping = {}
        for i, composite_loss_opts_as_dict in enumerate(composite_loss_opts):
            if 'loss_category' not in composite_loss_opts_as_dict:
                logger.error('loss_category is a mandatory key')
            if 'loss_weight' not in composite_loss_opts_as_dict:
                logger.error('Loss weight is a mandatory')
            loss_category = composite_loss_opts_as_dict.pop('loss_category')
            loss_weight = composite_loss_opts_as_dict.pop('loss_weight')
            if not isinstance(loss_weight, (float, int)):
                logger.error(f'loss weight should be either int or float. Got: value={loss_weight}, type={type(loss_weight)}')
            composite_loss_opts_as_dict = flatten_yaml_as_dict(composite_loss_opts_as_dict)
            loss_opts = copy.deepcopy(opts)
            for k, v in composite_loss_opts_as_dict.items():
                setattr(loss_opts, 'loss.' + k, v)
            task_loss_fn_mapping[loss_category] = build_loss_fn(*args, opts=loss_opts, category=loss_category, **kwargs)
            task_loss_wts_mapping[loss_category] = loss_weight
        is_intersection = task_loss_fn_mapping.keys().isdisjoint(task_loss_wts_mapping)
        assert is_intersection is False, f'The keys in task_loss_fn_mapping and task_loss_wts_mapping are not the same. Got: {task_loss_fn_mapping.keys()} and {task_loss_wts_mapping.keys()}'
        return task_loss_fn_mapping, task_loss_wts_mapping

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser') ->argparse.ArgumentParser:
        """Add criterion-specific arguments to the parser."""
        if cls != CompositeLoss:
            return parser
        group = parser.add_argument_group(cls.__name__)
        group.add_argument('--loss.composite-loss', type=json.loads, action='append')
        return parser

    def forward(self, input_sample: 'Any', prediction: 'Any', target: 'Any', *args, **kwargs) ->Any:
        """Compute the weighted sum of different loss functions.

        Args:
            input_sample: Input to the model.
            prediction: Model's output
            target: Ground truth labels

        Returns:
            A mapping of the form (str: scalar loss value) with `total_loss` as a mandatory key.
            The other keys corresponds to loss category names and their values contain category-specific
            scalar loss values. total_loss is weighted sum of these category-specific losses.
        """
        outputs = {}
        total_loss = 0.0
        for loss_name, loss_layer in self.loss_fns.items():
            loss_wt = self.loss_weights[loss_name]
            loss_val = loss_layer(*args, input_sample=input_sample, prediction=prediction, target=target, **kwargs)
            if not isinstance(loss_val, (Tensor, Mapping)):
                logger.error('Loss value is expected as a scalar or dictionary of scalars with total_loss as a mandatory key.')
            if isinstance(loss_val, Mapping) and 'total_loss' in loss_val:
                loss_val = loss_val['total_loss']
                if not isinstance(loss_val, Tensor):
                    logger.error(f'Value corresponding to total_loss key in {loss_val} is expected to be scalar.Got: {type(loss_val)}')
            loss_val = loss_val * loss_wt
            total_loss += loss_val
            outputs[loss_name] = loss_val
        outputs.update({'total_loss': total_loss})
        return outputs

    def train(self, mode: 'bool'=True) ->None:
        """Sets the loss functions in training mode."""
        for loss_name, loss_layer in self.loss_fns.items():
            loss_layer.train(mode=mode)

    def eval(self) ->None:
        """Sets the loss functions in evaluation mode."""
        for loss_name, loss_layer in self.loss_fns.items():
            loss_layer.eval()

    def __repr__(self) ->str:
        repr_str = f'{self.__class__.__name__}(\n\t'
        for k, v in self.loss_fns.items():
            repr_str += v.__repr__().replace('\n\t', ' ').replace('\n)', f' loss_wt={self.loss_weights[k]})')
            repr_str += '\n\t'
        repr_str += '\n)'
        return repr_str


def extract_opts_with_prefix_replacement(opts: 'argparse.Namespace', match_prefix: 'str', replacement_prefix: 'str') ->argparse.Namespace:
    """
    Helper function to extract a copy options with certain prefix and return them with an alternative prefix.
    An example usage is distillation, when we have used @extend_selected_args_with_prefix to add --teacher.model.*
        arguments to argparser, and now we want to re-use the handlers of model.* opts by teacher.model.* opts

    Args:
        match_prefix: Prefix to select opts for extraction.
            The value should not contain dashes and should end with "."
        replacement_prefix: Prefix to replace the @match_prefix
            The value should not contain dashes and should end with "."
    """
    regexp = '[^-]+\\.'
    assert re.match(regexp, match_prefix), f"match prefix '{match_prefix}' should match regexp '{regexp}'"
    assert re.match(regexp, replacement_prefix), f"replacement prefix '{replacement_prefix}' should match regexp '{regexp}'"
    opts_dict = vars(opts)
    result_dict = {key.replace(match_prefix, replacement_prefix): value for key, value in opts_dict.items() if key.startswith(match_prefix)}
    return argparse.Namespace(**result_dict)


def build_cls_teacher_from_opts(opts: 'argparse.Namespace') ->nn.Module:
    """Helper function to build a classification teacher model from command-line arguments

    Args:
        opts: command-line arguments

    Returns:
        A teacher model
    """
    pretrained_model = getattr(opts, 'teacher.model.classification.pretrained')
    pytest_env = is_test_env()
    if not pytest_env and pretrained_model is None:
        logger.error('For distillation, please specify teacher weights using teacher.model.classification.pretrained')
    teacher_opts = extract_opts_with_prefix_replacement(opts, 'teacher.model.', 'model.')
    return get_model(teacher_opts, category='classification')


class BaseDistillationCriteria(BaseCriteria):
    """Base class for defining distillation loss functions. Sub-classes must implement `_forward_distill` function.

    Args:
        opts: command line arguments
    """

    def __init__(self, opts: 'argparse.Namespace', *args, **kwargs) ->None:
        super().__init__(opts, *args, **kwargs)
        self.teacher = build_cls_teacher_from_opts(opts=opts)

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser') ->argparse.ArgumentParser:
        if cls != BaseDistillationCriteria:
            return parser
        group = parser.add_argument_group(cls.__name__)
        group.add_argument('--loss.distillation.name', type=str, default=None, help='Name of the loss function. Defaults to None.')
        return parser

    @torch.no_grad()
    def _logits_from_teacher(self, input_sample: 'Tensor') ->Tensor:
        """Compute logits from teacher given input image tensor.

        Args:
            input_sample: Input image tensor

        Shape:
            input_sample: Shape is [Batch size, 3, height, width]
            teacher_output or teacher_output["logits"]: Shape is [Batch size, number of classes]

        Returns:
            Teacher output tensor (without softmax)

        ...note:
            The output of teacher can be Tensor or Dict[str, Tensor]. In case
            of dictionary, logits is a mandatory key.
        """
        self.teacher.eval()
        teacher_output: 'Union[Tensor, Mapping[str, Tensor]]' = self.teacher(input_sample)
        if isinstance(teacher_output, Mapping):
            if 'logits' not in teacher_output:
                logger.error(f'The output type of teacher is dictionary and must contain logits as a key.Got: {teacher_output.keys()}')
            return teacher_output['logits']
        return teacher_output

    def _forward_distill(self, input_sample: 'Tensor', prediction: 'Tensor', *args, **kwargs) ->Tensor:
        """Computes distillation loss.

        Args:
            input_sample: Input image tensor
            prediction: Student model's output.

        Shapes:
            input_sample: Shape is [Batch size, 3, height, width]
            prediction: Shape is [Batch size, number of classes]

        Returns:
            A scalar loss value.
        """
        raise NotImplementedError

    def forward(self, input_sample: 'Tensor', prediction: 'Union[Mapping[str, Tensor], Tensor]', target: 'Tensor', *args, **kwargs) ->Union[Mapping[str, Tensor], Tensor]:
        """Computes distillation loss

        Args:
            input_sample: Input image tensor.
            prediction: Output of model. It can be a tensor or mapping of (string: Tensor). In case of a dictionary,
            `logits` is a required key.
            target: Target label tensor containing values in the range `[0, C)`, where :math:`C` is the number of classes

        Shapes:
            input_sample: The shape of input tensor is [N, C, H, W]
            prediction:
                * When prediction is a tensor, then shape is [N, C]
                * When prediction is a dictionary, then shape of prediction["logits"] is [N, C]
            target: The shape of target tensor is [N]

        Returns:
            * Scalar loss value is returned.
        """
        if isinstance(prediction, Tensor):
            return self._forward_distill(*args, input_sample=input_sample, prediction=prediction, **kwargs)
        elif isinstance(prediction, Mapping):
            if 'logits' not in prediction:
                logger.error(f'logits is a required key in {self.__class__.__name__} when prediction typeis dictionary. Got keys: {prediction.keys()}')
            predicted_logits = prediction['logits']
            distill_loss = self._forward_distill(*args, input_sample=input_sample, prediction=predicted_logits, **kwargs)
            return distill_loss
        else:
            logger.error(f'Prediction should be either a Tensor or Dictionary[str, Tensor]. Got: {type(prediction)}')


class HardDistillationLoss(BaseDistillationCriteria):
    """Hard distillation using cross-entropy for classification tasks. Given an input sample, hard-labels
    are generated from a teacher and cross-entropy loss is computed between hard-labels and student model's output.

    Args:
        opts: command-line arguments
    """

    def __init__(self, opts: 'argparse.Namespace', *args, **kwargs) ->None:
        top_k = getattr(opts, 'loss.distillation.hard_distillation.topk')
        if top_k < 1:
            logger.error(f'The value of top-k should be greater than 0. Got: {top_k}')
        label_smoothing = getattr(opts, 'loss.distillation.hard_distillation.label_smoothing')
        if not 0.0 <= label_smoothing < 1.0:
            logger.error(f'The value of label smoothing should be between 0 and 1. Got: {label_smoothing}')
        super().__init__(opts, *args, **kwargs)
        self.topk = top_k
        self.label_smoothing = label_smoothing

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser') ->argparse.ArgumentParser:
        if cls != HardDistillationLoss:
            return parser
        group = parser.add_argument_group(title=cls.__name__)
        group.add_argument('--loss.distillation.hard-distillation.topk', type=int, default=1, help=f'Distill top-k labels from teacher when in {cls.__name__}. Defaults to 1.')
        group.add_argument('--loss.distillation.hard-distillation.label-smoothing', type=float, default=0.0, help=f'Specifies the amount of smoothing when computing the classification loss in {cls.__name__}, where 0.0 means no smoothing. Defaults to 0.0.')
        return parser

    def _forward_distill(self, input_sample: 'Tensor', prediction: 'Tensor', *args, **kwargs) ->Tensor:
        """
        Computes cross entropy loss between students and hard labels generated from teacher.

        Args:
            input_sample: Input image tensor
            prediction: Output of student model

        Shapes:
            input_sample: Shape is [Batch size, 3, height, width]
            prediction: Shape is [Batch size, Number of classes]

        Returns:
            A scalar loss value

        ...note:
            When top-k labels extracted from teacher are used for distillation, binary cross entropy loss is used.
        """
        with torch.no_grad():
            teacher_logits = self._logits_from_teacher(input_sample)
            teacher_probs = F.softmax(teacher_logits, dim=-1).detach()
            _, teacher_topk_labels = torch.topk(teacher_probs, k=self.topk, dim=-1, largest=True, sorted=True)
        if self.topk > 1:
            num_classes = prediction.shape[-1]
            teacher_topk_labels = F.one_hot(teacher_topk_labels, num_classes=num_classes)
            teacher_topk_labels = teacher_topk_labels.sum(1)
            teacher_topk_labels = teacher_topk_labels
            smooth_class_p = (1.0 - self.label_smoothing) / self.topk
            smooth_non_class_p = self.label_smoothing / (num_classes - self.topk)
            teacher_topk_labels = torch.where(teacher_topk_labels == 1.0, smooth_class_p, smooth_non_class_p)
            loss = F.binary_cross_entropy_with_logits(input=prediction, target=teacher_topk_labels, reduction='mean') * num_classes
        else:
            teacher_topk_labels = teacher_topk_labels.reshape(-1)
            loss = F.cross_entropy(input=prediction, target=teacher_topk_labels, reduction='mean', label_smoothing=self.label_smoothing)
        return loss

    def extra_repr(self) ->str:
        return f'\n\t topk={self.topk}\n\tlabel_smoothing={self.label_smoothing}'


class SoftKLLoss(BaseDistillationCriteria):
    """Soft KL Loss for classification tasks. Given an input sample, soft-labels (or probabilities)
    are generated from a teacher and KL loss is computed between soft-labels and student model's output.

    Args:
        opts: command-line arguments
    """

    def __init__(self, opts: 'argparse.Namespace', *args, **kwargs) ->None:
        temperature = getattr(opts, 'loss.distillation.soft_kl_loss.temperature')
        if temperature <= 0.0:
            logger.error(f'The value of temperature in {self.__class__.__name__} should be positive.')
        super().__init__(opts, *args, **kwargs)
        self.temperature = temperature

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser') ->argparse.ArgumentParser:
        if cls != SoftKLLoss:
            return parser
        group = parser.add_argument_group(title=cls.__name__)
        group.add_argument('--loss.distillation.soft-kl-loss.temperature', type=float, default=1.0, help=f'Temperature for KL divergence loss in {cls.__name__}. Defaults to 1.')
        return parser

    def _forward_distill(self, input_sample: 'Tensor', prediction: 'Tensor', *args, **kwargs) ->Tensor:
        """Computes distillation loss.

        Args:
            input_sample: Input image tensor
            prediction: Student model's output.

        Shapes:
            input_sample: Shape is [Batch size, 3, height, width]
            prediction: Shape is [Batch size, number of classes]

        Returns:
            A scalar loss value.
        """
        with torch.no_grad():
            teacher_logits = self._logits_from_teacher(input_sample)
            teacher_lprobs = F.log_softmax(teacher_logits / self.temperature, dim=1).detach()
        student_lprobs = F.log_softmax(prediction / self.temperature, dim=-1)
        kl_loss = F.kl_div(student_lprobs, teacher_lprobs, reduction='batchmean', log_target=True)
        return kl_loss * self.temperature ** 2

    def extra_repr(self) ->str:
        return f'\n\t temperature={self.temperature}'


def cosine_curriculum(start: 'int', end: 'int', period: 'int') ->Tensor:
    """This function implements cosine curriculum
    Args:
        start: the starting value for the set of points
        end: the ending value for the set of points
        period: size of the constructed tensor

    Returns:
        A float tensor of length period
    """
    curr = [(end + 0.5 * (start - end) * (1 + math.cos(math.pi * i / (period + 1)))) for i in range(period + 1)]
    curr = torch.tensor(curr, dtype=torch.float)
    return curr


def linear_curriculum(start: 'int', end: 'int', period: 'int') ->Tensor:
    """This function implements linear curriculum

    Args:
        start: the starting value for the set of points
        end: the ending value for the set of points
        period: size of the constructed tensor

    Returns:
        A float tensor of length period
    """
    return torch.linspace(start=start, end=end, steps=period + 1, dtype=torch.float)


CURRICULUM_METHOD = {'linear': linear_curriculum, 'cosine': cosine_curriculum}


class NeuralAugmentation(BaseCriteria):
    """Compute the augmentation loss, as described in the
    `RangeAugment <https://arxiv.org/abs/2212.10553>`_ paper.

    Args:
        opts: command line arguments
    """
    __supported_metrics = ['psnr']

    def __init__(self, opts: 'argparse.Namespace', *args, **kwargs) ->None:
        super().__init__(opts, *args, **kwargs)
        perceptual_metric = getattr(opts, 'loss.neural_augmentation.perceptual_metric')
        is_master_node = is_master(opts)
        if perceptual_metric is None and is_master_node:
            logger.error("Perceptual metric can't be none. Please specify perceptual metric using --loss.auxiliary.neural-augmentation.perceptual-metric argument")
        if not isinstance(perceptual_metric, str) and is_master_node:
            logger.error('The type of perceptual metric is not string. Got: {}'.format(type(perceptual_metric)))
        perceptual_metric = perceptual_metric.lower()
        target_value = getattr(opts, 'loss.neural_augmentation.target_value')
        self.curriculumn_learning = False
        self.iteration_based_training = getattr(opts, 'scheduler.is_iteration_based', False)
        self.target_str = f'{target_value}'
        alpha = getattr(opts, 'loss.neural_augmentation.alpha')
        if perceptual_metric == 'psnr':
            if target_value is None and is_master_node:
                logger.error('Target PSNR value can not be None.')
            if isinstance(target_value, (int, float)):
                if target_value < 0:
                    if is_master_node:
                        logger.error('PSNR value should be >= 0 in {}. Got: {}'.format(self.__class__.__name__, target_value))
                target_mse = 10.0 ** ((20.0 * math.log10(255.0) - target_value) / 10.0)
                self.target_value = torch.ones(size=(1,), dtype=torch.float).fill_(target_mse)
                self.target_str = f'{target_value}'
            elif isinstance(target_value, (list, tuple)) and len(target_value) == 2:
                start_target_value = target_value[0]
                end_target_value = target_value[1]
                if start_target_value < 0 or end_target_value < 0:
                    if is_master_node:
                        logger.error('PSNR value should be >= 0 in {}. Got: {}'.format(self.__class__.__name__, target_value))
                start_target_mse = 10.0 ** ((20.0 * math.log10(255.0) - start_target_value) / 10.0)
                end_target_mse = 10.0 ** ((20.0 * math.log10(255.0) - end_target_value) / 10.0)
                max_steps = getattr(opts, 'scheduler.max_iterations') if self.iteration_based_training else getattr(opts, 'scheduler.max_epochs')
                if max_steps is None and is_master_node:
                    logger.error('Please specify {}. Got None.'.format('--scheduler.max-iterations' if self.iteration_based_training else '--scheduler.max-epochs'))
                curriculum_method = getattr(opts, 'loss.neural_augmentation.curriculum_method')
                if curriculum_method in CURRICULUM_METHOD.keys():
                    self.target_value = CURRICULUM_METHOD[curriculum_method](start=start_target_mse, end=end_target_mse, period=max_steps)
                else:
                    raise NotImplementedError
                self.curriculumn_learning = True
                self.target_str = f'[{start_target_value}, {end_target_value}]'
            else:
                raise NotImplementedError
            self.alpha = alpha / 65025.0
        elif is_master_node:
            logger.error('Supported perceptual metrics are: {}. Got: {}'.format(self.__supported_metrics, perceptual_metric))
        self.perceptual_metric = perceptual_metric
        self.device = getattr(opts, 'dev.device', torch.device('cpu'))

    @classmethod
    def add_arguments(cls, parser: 'argparse.ArgumentParser') ->argparse.ArgumentParser:
        group = parser.add_argument_group(cls.__name__)
        group.add_argument('--loss.neural-augmentation.perceptual-metric', type=str, default='psnr', choices=cls.__supported_metrics, help=f'Name of the perceptual metric to be used in {cls.__name__}.')
        group.add_argument('--loss.neural-augmentation.target-value', type=float, default=[40, 20], nargs='+', help=f'Target image similarity value in {cls.__name__}. Defaults to [40, 20]')
        group.add_argument('--loss.neural-augmentation.curriculum-method', type=str, default='cosine', choices=['linear', 'cosine'], help=f'Curriculum for varying the target image similarity value in {cls.__name__}.Supported curriculums are {cls.__supported_metrics}. Defaults to cosine')
        group.add_argument('--loss.neural-augmentation.alpha', default=100.0, type=float, help='Scale loss value by alpha value. Defaults to 100. Note: When perceptual metric is PSNR, alpha value is divided by 65025')
        return parser

    def _forward_psnr(self, input_tensor: 'Tensor', augmented_tensor: 'Tensor', *args, **kwargs) ->Tensor:
        """Compute the MSE error between input and augmented image, and minimizes
        the distance between MSE error and target error.

        Args:
            input_tensor: Input image of shape [N, C, H, W]
            augmented_tensor: Augmented image of shape [N, C, H, W]

        Returns:
            A scalar loss value
        """
        squared_err = ((augmented_tensor - input_tensor) * 255.0) ** 2
        pred_mse = torch.mean(squared_err, dim=[1, 2, 3])
        if self.curriculumn_learning:
            step = kwargs.get('iterations', 0) if self.iteration_based_training else kwargs.get('epoch', 0)
            if step >= len(self.target_value):
                step = -1
            target_mse = self.target_value[step]
        else:
            target_mse = self.target_value
        smooth_l1_loss = F.smooth_l1_loss(input=pred_mse, target=target_mse.expand_as(pred_mse), reduction='mean')
        loss_na = smooth_l1_loss * self.alpha
        return loss_na

    def _compute_loss(self, input_tensor: 'Tensor', augmented_tensor: 'Tensor', *args, **kwargs) ->Tensor:
        """Compute the neural augmentation loss.

        Args:
            input_tensor: Input image of shape [N, C, H, W]
            augmented_tensor: Augmented image of shape [N, C, H, W]

        Returns:
            A scalar value
        """
        if augmented_tensor is None:
            logger.error(f"Augmented tensor can't be None in {self.__class__.__name__} during training mode.")
        forward_loss_fn = getattr(self, f'_forward_{self.perceptual_metric}')
        loss_na = forward_loss_fn(*args, input_tensor=input_tensor, augmented_tensor=augmented_tensor, **kwargs)
        return loss_na

    def forward(self, input_sample: 'Union[Tensor, Mapping[str, Union[Tensor, List[Tensor]]]]', prediction: 'Mapping[str, Tensor]', *args, **kwargs) ->Tensor:
        """Compute the loss between input and augmented image, as described in
        `RangeAugment <https://arxiv.org/abs/2212.10553>`_ paper.

        Args:
            input_sample: Input sample can either be a Tensor or a dictionary with mandatory key "image". In
                case of a dictionary, the values can be a Tensor or list of Tensors.
            prediction: Output of augmentation model. Mapping of (string: Tensor) with `augmented_tensor`
                as the required key.

        Shapes:
            input_sample:
                * Tensor: The shape of input tensor is [N, C, H, W]
                * Mapping[str, Tensor]: The shape of tensor is [N, C, H, W]
                * Mapping[str, List[Tensor]]: The length of List is N, and the shape of each tensor is [1, C, H, W]
            prediction: The shape of prediction["augmented_tensor"] is [N, C, H, W]

        Returns:
            A scalar loss value

        ...note:
            During validation or evaluation, neural augmentation loss is not computed and 0 is returned
        """
        if not self.training:
            return torch.tensor(0.0, device=self.device, dtype=torch.float)
        if not isinstance(prediction, Mapping):
            logger.error('Prediction needs to be an instance of Mapping and must contain augmented_tensor as keys')
        if isinstance(input_sample, Mapping):
            input_sample = input_sample['image']
        if isinstance(input_sample, List):
            input_sample = torch.stack(input_sample, dim=0)
        augmented_tensor = prediction['augmented_tensor']
        loss_na = self._compute_loss(*args, input_tensor=input_sample, augmented_tensor=augmented_tensor, **kwargs)
        return loss_na

    def extra_repr(self) ->str:
        return '\n\t target_metric={}\n\t target_value={}\n\t curriculum_learning={}\n\t alpha={}'.format(self.perceptual_metric, self.target_str, self.curriculumn_learning, self.alpha)


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (AdaptiveAvgPool2d,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (AvgPool2d,
     lambda: ([], {'kernel_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (BaseNeuralAugmentor,
     lambda: ([], {'opts': _mock_config(model.learn_augmentation.lr_multiplier=4)}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (BatchNorm1d,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     True),
    (BatchNorm2d,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (BatchNorm2dFP32,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (BatchNorm3d,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4, 4])], {}),
     True),
    (Clip,
     lambda: ([], {'min_val': 4, 'max_val': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (Conv2d,
     lambda: ([], {'in_channels': 4, 'out_channels': 4, 'kernel_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (DistributionNeuralAugmentor,
     lambda: ([], {'opts': _mock_config(model.learn_augmentation.lr_multiplier=4, model.learn_augmentation.brightness=4, model.learn_augmentation.contrast=4, model.learn_augmentation.noise=4)}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (Dropout,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Dropout2d,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FixedSampler,
     lambda: ([], {'value': 4}),
     lambda: ([], {}),
     False),
    (Flatten,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (GELU,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (GlobalPool,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (GlobalPool2D,
     lambda: ([], {'opts': _mock_config(model.image_projection_head.lr_multiplier=4, model.image_projection_head.global_pool_nchw2nc.identity_if_same_size=4, ddp.rank=4, model.image_projection_head.global_pool_nchw2nc.no_feature_normalization=4), 'in_dim': 4, 'out_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (GroupLinear,
     lambda: ([], {'in_features': 4, 'out_features': 4, 'n_groups': 1}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (GroupNorm,
     lambda: ([], {'num_groups': 1, 'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Identity,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (InstanceNorm1d,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     True),
    (InstanceNorm2d,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (LayerNorm,
     lambda: ([], {'normalized_shape': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (LayerNorm2D_NCHW,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (LayerNormFP32,
     lambda: ([], {'normalized_shape': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (LeakyReLU,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (LearnablePositionEncoding,
     lambda: ([], {'embed_dim': 4, 'num_embeddings': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (MaxPool2d,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (PReLU,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ReLU,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ReLU6,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Sigmoid,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SimpleImageProjectionHead,
     lambda: ([], {'opts': _mock_config(model.image_projection_head.lr_multiplier=4, model.image_projection_head.simple_projection_nc2nc.identity_if_same_size=4, ddp.rank=4, model.image_projection_head.simple_projection_nc2nc.no_feature_normalization=4), 'in_dim': 4, 'out_dim': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     False),
    (SinusoidalPositionalEncoding,
     lambda: ([], {'d_model': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (Softmax,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Swish,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SyncBatchNorm,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (SyncBatchNormFP32,
     lambda: ([], {'num_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (Tanh,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (UniformSampler,
     lambda: ([], {'low': 4, 'high': 4}),
     lambda: ([], {}),
     False),
]

class Test_apple_ml_cvnets(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])

    def test_015(self):
        self._check(*TESTCASES[15])

    def test_016(self):
        self._check(*TESTCASES[16])

    def test_017(self):
        self._check(*TESTCASES[17])

    def test_018(self):
        self._check(*TESTCASES[18])

    def test_019(self):
        self._check(*TESTCASES[19])

    def test_020(self):
        self._check(*TESTCASES[20])

    def test_021(self):
        self._check(*TESTCASES[21])

    def test_022(self):
        self._check(*TESTCASES[22])

    def test_023(self):
        self._check(*TESTCASES[23])

    def test_024(self):
        self._check(*TESTCASES[24])

    def test_025(self):
        self._check(*TESTCASES[25])

    def test_026(self):
        self._check(*TESTCASES[26])

    def test_027(self):
        self._check(*TESTCASES[27])

    def test_028(self):
        self._check(*TESTCASES[28])

    def test_029(self):
        self._check(*TESTCASES[29])

    def test_030(self):
        self._check(*TESTCASES[30])

    def test_031(self):
        self._check(*TESTCASES[31])

    def test_032(self):
        self._check(*TESTCASES[32])

    def test_033(self):
        self._check(*TESTCASES[33])

    def test_034(self):
        self._check(*TESTCASES[34])

    def test_035(self):
        self._check(*TESTCASES[35])

    def test_036(self):
        self._check(*TESTCASES[36])

    def test_037(self):
        self._check(*TESTCASES[37])

    def test_038(self):
        self._check(*TESTCASES[38])

    def test_039(self):
        self._check(*TESTCASES[39])

