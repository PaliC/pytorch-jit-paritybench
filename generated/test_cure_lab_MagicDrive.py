import sys
_module = sys.modules[__name__]
del sys
helper = _module
interactive_gui = _module
run = _module
run_cond_on_view = _module
dataset = _module
dataset_wrapper = _module
nuscenes_dataset = _module
pipeline = _module
pipeline_utils = _module
utils = _module
common = _module
inception = _module
test_utils = _module
bbox_embedder = _module
blocks = _module
embedder = _module
map_embedder = _module
output_cls = _module
unet_2d_condition_multiview = _module
unet_addon_rawbox = _module
pipeline_bev_controlnet = _module
pipeline_bev_controlnet_given_view = _module
base_runner = _module
base_validator = _module
box_visualizer = _module
img_utils = _module
map_visualizer = _module
multiview_runner = _module
utils = _module
ddp_utils = _module
nuscenes_utils = _module
val_set_gen = _module
_config = _module
bit_diffusion = _module
checkpoint_merger = _module
clip_guided_images_mixing_stable_diffusion = _module
clip_guided_stable_diffusion = _module
clip_guided_stable_diffusion_img2img = _module
composable_stable_diffusion = _module
ddim_noise_comparative_analysis = _module
edict_pipeline = _module
imagic_stable_diffusion = _module
img2img_inpainting = _module
interpolate_stable_diffusion = _module
lpw_stable_diffusion = _module
lpw_stable_diffusion_onnx = _module
magic_mix = _module
mixture = _module
mixture_tiling = _module
multilingual_stable_diffusion = _module
one_step_unet = _module
sd_text2img_k_diffusion = _module
seed_resize_stable_diffusion = _module
speech_to_image_diffusion = _module
stable_diffusion_comparison = _module
stable_diffusion_controlnet_img2img = _module
stable_diffusion_controlnet_inpaint = _module
stable_diffusion_controlnet_inpaint_img2img = _module
stable_diffusion_controlnet_reference = _module
stable_diffusion_ipex = _module
stable_diffusion_mega = _module
stable_diffusion_reference = _module
stable_diffusion_repaint = _module
stable_diffusion_tensorrt_img2img = _module
stable_diffusion_tensorrt_inpaint = _module
stable_diffusion_tensorrt_txt2img = _module
stable_unclip = _module
text_inpainting = _module
tiled_upscaling = _module
unclip_image_interpolation = _module
unclip_text_interpolation = _module
wildcard_stable_diffusion = _module
conftest = _module
train_controlnet = _module
train_controlnet_flax = _module
retrieve = _module
train_custom_diffusion = _module
train_dreambooth = _module
train_dreambooth_flax = _module
train_dreambooth_lora = _module
image_to_image = _module
inpainting = _module
train_instruct_pix2pix = _module
inference = _module
train_dreambooth_colossalai = _module
train_dreambooth_inpaint = _module
train_dreambooth_inpaint_lora = _module
inference_bf16 = _module
textual_inversion_bf16 = _module
text2images = _module
textual_inversion = _module
train_text_to_image_lora = _module
multi_token_clip = _module
textual_inversion = _module
textual_inversion_flax = _module
train_multi_subject_dreambooth = _module
train_text_to_image = _module
textual_inversion = _module
train_unconditional = _module
run_diffuser_locomotion = _module
test_examples = _module
train_text_to_image = _module
train_text_to_image_flax = _module
train_text_to_image_lora = _module
textual_inversion = _module
textual_inversion_flax = _module
train_unconditional = _module
scripts = _module
change_naming_configs_and_checkpoints = _module
conversion_ldm_uncond = _module
convert_dance_diffusion_to_diffusers = _module
convert_ddpm_original_checkpoint_to_diffusers = _module
convert_diffusers_to_original_stable_diffusion = _module
convert_dit_to_diffusers = _module
convert_if = _module
convert_k_upscaler_to_diffusers = _module
convert_kakao_brain_unclip_to_diffusers = _module
convert_kandinsky_to_diffusers = _module
convert_ldm_original_checkpoint_to_diffusers = _module
convert_lora_safetensor_to_diffusers = _module
convert_models_diffuser_to_diffusers = _module
convert_ms_text_to_video_to_diffusers = _module
convert_music_spectrogram_to_diffusers = _module
convert_ncsnpp_original_checkpoint_to_diffusers = _module
convert_original_audioldm_to_diffusers = _module
convert_original_controlnet_to_diffusers = _module
convert_original_stable_diffusion_to_diffusers = _module
convert_stable_diffusion_checkpoint_to_onnx = _module
convert_unclip_txt2img_to_image_variation = _module
convert_unidiffuser_to_diffusers = _module
convert_vae_diff_to_onnx = _module
convert_vae_pt_to_diffusers = _module
convert_versatile_diffusion_to_diffusers = _module
convert_vq_diffusion_to_diffusers = _module
generate_logits = _module
setup = _module
diffusers = _module
commands = _module
diffusers_cli = _module
env = _module
configuration_utils = _module
dependency_versions_check = _module
dependency_versions_table = _module
experimental = _module
rl = _module
value_guided_sampling = _module
image_processor = _module
loaders = _module
models = _module
activations = _module
attention = _module
attention_flax = _module
attention_processor = _module
autoencoder_kl = _module
controlnet = _module
controlnet_flax = _module
cross_attention = _module
dual_transformer_2d = _module
embeddings = _module
embeddings_flax = _module
modeling_flax_pytorch_utils = _module
modeling_flax_utils = _module
modeling_pytorch_flax_utils = _module
modeling_utils = _module
prior_transformer = _module
resnet = _module
resnet_flax = _module
t5_film_transformer = _module
transformer_2d = _module
transformer_temporal = _module
unet_1d = _module
unet_1d_blocks = _module
unet_2d = _module
unet_2d_blocks = _module
unet_2d_blocks_flax = _module
unet_2d_condition = _module
unet_2d_condition_flax = _module
unet_3d_blocks = _module
unet_3d_condition = _module
vae = _module
vae_flax = _module
vq_model = _module
optimization = _module
pipelines = _module
alt_diffusion = _module
modeling_roberta_series = _module
pipeline_alt_diffusion = _module
pipeline_alt_diffusion_img2img = _module
audio_diffusion = _module
mel = _module
pipeline_audio_diffusion = _module
audioldm = _module
pipeline_audioldm = _module
multicontrolnet = _module
pipeline_controlnet = _module
pipeline_controlnet_img2img = _module
pipeline_controlnet_inpaint = _module
pipeline_flax_controlnet = _module
dance_diffusion = _module
pipeline_dance_diffusion = _module
ddim = _module
pipeline_ddim = _module
ddpm = _module
pipeline_ddpm = _module
deepfloyd_if = _module
pipeline_if = _module
pipeline_if_img2img = _module
pipeline_if_img2img_superresolution = _module
pipeline_if_inpainting = _module
pipeline_if_inpainting_superresolution = _module
pipeline_if_superresolution = _module
safety_checker = _module
timesteps = _module
watermark = _module
dit = _module
pipeline_dit = _module
kandinsky = _module
pipeline_kandinsky = _module
pipeline_kandinsky_img2img = _module
pipeline_kandinsky_inpaint = _module
pipeline_kandinsky_prior = _module
text_encoder = _module
latent_diffusion = _module
pipeline_latent_diffusion = _module
pipeline_latent_diffusion_superresolution = _module
latent_diffusion_uncond = _module
pipeline_latent_diffusion_uncond = _module
onnx_utils = _module
paint_by_example = _module
image_encoder = _module
pipeline_paint_by_example = _module
pipeline_flax_utils = _module
pipeline_utils = _module
pndm = _module
pipeline_pndm = _module
repaint = _module
pipeline_repaint = _module
score_sde_ve = _module
pipeline_score_sde_ve = _module
semantic_stable_diffusion = _module
pipeline_semantic_stable_diffusion = _module
spectrogram_diffusion = _module
continous_encoder = _module
midi_utils = _module
notes_encoder = _module
pipeline_spectrogram_diffusion = _module
stable_diffusion = _module
convert_from_ckpt = _module
pipeline_cycle_diffusion = _module
pipeline_flax_stable_diffusion = _module
pipeline_flax_stable_diffusion_controlnet = _module
pipeline_flax_stable_diffusion_img2img = _module
pipeline_flax_stable_diffusion_inpaint = _module
pipeline_onnx_stable_diffusion = _module
pipeline_onnx_stable_diffusion_img2img = _module
pipeline_onnx_stable_diffusion_inpaint = _module
pipeline_onnx_stable_diffusion_inpaint_legacy = _module
pipeline_onnx_stable_diffusion_upscale = _module
pipeline_stable_diffusion = _module
pipeline_stable_diffusion_attend_and_excite = _module
pipeline_stable_diffusion_controlnet = _module
pipeline_stable_diffusion_depth2img = _module
pipeline_stable_diffusion_diffedit = _module
pipeline_stable_diffusion_image_variation = _module
pipeline_stable_diffusion_img2img = _module
pipeline_stable_diffusion_inpaint = _module
pipeline_stable_diffusion_inpaint_legacy = _module
pipeline_stable_diffusion_instruct_pix2pix = _module
pipeline_stable_diffusion_k_diffusion = _module
pipeline_stable_diffusion_latent_upscale = _module
pipeline_stable_diffusion_model_editing = _module
pipeline_stable_diffusion_panorama = _module
pipeline_stable_diffusion_pix2pix_zero = _module
pipeline_stable_diffusion_sag = _module
pipeline_stable_diffusion_upscale = _module
pipeline_stable_unclip = _module
pipeline_stable_unclip_img2img = _module
safety_checker = _module
safety_checker_flax = _module
stable_unclip_image_normalizer = _module
stable_diffusion_safe = _module
pipeline_stable_diffusion_safe = _module
safety_checker = _module
stochastic_karras_ve = _module
pipeline_stochastic_karras_ve = _module
text_to_video_synthesis = _module
pipeline_text_to_video_synth = _module
pipeline_text_to_video_zero = _module
unclip = _module
pipeline_unclip = _module
pipeline_unclip_image_variation = _module
text_proj = _module
unidiffuser = _module
modeling_text_decoder = _module
modeling_uvit = _module
pipeline_unidiffuser = _module
versatile_diffusion = _module
modeling_text_unet = _module
pipeline_versatile_diffusion = _module
pipeline_versatile_diffusion_dual_guided = _module
pipeline_versatile_diffusion_image_variation = _module
pipeline_versatile_diffusion_text_to_image = _module
vq_diffusion = _module
pipeline_vq_diffusion = _module
schedulers = _module
scheduling_ddim = _module
scheduling_ddim_flax = _module
scheduling_ddim_inverse = _module
scheduling_ddpm = _module
scheduling_ddpm_flax = _module
scheduling_deis_multistep = _module
scheduling_dpmsolver_multistep = _module
scheduling_dpmsolver_multistep_flax = _module
scheduling_dpmsolver_multistep_inverse = _module
scheduling_dpmsolver_sde = _module
scheduling_dpmsolver_singlestep = _module
scheduling_euler_ancestral_discrete = _module
scheduling_euler_discrete = _module
scheduling_heun_discrete = _module
scheduling_ipndm = _module
scheduling_k_dpm_2_ancestral_discrete = _module
scheduling_k_dpm_2_discrete = _module
scheduling_karras_ve = _module
scheduling_karras_ve_flax = _module
scheduling_lms_discrete = _module
scheduling_lms_discrete_flax = _module
scheduling_pndm = _module
scheduling_pndm_flax = _module
scheduling_repaint = _module
scheduling_sde_ve = _module
scheduling_sde_ve_flax = _module
scheduling_sde_vp = _module
scheduling_unclip = _module
scheduling_unipc_multistep = _module
scheduling_utils = _module
scheduling_utils_flax = _module
scheduling_vq_diffusion = _module
training_utils = _module
accelerate_utils = _module
constants = _module
deprecation_utils = _module
doc_utils = _module
dummy_flax_and_transformers_objects = _module
dummy_flax_objects = _module
dummy_note_seq_objects = _module
dummy_onnx_objects = _module
dummy_pt_objects = _module
dummy_torch_and_librosa_objects = _module
dummy_torch_and_scipy_objects = _module
dummy_torch_and_torchsde_objects = _module
dummy_torch_and_transformers_and_k_diffusion_objects = _module
dummy_torch_and_transformers_and_onnx_objects = _module
dummy_torch_and_transformers_objects = _module
dummy_transformers_and_torch_and_note_seq_objects = _module
dynamic_modules_utils = _module
hub_utils = _module
import_utils = _module
logging = _module
outputs = _module
pil_utils = _module
testing_utils = _module
torch_utils = _module
tests = _module
pipeline = _module
what_ever = _module
test_activations = _module
test_attention_processor = _module
test_layers_utils = _module
test_lora_layers = _module
test_modeling_common = _module
test_modeling_common_flax = _module
test_models_unet_1d = _module
test_models_unet_2d = _module
test_models_unet_2d_condition = _module
test_models_unet_2d_flax = _module
test_models_unet_3d_condition = _module
test_models_vae = _module
test_models_vae_flax = _module
test_models_vq = _module
test_unet_2d_blocks = _module
test_unet_blocks_common = _module
test_check_copies = _module
test_check_dummies = _module
test_config = _module
test_ema = _module
test_hub_utils = _module
test_image_processor = _module
test_outputs = _module
test_training = _module
altdiffusion = _module
test_alt_diffusion = _module
test_alt_diffusion_img2img = _module
test_audio_diffusion = _module
test_audioldm = _module
test_controlnet = _module
test_controlnet_img2img = _module
test_controlnet_inpaint = _module
test_flax_controlnet = _module
test_dance_diffusion = _module
test_ddim = _module
test_ddpm = _module
deepfloyd_if = _module
test_if = _module
test_if_img2img = _module
test_if_img2img_superresolution = _module
test_if_inpainting = _module
test_if_inpainting_superresolution = _module
test_if_superresolution = _module
test_dit = _module
test_kandinsky = _module
test_kandinsky_img2img = _module
test_kandinsky_inpaint = _module
test_kandinsky_prior = _module
karras_ve = _module
test_karras_ve = _module
test_latent_diffusion = _module
test_latent_diffusion_superresolution = _module
test_latent_diffusion_uncond = _module
test_paint_by_example = _module
pipeline_params = _module
test_pndm = _module
test_repaint = _module
test_score_sde_ve = _module
test_semantic_diffusion = _module
test_spectrogram_diffusion = _module
test_cycle_diffusion = _module
test_onnx_stable_diffusion = _module
test_onnx_stable_diffusion_img2img = _module
test_onnx_stable_diffusion_inpaint = _module
test_onnx_stable_diffusion_inpaint_legacy = _module
test_onnx_stable_diffusion_upscale = _module
test_stable_diffusion = _module
test_stable_diffusion_image_variation = _module
test_stable_diffusion_img2img = _module
test_stable_diffusion_inpaint = _module
test_stable_diffusion_inpaint_legacy = _module
test_stable_diffusion_instruction_pix2pix = _module
test_stable_diffusion_k_diffusion = _module
test_stable_diffusion_model_editing = _module
test_stable_diffusion_panorama = _module
test_stable_diffusion_pix2pix_zero = _module
test_stable_diffusion_sag = _module
stable_diffusion_2 = _module
test_stable_diffusion = _module
test_stable_diffusion_attend_and_excite = _module
test_stable_diffusion_depth = _module
test_stable_diffusion_diffedit = _module
test_stable_diffusion_flax = _module
test_stable_diffusion_flax_inpaint = _module
test_stable_diffusion_inpaint = _module
test_stable_diffusion_latent_upscale = _module
test_stable_diffusion_upscale = _module
test_stable_diffusion_v_pred = _module
test_safe_diffusion = _module
test_stable_unclip = _module
test_stable_unclip_img2img = _module
test_pipeline_utils = _module
test_pipelines = _module
test_pipelines_common = _module
test_pipelines_flax = _module
test_pipelines_onnx_common = _module
text_to_video = _module
test_text_to_video = _module
test_text_to_video_zero = _module
test_unclip = _module
test_unclip_image_variation = _module
test_unidiffuser = _module
test_versatile_diffusion_dual_guided = _module
test_versatile_diffusion_image_variation = _module
test_versatile_diffusion_mega = _module
test_versatile_diffusion_text_to_image = _module
test_vq_diffusion = _module
test_scheduler_ddim = _module
test_scheduler_ddpm = _module
test_scheduler_deis = _module
test_scheduler_dpm_multi = _module
test_scheduler_dpm_sde = _module
test_scheduler_dpm_single = _module
test_scheduler_euler = _module
test_scheduler_euler_ancestral = _module
test_scheduler_flax = _module
test_scheduler_heun = _module
test_scheduler_ipndm = _module
test_scheduler_kdpm2_ancestral = _module
test_scheduler_kdpm2_discrete = _module
test_scheduler_lms = _module
test_scheduler_pndm = _module
test_scheduler_score_sde_ve = _module
test_scheduler_unclip = _module
test_scheduler_unipc = _module
test_scheduler_vq_diffusion = _module
test_schedulers = _module
check_config_docstrings = _module
check_copies = _module
check_doc_toc = _module
check_dummies = _module
check_inits = _module
check_repo = _module
check_table = _module
custom_init_isort = _module
get_modified_files = _module
overwrite_expected_slice = _module
print_env = _module
release = _module
stale = _module
gpu_benchmark_diff = _module
run_benchmark_wrapper = _module
conf = _module
my_model = _module
cifar_MetaFormer = _module
cifar_ViT = _module
microGPT = _module
ragged_inference = _module
garbage_pad_ragged_acts = _module
seq_kv_cache = _module
test_utils = _module
triton_v2_matmul = _module
triton_v2_qk_dotprod = _module
triton_v2_ragged_qk_dotprod = _module
test_seq_kv_cache = _module
test_triton_v2_matmul = _module
test_triton_v2_qk_dotprod = _module
test_triton_v2_ragged_qk_dotprod = _module
build_conda = _module
compute_wheel_version = _module
setup = _module
torch_stub_tests = _module
test_attention_mask = _module
test_attention_patterns = _module
test_attention_utils = _module
test_attentions = _module
test_block_factory = _module
test_compositional_attention = _module
test_core_attention = _module
test_custom_ops = _module
test_embedding = _module
test_favor = _module
test_feedforward = _module
test_global_attention = _module
test_hierarchical_transformer = _module
test_hydra_helper = _module
test_indexing = _module
test_mem_eff_attention = _module
test_model_factory = _module
test_nystrom_attention = _module
test_ortho_attention = _module
test_pickling = _module
test_profiler = _module
test_pytorch_transformer_parity = _module
test_residual = _module
test_reversible = _module
test_rotary_embeddings = _module
test_sparse_tensors = _module
test_sparsecs = _module
test_swiglu = _module
test_timm_sparse = _module
test_triton_basics = _module
test_triton_blocksparse = _module
test_triton_dropout = _module
test_triton_fused_linear = _module
test_triton_layernorm = _module
test_triton_softmax = _module
test_unbind = _module
utils = _module
conv2d = _module
gemm = _module
gemm_grouped = _module
simt_sm50 = _module
conv2d_operation = _module
conv3d_operation = _module
gemm_operation = _module
generator = _module
library = _module
manifest = _module
conv2d_f16_sm80 = _module
gemm_f32_sm80 = _module
pycutlass = _module
arguments = _module
c_types = _module
compiler = _module
conv2d_operation = _module
epilogue = _module
frontend = _module
gemm_operation = _module
memory_manager = _module
operation = _module
parser = _module
reduction_operation = _module
tensor_ref = _module
test = _module
conv2d_testbed = _module
gemm_grouped_testbed = _module
gemm_testbed = _module
profiler = _module
type_hint = _module
reference_model = _module
conv = _module
conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80 = _module
conv2d_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80 = _module
conv2d_dgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80 = _module
conv2d_dgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80 = _module
conv2d_fprop_few_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80 = _module
conv2d_fprop_fixed_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80 = _module
conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80 = _module
conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80 = _module
conv2d_fprop_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80 = _module
conv2d_fprop_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80 = _module
conv2d_strided_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80 = _module
conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80 = _module
conv2d_wgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80 = _module
conv2d_wgrad_implicit_gemm_f32nhwc_f32nhwc_f32nhwc_simt_f32_sm80 = _module
conv2d_wgrad_implicit_gemm_tf32nhwc_tf32nhwc_f32nhwc_tensor_op_f32_sm80 = _module
run_all_tests = _module
test_frontend = _module
gemm_bf16_sm80 = _module
gemm_f16_sm80 = _module
gemm_f64_sm80 = _module
gemm_grouped_sm80 = _module
gemm_s8_sm80 = _module
test_sm80 = _module
rank_2k_operation = _module
rank_k_operation = _module
rt = _module
symm_operation = _module
trmm_operation = _module
xformers = _module
_cpp_lib = _module
LRA = _module
batch_fetch_results = _module
batch_submit = _module
code = _module
dataset = _module
model_wrapper = _module
run_grid_search = _module
run_tasks = _module
run_with_submitit = _module
cifar10 = _module
listops = _module
pathfinder = _module
retrieval = _module
text = _module
benchmarks = _module
benchmark_blocksparse_transformers = _module
benchmark_causal_blocksparse = _module
benchmark_core = _module
benchmark_encoder = _module
benchmark_indexing = _module
benchmark_mem_eff_attention = _module
benchmark_mlp = _module
benchmark_multi_head_dispatch = _module
benchmark_nvfuser = _module
benchmark_nystrom_utils = _module
benchmark_pytorch_transformer = _module
benchmark_revnet = _module
benchmark_sddmm = _module
benchmark_swiglu = _module
benchmark_transformer = _module
benchmark_triton_blocksparse = _module
benchmark_triton_dropout = _module
benchmark_triton_fused_linear = _module
benchmark_triton_layernorm = _module
benchmark_triton_softmax = _module
benchmark_triton_stride_sum = _module
utils = _module
components = _module
activations = _module
attention = _module
_sputnik_sparse = _module
attention_mask = _module
attention_patterns = _module
base = _module
blocksparse = _module
compositional = _module
core = _module
favor = _module
feature_maps = _module
base = _module
softmax = _module
fourier_mix = _module
global_tokens = _module
lambda_layer = _module
linformer = _module
local = _module
nystrom = _module
ortho = _module
pooling = _module
random = _module
scaled_dot_product = _module
sparsity_config = _module
utils = _module
visual = _module
feedforward = _module
base = _module
conv_mlp = _module
fused_mlp = _module
mixture_of_experts = _module
mlp = _module
input_projection = _module
multi_head_dispatch = _module
nvfuser = _module
bias_act_dropout = _module
bias_dropout_res = _module
bias_dropout_res_layernorm = _module
utils = _module
patch_embedding = _module
positional_embedding = _module
base = _module
param = _module
rotary = _module
sine = _module
vocab = _module
residual = _module
reversible = _module
simplicial_embedding = _module
generate_kernels = _module
factory = _module
block_configs = _module
block_factory = _module
hydra_helper = _module
model_factory = _module
weight_init = _module
helpers = _module
hierarchical_configs = _module
test_utils = _module
timm_sparse_attention = _module
info = _module
ops = _module
common = _module
fmha = _module
attn_bias = _module
common = _module
cutlass = _module
dispatch = _module
flash = _module
small_k = _module
triton = _module
indexing = _module
swiglu_op = _module
unbind = _module
api = _module
device_limits = _module
profiler = _module
slow_ops_profiler = _module
sparse = _module
_csr_ops = _module
blocksparse_tensor = _module
csr_tensor = _module
utils = _module
triton = _module
dropout = _module
fused_linear_layer = _module
k_activations = _module
k_dropout = _module
k_fused_matmul_bw = _module
k_fused_matmul_fw = _module
k_layer_norm = _module
k_softmax = _module
k_sum = _module
layer_norm = _module
softmax = _module
sum_strided = _module
utils = _module
create_data = _module
data_converter = _module
create_gt_database = _module
nuscenes_converter = _module
fid_score = _module
prepare_map_aux = _module
test = _module
train = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import operator as op
from dataclasses import dataclass
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


from typing import Tuple


from typing import List


from typing import Optional


import copy


import torch


import numpy as np


from torchvision.transforms.functional import to_pil_image


from copy import deepcopy


from functools import partial


from typing import Union


import logging


import warnings


from typing import Any


from typing import Dict


from numpy import random


import random


from functools import update_wrapper


import torch.nn as nn


import torch.nn.functional as F


import torchvision


import torch.utils.checkpoint


from typing import Callable


import inspect


import math


from torchvision.transforms.functional import to_tensor


import matplotlib.patches as mpatches


import matplotlib.pyplot as plt


from torchvision.transforms import InterpolationMode


from torch.nn import functional as F


from torchvision import transforms


from torch import nn


import time


import re


from torchvision import transforms as tfms


from enum import Enum


from collections import OrderedDict


from copy import copy


import types


from torch.utils.data import IterableDataset


import itertools


from torch.utils.data import Dataset


from typing import Iterable


from torchvision.datasets.utils import download_url


import numpy as onp


from torch.onnx import export


import functools


from collections import defaultdict


from torch import Tensor


from torch import device


from torch.optim import Optimizer


from torch.optim.lr_scheduler import LambdaLR


from math import acos


from math import sin


from itertools import repeat


from typing import Mapping


from typing import MutableMapping


from typing import Sequence


from logging import getLogger


from torch.nn.functional import grid_sample


from scipy import integrate


from random import random


from uuid import uuid4


from torch.utils.data import DataLoader


from torch.utils.data import RandomSampler


from functools import lru_cache


from torch.utils.cpp_extension import CUDA_HOME


from torch.utils.cpp_extension import BuildExtension


from torch.utils.cpp_extension import CppExtension


from torch.utils.cpp_extension import CUDAExtension


from typing import TypeVar


from typing import Type


from torch.utils.checkpoint import checkpoint


from torch.utils._python_dispatch import TorchDispatchMode


from torch.utils._python_dispatch import _get_current_dispatch_mode


from typing import ContextManager


from typing import cast


from torch.cuda.amp.autocast_mode import autocast


from typing import TYPE_CHECKING


import enum


from torch.utils.data.dataset import Dataset


from collections import namedtuple


from torch.utils import benchmark


import pandas as pd


from sklearn.model_selection import ParameterGrid


from torch.autograd.profiler import record_function


from torch.cuda.amp import autocast


from functools import reduce


from typing import Generator


from typing import Set


from abc import ABCMeta


from abc import abstractmethod


from enum import auto


import torch.autograd.profiler as profiler


import torch.nn.functional as Fn


from torch.nn.init import constant_


from torch.autograd.function import Function


from torch.utils.checkpoint import get_device_states


from torch.utils.checkpoint import set_device_states


from torch.nn.init import _calculate_fan_in_and_fan_out


from torch.nn.init import _no_grad_trunc_normal_


from torch.nn.init import _no_grad_uniform_


import queue


import torch.cuda.memory


import torch.cuda.nvtx


import torch.profiler


import torch.utils.hooks


from torch.utils._python_dispatch import _pop_mode_temporarily


from torch.utils._pytree import tree_map


from torch.cuda.amp import custom_bwd


from torch.cuda.amp import custom_fwd


import torchvision.transforms as TF


from scipy import linalg


from torch.nn.functional import adaptive_avg_pool2d


def _inception_v3(*args, **kwargs):
    """Wraps `torchvision.models.inception_v3`"""
    try:
        version = tuple(map(int, torchvision.__version__.split('.')[:2]))
    except ValueError:
        version = 0,
    if version >= (0, 6):
        kwargs['init_weights'] = False
    if version < (0, 13) and 'weights' in kwargs:
        if kwargs['weights'] == 'DEFAULT':
            kwargs['pretrained'] = True
        elif kwargs['weights'] is None:
            kwargs['pretrained'] = False
        else:
            raise ValueError('weights=={} not supported in torchvision {}'.format(kwargs['weights'], torchvision.__version__))
        del kwargs['weights']
    return torchvision.models.inception_v3(*args, **kwargs)


class FIDInceptionA(torchvision.models.inception.InceptionA):
    """InceptionA block patched for FID computation"""

    def __init__(self, in_channels, pool_features):
        super(FIDInceptionA, self).__init__(in_channels, pool_features)

    def forward(self, x):
        branch1x1 = self.branch1x1(x)
        branch5x5 = self.branch5x5_1(x)
        branch5x5 = self.branch5x5_2(branch5x5)
        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)
        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1, count_include_pad=False)
        branch_pool = self.branch_pool(branch_pool)
        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]
        return torch.cat(outputs, 1)


class FIDInceptionC(torchvision.models.inception.InceptionC):
    """InceptionC block patched for FID computation"""

    def __init__(self, in_channels, channels_7x7):
        super(FIDInceptionC, self).__init__(in_channels, channels_7x7)

    def forward(self, x):
        branch1x1 = self.branch1x1(x)
        branch7x7 = self.branch7x7_1(x)
        branch7x7 = self.branch7x7_2(branch7x7)
        branch7x7 = self.branch7x7_3(branch7x7)
        branch7x7dbl = self.branch7x7dbl_1(x)
        branch7x7dbl = self.branch7x7dbl_2(branch7x7dbl)
        branch7x7dbl = self.branch7x7dbl_3(branch7x7dbl)
        branch7x7dbl = self.branch7x7dbl_4(branch7x7dbl)
        branch7x7dbl = self.branch7x7dbl_5(branch7x7dbl)
        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1, count_include_pad=False)
        branch_pool = self.branch_pool(branch_pool)
        outputs = [branch1x1, branch7x7, branch7x7dbl, branch_pool]
        return torch.cat(outputs, 1)


class FIDInceptionE_1(torchvision.models.inception.InceptionE):
    """First InceptionE block patched for FID computation"""

    def __init__(self, in_channels):
        super(FIDInceptionE_1, self).__init__(in_channels)

    def forward(self, x):
        branch1x1 = self.branch1x1(x)
        branch3x3 = self.branch3x3_1(x)
        branch3x3 = [self.branch3x3_2a(branch3x3), self.branch3x3_2b(branch3x3)]
        branch3x3 = torch.cat(branch3x3, 1)
        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = [self.branch3x3dbl_3a(branch3x3dbl), self.branch3x3dbl_3b(branch3x3dbl)]
        branch3x3dbl = torch.cat(branch3x3dbl, 1)
        branch_pool = F.avg_pool2d(x, kernel_size=3, stride=1, padding=1, count_include_pad=False)
        branch_pool = self.branch_pool(branch_pool)
        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]
        return torch.cat(outputs, 1)


class FIDInceptionE_2(torchvision.models.inception.InceptionE):
    """Second InceptionE block patched for FID computation"""

    def __init__(self, in_channels):
        super(FIDInceptionE_2, self).__init__(in_channels)

    def forward(self, x):
        branch1x1 = self.branch1x1(x)
        branch3x3 = self.branch3x3_1(x)
        branch3x3 = [self.branch3x3_2a(branch3x3), self.branch3x3_2b(branch3x3)]
        branch3x3 = torch.cat(branch3x3, 1)
        branch3x3dbl = self.branch3x3dbl_1(x)
        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)
        branch3x3dbl = [self.branch3x3dbl_3a(branch3x3dbl), self.branch3x3dbl_3b(branch3x3dbl)]
        branch3x3dbl = torch.cat(branch3x3dbl, 1)
        branch_pool = F.max_pool2d(x, kernel_size=3, stride=1, padding=1)
        branch_pool = self.branch_pool(branch_pool)
        outputs = [branch1x1, branch3x3, branch3x3dbl, branch_pool]
        return torch.cat(outputs, 1)


FID_WEIGHTS_URL = 'https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth'


def fid_inception_v3():
    """Build pretrained Inception model for FID computation

    The Inception model for FID computation uses a different set of weights
    and has a slightly different structure than torchvision's Inception.

    This method first constructs torchvision's Inception and then patches the
    necessary parts that are different in the FID Inception model.
    """
    inception = _inception_v3(num_classes=1008, aux_logits=False, weights=None)
    inception.Mixed_5b = FIDInceptionA(192, pool_features=32)
    inception.Mixed_5c = FIDInceptionA(256, pool_features=64)
    inception.Mixed_5d = FIDInceptionA(288, pool_features=64)
    inception.Mixed_6b = FIDInceptionC(768, channels_7x7=128)
    inception.Mixed_6c = FIDInceptionC(768, channels_7x7=160)
    inception.Mixed_6d = FIDInceptionC(768, channels_7x7=160)
    inception.Mixed_6e = FIDInceptionC(768, channels_7x7=192)
    inception.Mixed_7b = FIDInceptionE_1(1280)
    inception.Mixed_7c = FIDInceptionE_2(2048)
    state_dict = load_state_dict_from_url(FID_WEIGHTS_URL, progress=True)
    inception.load_state_dict(state_dict)
    return inception


class InceptionV3(nn.Module):
    """Pretrained InceptionV3 network returning feature maps"""
    DEFAULT_BLOCK_INDEX = 3
    BLOCK_INDEX_BY_DIM = {(64): 0, (192): 1, (768): 2, (2048): 3}

    def __init__(self, output_blocks=(DEFAULT_BLOCK_INDEX,), resize_input=True, normalize_input=True, requires_grad=False, use_fid_inception=True):
        """Build pretrained InceptionV3

        Parameters
        ----------
        output_blocks : list of int
            Indices of blocks to return features of. Possible values are:
                - 0: corresponds to output of first max pooling
                - 1: corresponds to output of second max pooling
                - 2: corresponds to output which is fed to aux classifier
                - 3: corresponds to output of final average pooling
        resize_input : bool
            If true, bilinearly resizes input to width and height 299 before
            feeding input to model. As the network without fully connected
            layers is fully convolutional, it should be able to handle inputs
            of arbitrary size, so resizing might not be strictly needed
        normalize_input : bool
            If true, scales the input from range (0, 1) to the range the
            pretrained Inception network expects, namely (-1, 1)
        requires_grad : bool
            If true, parameters of the model require gradients. Possibly useful
            for finetuning the network
        use_fid_inception : bool
            If true, uses the pretrained Inception model used in Tensorflow's
            FID implementation. If false, uses the pretrained Inception model
            available in torchvision. The FID Inception model has different
            weights and a slightly different structure from torchvision's
            Inception model. If you want to compute FID scores, you are
            strongly advised to set this parameter to true to get comparable
            results.
        """
        super(InceptionV3, self).__init__()
        self.resize_input = resize_input
        self.normalize_input = normalize_input
        self.output_blocks = sorted(output_blocks)
        self.last_needed_block = max(output_blocks)
        assert self.last_needed_block <= 3, 'Last possible output block index is 3'
        self.blocks = nn.ModuleList()
        if use_fid_inception:
            inception = fid_inception_v3()
        else:
            inception = _inception_v3(weights='DEFAULT')
        block0 = [inception.Conv2d_1a_3x3, inception.Conv2d_2a_3x3, inception.Conv2d_2b_3x3, nn.MaxPool2d(kernel_size=3, stride=2)]
        self.blocks.append(nn.Sequential(*block0))
        if self.last_needed_block >= 1:
            block1 = [inception.Conv2d_3b_1x1, inception.Conv2d_4a_3x3, nn.MaxPool2d(kernel_size=3, stride=2)]
            self.blocks.append(nn.Sequential(*block1))
        if self.last_needed_block >= 2:
            block2 = [inception.Mixed_5b, inception.Mixed_5c, inception.Mixed_5d, inception.Mixed_6a, inception.Mixed_6b, inception.Mixed_6c, inception.Mixed_6d, inception.Mixed_6e]
            self.blocks.append(nn.Sequential(*block2))
        if self.last_needed_block >= 3:
            block3 = [inception.Mixed_7a, inception.Mixed_7b, inception.Mixed_7c, nn.AdaptiveAvgPool2d(output_size=(1, 1))]
            self.blocks.append(nn.Sequential(*block3))
        for param in self.parameters():
            param.requires_grad = requires_grad

    def forward(self, inp):
        """Get Inception feature maps

        Parameters
        ----------
        inp : torch.autograd.Variable
            Input tensor of shape Bx3xHxW. Values are expected to be in
            range (0, 1)

        Returns
        -------
        List of torch.autograd.Variable, corresponding to the selected output
        block, sorted ascending by index
        """
        outp = []
        x = inp
        if self.resize_input:
            x = F.interpolate(x, size=(299, 299), mode='bilinear', align_corners=False)
        if self.normalize_input:
            x = 2 * x - 1
        for idx, block in enumerate(self.blocks):
            x = block(x)
            if idx in self.output_blocks:
                outp.append(x)
            if idx == self.last_needed_block:
                break
        return outp


class Embedder:
    """
    borrow from
    https://github.com/zju3dv/animatable_nerf/blob/master/lib/networks/embedder.py
    """

    def __init__(self, **kwargs):
        self.kwargs = kwargs
        self.create_embedding_fn()

    def create_embedding_fn(self):
        embed_fns = []
        d = self.kwargs['input_dims']
        out_dim = 0
        if self.kwargs['include_input']:
            embed_fns.append(lambda x: x)
            out_dim += d
        max_freq = self.kwargs['max_freq_log2']
        N_freqs = self.kwargs['num_freqs']
        if self.kwargs['log_sampling']:
            freq_bands = 2.0 ** torch.linspace(0.0, max_freq, steps=N_freqs)
        else:
            freq_bands = torch.linspace(2.0 ** 0.0, 2.0 ** max_freq, steps=N_freqs)
        for freq in freq_bands:
            for p_fn in self.kwargs['periodic_fns']:
                embed_fns.append(lambda x, p_fn=p_fn, freq=freq: p_fn(x * freq))
                out_dim += d
        self.embed_fns = embed_fns
        self.out_dim = out_dim

    def __call__(self, inputs):
        return torch.cat([fn(inputs) for fn in self.embed_fns], -1)


def get_embedder(input_dims, num_freqs, include_input=True, log_sampling=True):
    embed_kwargs = {'input_dims': input_dims, 'num_freqs': num_freqs, 'max_freq_log2': num_freqs - 1, 'include_input': include_input, 'log_sampling': log_sampling, 'periodic_fns': [torch.sin, torch.cos]}
    embedder_obj = Embedder(**embed_kwargs)
    logging.debug(f'embedder out dim = {embedder_obj.out_dim}')
    return embedder_obj


XYZ_MIN = [-200, -300, -20]


XYZ_RANGE = [350, 650, 80]


def normalizer(mode, data):
    if mode == 'cxyz' or mode == 'all-xyz':
        mins = torch.as_tensor(XYZ_MIN, dtype=data.dtype, device=data.device)[None, None]
        divider = torch.as_tensor(XYZ_RANGE, dtype=data.dtype, device=data.device)[None, None]
        data = (data - mins) / divider
    elif mode == 'owhr':
        raise NotImplementedError(f'wait for implementation on {mode}')
    else:
        raise NotImplementedError(f'not support {mode}')
    return data


class ContinuousBBoxWithTextEmbedding(nn.Module):
    """
    Use continuous bbox corrdicate and text embedding with CLIP encoder
    """

    def __init__(self, n_classes, class_token_dim=768, trainable_class_token=False, embedder_num_freq=4, proj_dims=[768, 512, 512, 768], mode='cxyz', minmax_normalize=True, use_text_encoder_init=True, **kwargs):
        """
        Args:
            mode (str, optional): cxyz -> all points; all-xyz -> all points;
                owhr -> center, l, w, h, z-orientation.
        """
        super().__init__()
        self.mode = mode
        if self.mode == 'cxyz':
            input_dims = 3
            output_num = 4
        elif self.mode == 'all-xyz':
            input_dims = 3
            output_num = 8
        elif self.mode == 'owhr':
            raise NotImplementedError('Not sure how to do this.')
        else:
            raise NotImplementedError(f'Wrong mode {mode}')
        self.minmax_normalize = minmax_normalize
        self.use_text_encoder_init = use_text_encoder_init
        self.fourier_embedder = get_embedder(input_dims, embedder_num_freq)
        logging.info(f'[ContinuousBBoxWithTextEmbedding] bbox embedder has {self.fourier_embedder.out_dim} dims.')
        self.bbox_proj = nn.Linear(self.fourier_embedder.out_dim * output_num, proj_dims[0])
        self.second_linear = nn.Sequential(nn.Linear(proj_dims[0] + class_token_dim, proj_dims[1]), nn.SiLU(), nn.Linear(proj_dims[1], proj_dims[2]), nn.SiLU(), nn.Linear(proj_dims[2], proj_dims[3]))
        self._class_tokens_set_or_warned = not self.use_text_encoder_init
        if trainable_class_token:
            class_tokens = torch.randn(n_classes, class_token_dim)
            self.register_parameter('_class_tokens', nn.Parameter(class_tokens))
        else:
            class_tokens = torch.randn(n_classes, class_token_dim)
            self.register_buffer('_class_tokens', class_tokens)
            if not self.use_text_encoder_init:
                logging.warn('[ContinuousBBoxWithTextEmbedding] Your class_tokens is not trainable but you set `use_text_encoder_init` to False. Please check your config!')
        self.null_class_feature = torch.nn.Parameter(torch.zeros([class_token_dim]))
        self.null_pos_feature = torch.nn.Parameter(torch.zeros([self.fourier_embedder.out_dim * output_num]))

    @property
    def class_tokens(self):
        if not self._class_tokens_set_or_warned:
            logging.warn('[ContinuousBBoxWithTextEmbedding] Your class_tokens is not trainable and used without initialization. Please check your training code!')
            self._class_tokens_set_or_warned = True
        return self._class_tokens

    def prepare(self, cfg, **kwargs):
        if self.use_text_encoder_init:
            self.set_category_token(kwargs['tokenizer'], kwargs['text_encoder'], cfg.dataset.object_classes)
        else:
            logging.info('[ContinuousBBoxWithTextEmbedding] Your class_tokens initilzed with random.')

    @torch.no_grad()
    def set_category_token(self, tokenizer, text_encoder, class_names):
        logging.info('[ContinuousBBoxWithTextEmbedding] Initialzing your class_tokens with text_encoder')
        self._class_tokens_set_or_warned = True
        device = self.class_tokens.device
        for idx, name in enumerate(class_names):
            inputs = tokenizer([name], padding='do_not_pad', return_tensors='pt')
            inputs = inputs.input_ids
            hidden_state = text_encoder(inputs).pooler_output[0]
            self.class_tokens[idx].copy_(hidden_state)

    def add_n_uncond_tokens(self, hidden_states, token_num):
        B = hidden_states.shape[0]
        uncond_token = self.forward_feature(self.null_pos_feature[None], self.null_class_feature[None])
        uncond_token = repeat(uncond_token, 'c -> b n c', b=B, n=token_num)
        hidden_states = torch.cat([hidden_states, uncond_token], dim=1)
        return hidden_states

    def forward_feature(self, pos_emb, cls_emb):
        emb = self.bbox_proj(pos_emb)
        emb = F.silu(emb)
        emb = torch.cat([emb, cls_emb], dim=-1)
        emb = self.second_linear(emb)
        return emb

    def forward(self, bboxes: 'torch.Tensor', classes: 'torch.LongTensor', masks=None, **kwargs):
        """Please do filter before input is needed.

        Args:
            bboxes (torch.Tensor): Expect (B, N, 4, 3) for cxyz mode.
            classes (torch.LongTensor): (B, N)

        Return:
            size B x N x emb_dim=768
        """
        B, N = classes.shape
        bboxes = rearrange(bboxes, 'b n ... -> (b n) ...')
        if masks is None:
            masks = torch.ones(len(bboxes))
        else:
            masks = masks.flatten()
        masks = masks.unsqueeze(-1).type_as(self.null_pos_feature)
        if self.minmax_normalize:
            bboxes = normalizer(self.mode, bboxes)
        pos_emb = self.fourier_embedder(bboxes)
        pos_emb = pos_emb.reshape(pos_emb.shape[0], -1).type_as(self.null_pos_feature)
        pos_emb = pos_emb * masks + self.null_pos_feature[None] * (1 - masks)
        cls_emb = torch.stack([self.class_tokens[i] for i in classes.flatten()])
        cls_emb = cls_emb * masks + self.null_class_feature[None] * (1 - masks)
        emb = self.forward_feature(pos_emb, cls_emb)
        emb = rearrange(emb, '(b n) ... -> b n ...', n=N)
        return emb


class GatedConnector(nn.Module):

    def __init__(self, dim) ->None:
        super().__init__()
        data = torch.zeros(dim)
        self.alpha = nn.parameter.Parameter(data)

    def forward(self, inx):
        return F.tanh(self.alpha) * inx


def zero_module(module):
    for p in module.parameters():
        nn.init.zeros_(p)
    return module


class BEVControlNetConditioningEmbedding(nn.Module):
    """
    Quoting from https://arxiv.org/abs/2302.05543: "Stable Diffusion uses a pre-processing method similar to VQ-GAN
    [11] to convert the entire dataset of 512 × 512 images into smaller 64 × 64 “latent images” for stabilized
    training. This requires ControlNets to convert image-based conditions to 64 × 64 feature space to match the
    convolution size. We use a tiny network E(·) of four convolution layers with 4 × 4 kernels and 2 × 2 strides
    (activated by ReLU, channels are 16, 32, 64, 128, initialized with Gaussian weights, trained jointly with the full
    model) to encode image-space conditions ... into feature maps ..."
    """

    def __init__(self, conditioning_embedding_channels: 'int'=320, conditioning_size: 'Tuple[int, int, int]'=(25, 200, 200), block_out_channels: 'Tuple[int]'=(32, 64, 128, 256)):
        super().__init__()
        self.conv_in = nn.Conv2d(conditioning_size[0], block_out_channels[0], kernel_size=3, padding=1)
        self.blocks = nn.ModuleList([])
        for i in range(len(block_out_channels) - 2):
            channel_in = block_out_channels[i]
            channel_out = block_out_channels[i + 1]
            self.blocks.append(nn.Conv2d(channel_in, channel_in, kernel_size=3, padding=1))
            self.blocks.append(nn.Conv2d(channel_in, channel_out, kernel_size=3, padding=(2, 1), stride=2))
        channel_in = block_out_channels[-2]
        channel_out = block_out_channels[-1]
        self.blocks.append(nn.Conv2d(channel_in, channel_in, kernel_size=3, padding=(2, 1)))
        self.blocks.append(nn.Conv2d(channel_in, channel_out, kernel_size=3, padding=(2, 1), stride=(2, 1)))
        self.conv_out = zero_module(nn.Conv2d(block_out_channels[-1], conditioning_embedding_channels, kernel_size=3, padding=1))

    def forward(self, conditioning):
        embedding = self.conv_in(conditioning)
        embedding = F.silu(embedding)
        for block in self.blocks:
            embedding = block(embedding)
            embedding = F.silu(embedding)
        embedding = self.conv_out(embedding)
        return embedding


class BEVControlNetConditioningEmbeddingPlus(BEVControlNetConditioningEmbedding):

    def __init__(self, conditioning_embedding_size: 'Tuple[int]', conditioning_embedding_channels: 'int'=320, conditioning_size: 'Tuple[int, int, int]'=(25, 200, 200), block_out_channels: 'Tuple[int]'=(16, 32, 96, 256)):
        super().__init__()
        self.conv_in = nn.Conv2d(conditioning_size[0], block_out_channels[0], kernel_size=3, padding=1)
        self.blocks = nn.ModuleList([])
        for i in range(len(block_out_channels) - 2):
            channel_in = block_out_channels[i]
            channel_out = block_out_channels[i + 1]
            self.blocks.append(nn.Conv2d(channel_in, channel_in, kernel_size=3, padding=(1, 1)))
            stride = 1 if i == 0 else 2
            self.blocks.append(nn.Conv2d(channel_in, channel_out, kernel_size=3, padding=(1, 1), stride=stride))
        channel_in = block_out_channels[-2]
        channel_out = block_out_channels[-1]
        self.blocks.append(nn.Conv2d(channel_in, channel_in, kernel_size=3, padding=(1, 1)))
        self.blocks.append(nn.Conv2d(channel_in, channel_out, kernel_size=3, padding=(1, 1), stride=(2, 1)))
        self.blocks.append(nn.AdaptiveAvgPool2d(conditioning_embedding_size))
        self.conv_out = zero_module(nn.Conv2d(block_out_channels[-1], conditioning_embedding_channels, kernel_size=3, padding=1))


class MakeCutouts(nn.Module):

    def __init__(self, cut_size, cut_power=1.0):
        super().__init__()
        self.cut_size = cut_size
        self.cut_power = cut_power

    def forward(self, pixel_values, num_cutouts):
        sideY, sideX = pixel_values.shape[2:4]
        max_size = min(sideX, sideY)
        min_size = min(sideX, sideY, self.cut_size)
        cutouts = []
        for _ in range(num_cutouts):
            size = int(torch.rand([]) ** self.cut_power * (max_size - min_size) + min_size)
            offsetx = torch.randint(0, sideX - size + 1, ())
            offsety = torch.randint(0, sideY - size + 1, ())
            cutout = pixel_values[:, :, offsety:offsety + size, offsetx:offsetx + size]
            cutouts.append(F.adaptive_avg_pool2d(cutout, self.cut_size))
        return torch.cat(cutouts)


class TorchVAEEncoder(torch.nn.Module):

    def __init__(self, model):
        super().__init__()
        self.vae_encoder = model

    def forward(self, x):
        return self.vae_encoder.encode(x).latent_dist.sample()


class DiffusionUncond(nn.Module):

    def __init__(self, global_args):
        super().__init__()
        self.diffusion = DiffusionAttnUnet1D(global_args, n_attn_layers=4)
        self.diffusion_ema = deepcopy(self.diffusion)
        self.rng = torch.quasirandom.SobolEngine(1, scramble=True)


class AttnProcsLayers(torch.nn.Module):

    def __init__(self, state_dict: 'Dict[str, torch.Tensor]'):
        super().__init__()
        self.layers = torch.nn.ModuleList(state_dict.values())
        self.mapping = dict(enumerate(state_dict.keys()))
        self.rev_mapping = {v: k for k, v in enumerate(state_dict.keys())}
        self.split_keys = ['.processor', '.self_attn']

        def map_to(module, state_dict, *args, **kwargs):
            new_state_dict = {}
            for key, value in state_dict.items():
                num = int(key.split('.')[1])
                new_key = key.replace(f'layers.{num}', module.mapping[num])
                new_state_dict[new_key] = value
            return new_state_dict

        def remap_key(key, state_dict):
            for k in self.split_keys:
                if k in key:
                    return key.split(k)[0] + k
            raise ValueError(f'There seems to be a problem with the state_dict: {set(state_dict.keys())}. {key} has to have one of {self.split_keys}.')

        def map_from(module, state_dict, *args, **kwargs):
            all_keys = list(state_dict.keys())
            for key in all_keys:
                replace_key = remap_key(key, state_dict)
                new_key = key.replace(replace_key, f'layers.{module.rev_mapping[replace_key]}')
                state_dict[new_key] = state_dict[key]
                del state_dict[key]
        self._register_state_dict_hook(map_to)
        self._register_load_state_dict_pre_hook(map_from, with_module=True)


class AdaLayerNorm(nn.Module):
    """
    Norm layer modified to incorporate timestep embeddings.
    """

    def __init__(self, embedding_dim, num_embeddings):
        super().__init__()
        self.emb = nn.Embedding(num_embeddings, embedding_dim)
        self.silu = nn.SiLU()
        self.linear = nn.Linear(embedding_dim, embedding_dim * 2)
        self.norm = nn.LayerNorm(embedding_dim, elementwise_affine=False)

    def forward(self, x, timestep):
        emb = self.linear(self.silu(self.emb(timestep)))
        scale, shift = torch.chunk(emb, 2)
        x = self.norm(x) * (1 + scale) + shift
        return x


class LabelEmbedding(nn.Module):
    """
    Embeds class labels into vector representations. Also handles label dropout for classifier-free guidance.

    Args:
        num_classes (`int`): The number of classes.
        hidden_size (`int`): The size of the vector embeddings.
        dropout_prob (`float`): The probability of dropping a label.
    """

    def __init__(self, num_classes, hidden_size, dropout_prob):
        super().__init__()
        use_cfg_embedding = dropout_prob > 0
        self.embedding_table = nn.Embedding(num_classes + use_cfg_embedding, hidden_size)
        self.num_classes = num_classes
        self.dropout_prob = dropout_prob

    def token_drop(self, labels, force_drop_ids=None):
        """
        Drops labels to enable classifier-free guidance.
        """
        if force_drop_ids is None:
            drop_ids = torch.rand(labels.shape[0], device=labels.device) < self.dropout_prob
        else:
            drop_ids = torch.tensor(force_drop_ids == 1)
        labels = torch.where(drop_ids, self.num_classes, labels)
        return labels

    def forward(self, labels: 'torch.LongTensor', force_drop_ids=None):
        use_dropout = self.dropout_prob > 0
        if self.training and use_dropout or force_drop_ids is not None:
            labels = self.token_drop(labels, force_drop_ids)
        embeddings = self.embedding_table(labels)
        return embeddings


def get_activation(act_fn):
    if act_fn in ['swish', 'silu']:
        return nn.SiLU()
    elif act_fn == 'mish':
        return nn.Mish()
    elif act_fn == 'gelu':
        return nn.GELU()
    else:
        raise ValueError(f'Unsupported activation function: {act_fn}')


class TimestepEmbedding(nn.Module):

    def __init__(self, in_channels: 'int', time_embed_dim: 'int', act_fn: 'str'='silu', out_dim: 'int'=None, post_act_fn: 'Optional[str]'=None, cond_proj_dim=None):
        super().__init__()
        self.linear_1 = nn.Linear(in_channels, time_embed_dim)
        if cond_proj_dim is not None:
            self.cond_proj = nn.Linear(cond_proj_dim, in_channels, bias=False)
        else:
            self.cond_proj = None
        self.act = get_activation(act_fn)
        if out_dim is not None:
            time_embed_dim_out = out_dim
        else:
            time_embed_dim_out = time_embed_dim
        self.linear_2 = nn.Linear(time_embed_dim, time_embed_dim_out)
        if post_act_fn is None:
            self.post_act = None
        else:
            self.post_act = get_activation(post_act_fn)

    def forward(self, sample, condition=None):
        if condition is not None:
            sample = sample + self.cond_proj(condition)
        sample = self.linear_1(sample)
        if self.act is not None:
            sample = self.act(sample)
        sample = self.linear_2(sample)
        if self.post_act is not None:
            sample = self.post_act(sample)
        return sample


def get_timestep_embedding(timesteps: 'torch.Tensor', embedding_dim: 'int', flip_sin_to_cos: 'bool'=False, downscale_freq_shift: 'float'=1, scale: 'float'=1, max_period: 'int'=10000):
    """
    This matches the implementation in Denoising Diffusion Probabilistic Models: Create sinusoidal timestep embeddings.

    :param timesteps: a 1-D Tensor of N indices, one per batch element.
                      These may be fractional.
    :param embedding_dim: the dimension of the output. :param max_period: controls the minimum frequency of the
    embeddings. :return: an [N x dim] Tensor of positional embeddings.
    """
    assert len(timesteps.shape) == 1, 'Timesteps should be a 1d-array'
    half_dim = embedding_dim // 2
    exponent = -math.log(max_period) * torch.arange(start=0, end=half_dim, dtype=torch.float32, device=timesteps.device)
    exponent = exponent / (half_dim - downscale_freq_shift)
    emb = torch.exp(exponent)
    emb = timesteps[:, None].float() * emb[None, :]
    emb = scale * emb
    emb = torch.cat([torch.sin(emb), torch.cos(emb)], dim=-1)
    if flip_sin_to_cos:
        emb = torch.cat([emb[:, half_dim:], emb[:, :half_dim]], dim=-1)
    if embedding_dim % 2 == 1:
        emb = torch.nn.functional.pad(emb, (0, 1, 0, 0))
    return emb


class Timesteps(nn.Module):

    def __init__(self, num_channels: 'int', flip_sin_to_cos: 'bool', downscale_freq_shift: 'float'):
        super().__init__()
        self.num_channels = num_channels
        self.flip_sin_to_cos = flip_sin_to_cos
        self.downscale_freq_shift = downscale_freq_shift

    def forward(self, timesteps):
        t_emb = get_timestep_embedding(timesteps, self.num_channels, flip_sin_to_cos=self.flip_sin_to_cos, downscale_freq_shift=self.downscale_freq_shift)
        return t_emb


class CombinedTimestepLabelEmbeddings(nn.Module):

    def __init__(self, num_classes, embedding_dim, class_dropout_prob=0.1):
        super().__init__()
        self.time_proj = Timesteps(num_channels=256, flip_sin_to_cos=True, downscale_freq_shift=1)
        self.timestep_embedder = TimestepEmbedding(in_channels=256, time_embed_dim=embedding_dim)
        self.class_embedder = LabelEmbedding(num_classes, embedding_dim, class_dropout_prob)

    def forward(self, timestep, class_labels, hidden_dtype=None):
        timesteps_proj = self.time_proj(timestep)
        timesteps_emb = self.timestep_embedder(timesteps_proj)
        class_labels = self.class_embedder(class_labels)
        conditioning = timesteps_emb + class_labels
        return conditioning


class AdaLayerNormZero(nn.Module):
    """
    Norm layer adaptive layer norm zero (adaLN-Zero).
    """

    def __init__(self, embedding_dim, num_embeddings):
        super().__init__()
        self.emb = CombinedTimestepLabelEmbeddings(num_embeddings, embedding_dim)
        self.silu = nn.SiLU()
        self.linear = nn.Linear(embedding_dim, 6 * embedding_dim, bias=True)
        self.norm = nn.LayerNorm(embedding_dim, elementwise_affine=False, eps=1e-06)

    def forward(self, x, timestep, class_labels, hidden_dtype=None):
        emb = self.linear(self.silu(self.emb(timestep, class_labels, hidden_dtype=hidden_dtype)))
        shift_msa, scale_msa, gate_msa, shift_mlp, scale_mlp, gate_mlp = emb.chunk(6, dim=1)
        x = self.norm(x) * (1 + scale_msa[:, None]) + shift_msa[:, None]
        return x, gate_msa, shift_mlp, scale_mlp, gate_mlp


Self = TypeVar('Self', bound='SimplicialEmbedding')


class Attention(nn.Module, metaclass=ABCMeta):
    """The base Attention mechanism, which is typically a sub-part of the multi-head attention"""
    _causal_mask: 'Optional[AttentionMask]' = None

    @abstractmethod
    def __init__(self, dropout: 'Optional[float]'=None, *args, **kwargs):
        super().__init__()
        self.requires_input_projection = True
        self.requires_head_dimension = False
        self.requires_separate_masks = False
        self.requires_same_k_q_dimensions = False
        self.requires_skip_multi_head = False
        self.requires_squared_context = False
        self.supports_attention_mask = True
        self.supports_key_padding_mask = False

    @classmethod
    def from_config(cls: 'Type[Self]', config: 'AttentionConfig') ->Self:
        fields = asdict(config)
        fields = {k: v for k, v in fields.items() if v is not None}
        return cls(**fields)

    @abstractmethod
    def forward(self, q: 'torch.Tensor', k: 'torch.Tensor', v: 'torch.Tensor', *args, **kwargs) ->torch.Tensor:
        raise NotImplementedError

    @staticmethod
    def _maybe_pad_sequence(x: 'torch.Tensor', mask: 'torch.Tensor'):
        """
        If the sequence is shorter than the mask, return a padded view
        """
        if x.shape[-2] != mask.shape[-1]:
            assert x.shape[-2] < mask.shape[-1], 'Sequence is bigger than the provided mask, cannot infer what to do with it. Please update your attention mask'
            pad_size = 0, 0, 0, mask.shape[-1] - x.shape[-2], 0, 0
            return torch.nn.functional.pad(x, pad_size, mode='constant', value=0.0)
        return x


class ApproximateGELU(nn.Module):
    """
    The approximate form of Gaussian Error Linear Unit (GELU)

    For more details, see section 2: https://arxiv.org/abs/1606.08415
    """

    def __init__(self, dim_in: 'int', dim_out: 'int'):
        super().__init__()
        self.proj = nn.Linear(dim_in, dim_out)

    def forward(self, x):
        x = self.proj(x)
        return x * torch.sigmoid(1.702 * x)


class GEGLU(nn.Module):
    """
    A variant of the gated linear unit activation function from https://arxiv.org/abs/2002.05202.

    Parameters:
        dim_in (`int`): The number of channels in the input.
        dim_out (`int`): The number of channels in the output.
    """

    def __init__(self, dim_in: 'int', dim_out: 'int'):
        super().__init__()
        self.proj = nn.Linear(dim_in, dim_out * 2)

    def gelu(self, gate):
        if gate.device.type != 'mps':
            return F.gelu(gate)
        return F.gelu(gate.to(dtype=torch.float32))

    def forward(self, hidden_states):
        hidden_states, gate = self.proj(hidden_states).chunk(2, dim=-1)
        return hidden_states * self.gelu(gate)


class GELU(nn.Module):
    """
    GELU activation function with tanh approximation support with `approximate="tanh"`.
    """

    def __init__(self, dim_in: 'int', dim_out: 'int', approximate: 'str'='none'):
        super().__init__()
        self.proj = nn.Linear(dim_in, dim_out)
        self.approximate = approximate

    def gelu(self, gate):
        if gate.device.type != 'mps':
            return F.gelu(gate, approximate=self.approximate)
        return F.gelu(gate.to(dtype=torch.float32), approximate=self.approximate)

    def forward(self, hidden_states):
        hidden_states = self.proj(hidden_states)
        hidden_states = self.gelu(hidden_states)
        return hidden_states


class FeedForward(nn.Module):
    """
    A feed-forward layer.

    Parameters:
        dim (`int`): The number of channels in the input.
        dim_out (`int`, *optional*): The number of channels in the output. If not given, defaults to `dim`.
        mult (`int`, *optional*, defaults to 4): The multiplier to use for the hidden dimension.
        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.
        activation_fn (`str`, *optional*, defaults to `"geglu"`): Activation function to be used in feed-forward.
        final_dropout (`bool` *optional*, defaults to False): Apply a final dropout.
    """

    def __init__(self, dim: 'int', dim_out: 'Optional[int]'=None, mult: 'int'=4, dropout: 'float'=0.0, activation_fn: 'str'='geglu', final_dropout: 'bool'=False):
        super().__init__()
        inner_dim = int(dim * mult)
        dim_out = dim_out if dim_out is not None else dim
        if activation_fn == 'gelu':
            act_fn = GELU(dim, inner_dim)
        if activation_fn == 'gelu-approximate':
            act_fn = GELU(dim, inner_dim, approximate='tanh')
        elif activation_fn == 'geglu':
            act_fn = GEGLU(dim, inner_dim)
        elif activation_fn == 'geglu-approximate':
            act_fn = ApproximateGELU(dim, inner_dim)
        self.net = nn.ModuleList([])
        self.net.append(act_fn)
        self.net.append(nn.Dropout(dropout))
        self.net.append(nn.Linear(inner_dim, dim_out))
        if final_dropout:
            self.net.append(nn.Dropout(dropout))

    def forward(self, hidden_states):
        for module in self.net:
            hidden_states = module(hidden_states)
        return hidden_states


class AdaGroupNorm(nn.Module):
    """
    GroupNorm layer modified to incorporate timestep embeddings.
    """

    def __init__(self, embedding_dim: 'int', out_dim: 'int', num_groups: 'int', act_fn: 'Optional[str]'=None, eps: 'float'=1e-05):
        super().__init__()
        self.num_groups = num_groups
        self.eps = eps
        if act_fn is None:
            self.act = None
        else:
            self.act = get_activation(act_fn)
        self.linear = nn.Linear(embedding_dim, out_dim * 2)

    def forward(self, x, emb):
        if self.act:
            emb = self.act(emb)
        emb = self.linear(emb)
        emb = emb[:, :, None, None]
        scale, shift = emb.chunk(2, dim=1)
        x = F.group_norm(x, self.num_groups, eps=self.eps)
        x = x * (1 + scale) + shift
        return x


def _query_chunk_attention(query, key, value, precision, key_chunk_size: 'int'=4096):
    """Multi-head dot product attention with a limited number of queries."""
    num_kv, num_heads, k_features = key.shape[-3:]
    v_features = value.shape[-1]
    key_chunk_size = min(key_chunk_size, num_kv)
    query = query / jnp.sqrt(k_features)

    @functools.partial(jax.checkpoint, prevent_cse=False)
    def summarize_chunk(query, key, value):
        attn_weights = jnp.einsum('...qhd,...khd->...qhk', query, key, precision=precision)
        max_score = jnp.max(attn_weights, axis=-1, keepdims=True)
        max_score = jax.lax.stop_gradient(max_score)
        exp_weights = jnp.exp(attn_weights - max_score)
        exp_values = jnp.einsum('...vhf,...qhv->...qhf', value, exp_weights, precision=precision)
        max_score = jnp.einsum('...qhk->...qh', max_score)
        return exp_values, exp_weights.sum(axis=-1), max_score

    def chunk_scanner(chunk_idx):
        key_chunk = jax.lax.dynamic_slice(operand=key, start_indices=[0] * (key.ndim - 3) + [chunk_idx, 0, 0], slice_sizes=list(key.shape[:-3]) + [key_chunk_size, num_heads, k_features])
        value_chunk = jax.lax.dynamic_slice(operand=value, start_indices=[0] * (value.ndim - 3) + [chunk_idx, 0, 0], slice_sizes=list(value.shape[:-3]) + [key_chunk_size, num_heads, v_features])
        return summarize_chunk(query, key_chunk, value_chunk)
    chunk_values, chunk_weights, chunk_max = jax.lax.map(f=chunk_scanner, xs=jnp.arange(0, num_kv, key_chunk_size))
    global_max = jnp.max(chunk_max, axis=0, keepdims=True)
    max_diffs = jnp.exp(chunk_max - global_max)
    chunk_values *= jnp.expand_dims(max_diffs, axis=-1)
    chunk_weights *= max_diffs
    all_values = chunk_values.sum(axis=0)
    all_weights = jnp.expand_dims(chunk_weights, -1).sum(axis=0)
    return all_values / all_weights


class LoRALinearLayer(nn.Module):

    def __init__(self, in_features, out_features, rank=4, network_alpha=None):
        super().__init__()
        if rank > min(in_features, out_features):
            raise ValueError(f'LoRA rank {rank} must be less or equal than {min(in_features, out_features)}')
        self.down = nn.Linear(in_features, rank, bias=False)
        self.up = nn.Linear(rank, out_features, bias=False)
        self.network_alpha = network_alpha
        self.rank = rank
        nn.init.normal_(self.down.weight, std=1 / rank)
        nn.init.zeros_(self.up.weight)

    def forward(self, hidden_states):
        orig_dtype = hidden_states.dtype
        dtype = self.down.weight.dtype
        down_hidden_states = self.down(hidden_states)
        up_hidden_states = self.up(down_hidden_states)
        if self.network_alpha is not None:
            up_hidden_states *= self.network_alpha / self.rank
        return up_hidden_states


class LoRAAttnProcessor(nn.Module):
    """
    Processor for implementing the LoRA attention mechanism.

    Args:
        hidden_size (`int`, *optional*):
            The hidden size of the attention layer.
        cross_attention_dim (`int`, *optional*):
            The number of channels in the `encoder_hidden_states`.
        rank (`int`, defaults to 4):
            The dimension of the LoRA update matrices.
        network_alpha (`int`, *optional*):
            Equivalent to `alpha` but it's usage is specific to Kohya (A1111) style LoRAs.
    """

    def __init__(self, hidden_size, cross_attention_dim=None, rank=4, network_alpha=None):
        super().__init__()
        self.hidden_size = hidden_size
        self.cross_attention_dim = cross_attention_dim
        self.rank = rank
        self.to_q_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)
        self.to_k_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)
        self.to_v_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)
        self.to_out_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)

    def __call__(self, attn: 'Attention', hidden_states, encoder_hidden_states=None, attention_mask=None, scale=1.0, temb=None):
        residual = hidden_states
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states) + scale * self.to_q_lora(hidden_states)
        query = attn.head_to_batch_dim(query)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        key = attn.to_k(encoder_hidden_states) + scale * self.to_k_lora(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states) + scale * self.to_v_lora(encoder_hidden_states)
        key = attn.head_to_batch_dim(key)
        value = attn.head_to_batch_dim(value)
        attention_probs = attn.get_attention_scores(query, key, attention_mask)
        hidden_states = torch.bmm(attention_probs, value)
        hidden_states = attn.batch_to_head_dim(hidden_states)
        hidden_states = attn.to_out[0](hidden_states) + scale * self.to_out_lora(hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


class CustomDiffusionAttnProcessor(nn.Module):
    """
    Processor for implementing attention for the Custom Diffusion method.

    Args:
        train_kv (`bool`, defaults to `True`):
            Whether to newly train the key and value matrices corresponding to the text features.
        train_q_out (`bool`, defaults to `True`):
            Whether to newly train query matrices corresponding to the latent image features.
        hidden_size (`int`, *optional*, defaults to `None`):
            The hidden size of the attention layer.
        cross_attention_dim (`int`, *optional*, defaults to `None`):
            The number of channels in the `encoder_hidden_states`.
        out_bias (`bool`, defaults to `True`):
            Whether to include the bias parameter in `train_q_out`.
        dropout (`float`, *optional*, defaults to 0.0):
            The dropout probability to use.
    """

    def __init__(self, train_kv=True, train_q_out=True, hidden_size=None, cross_attention_dim=None, out_bias=True, dropout=0.0):
        super().__init__()
        self.train_kv = train_kv
        self.train_q_out = train_q_out
        self.hidden_size = hidden_size
        self.cross_attention_dim = cross_attention_dim
        if self.train_kv:
            self.to_k_custom_diffusion = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)
            self.to_v_custom_diffusion = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)
        if self.train_q_out:
            self.to_q_custom_diffusion = nn.Linear(hidden_size, hidden_size, bias=False)
            self.to_out_custom_diffusion = nn.ModuleList([])
            self.to_out_custom_diffusion.append(nn.Linear(hidden_size, hidden_size, bias=out_bias))
            self.to_out_custom_diffusion.append(nn.Dropout(dropout))

    def __call__(self, attn: 'Attention', hidden_states, encoder_hidden_states=None, attention_mask=None):
        batch_size, sequence_length, _ = hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        if self.train_q_out:
            query = self.to_q_custom_diffusion(hidden_states)
        else:
            query = attn.to_q(hidden_states)
        if encoder_hidden_states is None:
            crossattn = False
            encoder_hidden_states = hidden_states
        else:
            crossattn = True
            if attn.norm_cross:
                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        if self.train_kv:
            key = self.to_k_custom_diffusion(encoder_hidden_states)
            value = self.to_v_custom_diffusion(encoder_hidden_states)
        else:
            key = attn.to_k(encoder_hidden_states)
            value = attn.to_v(encoder_hidden_states)
        if crossattn:
            detach = torch.ones_like(key)
            detach[:, :1, :] = detach[:, :1, :] * 0.0
            key = detach * key + (1 - detach) * key.detach()
            value = detach * value + (1 - detach) * value.detach()
        query = attn.head_to_batch_dim(query)
        key = attn.head_to_batch_dim(key)
        value = attn.head_to_batch_dim(value)
        attention_probs = attn.get_attention_scores(query, key, attention_mask)
        hidden_states = torch.bmm(attention_probs, value)
        hidden_states = attn.batch_to_head_dim(hidden_states)
        if self.train_q_out:
            hidden_states = self.to_out_custom_diffusion[0](hidden_states)
            hidden_states = self.to_out_custom_diffusion[1](hidden_states)
        else:
            hidden_states = attn.to_out[0](hidden_states)
            hidden_states = attn.to_out[1](hidden_states)
        return hidden_states


class LoRAAttnAddedKVProcessor(nn.Module):
    """
    Processor for implementing the LoRA attention mechanism with extra learnable key and value matrices for the text
    encoder.

    Args:
        hidden_size (`int`, *optional*):
            The hidden size of the attention layer.
        cross_attention_dim (`int`, *optional*, defaults to `None`):
            The number of channels in the `encoder_hidden_states`.
        rank (`int`, defaults to 4):
            The dimension of the LoRA update matrices.

    """

    def __init__(self, hidden_size, cross_attention_dim=None, rank=4, network_alpha=None):
        super().__init__()
        self.hidden_size = hidden_size
        self.cross_attention_dim = cross_attention_dim
        self.rank = rank
        self.to_q_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)
        self.add_k_proj_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)
        self.add_v_proj_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)
        self.to_k_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)
        self.to_v_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)
        self.to_out_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)

    def __call__(self, attn: 'Attention', hidden_states, encoder_hidden_states=None, attention_mask=None, scale=1.0):
        residual = hidden_states
        hidden_states = hidden_states.view(hidden_states.shape[0], hidden_states.shape[1], -1).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states) + scale * self.to_q_lora(hidden_states)
        query = attn.head_to_batch_dim(query)
        encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states) + scale * self.add_k_proj_lora(encoder_hidden_states)
        encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states) + scale * self.add_v_proj_lora(encoder_hidden_states)
        encoder_hidden_states_key_proj = attn.head_to_batch_dim(encoder_hidden_states_key_proj)
        encoder_hidden_states_value_proj = attn.head_to_batch_dim(encoder_hidden_states_value_proj)
        if not attn.only_cross_attention:
            key = attn.to_k(hidden_states) + scale * self.to_k_lora(hidden_states)
            value = attn.to_v(hidden_states) + scale * self.to_v_lora(hidden_states)
            key = attn.head_to_batch_dim(key)
            value = attn.head_to_batch_dim(value)
            key = torch.cat([encoder_hidden_states_key_proj, key], dim=1)
            value = torch.cat([encoder_hidden_states_value_proj, value], dim=1)
        else:
            key = encoder_hidden_states_key_proj
            value = encoder_hidden_states_value_proj
        attention_probs = attn.get_attention_scores(query, key, attention_mask)
        hidden_states = torch.bmm(attention_probs, value)
        hidden_states = attn.batch_to_head_dim(hidden_states)
        hidden_states = attn.to_out[0](hidden_states) + scale * self.to_out_lora(hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        hidden_states = hidden_states.transpose(-1, -2).reshape(residual.shape)
        hidden_states = hidden_states + residual
        return hidden_states


class LoRAXFormersAttnProcessor(nn.Module):
    """
    Processor for implementing the LoRA attention mechanism with memory efficient attention using xFormers.

    Args:
        hidden_size (`int`, *optional*):
            The hidden size of the attention layer.
        cross_attention_dim (`int`, *optional*):
            The number of channels in the `encoder_hidden_states`.
        rank (`int`, defaults to 4):
            The dimension of the LoRA update matrices.
        attention_op (`Callable`, *optional*, defaults to `None`):
            The base
            [operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase) to
            use as the attention operator. It is recommended to set to `None`, and allow xFormers to choose the best
            operator.
        network_alpha (`int`, *optional*):
            Equivalent to `alpha` but it's usage is specific to Kohya (A1111) style LoRAs.

    """

    def __init__(self, hidden_size, cross_attention_dim, rank=4, attention_op: 'Optional[Callable]'=None, network_alpha=None):
        super().__init__()
        self.hidden_size = hidden_size
        self.cross_attention_dim = cross_attention_dim
        self.rank = rank
        self.attention_op = attention_op
        self.to_q_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)
        self.to_k_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)
        self.to_v_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)
        self.to_out_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)

    def __call__(self, attn: 'Attention', hidden_states, encoder_hidden_states=None, attention_mask=None, scale=1.0, temb=None):
        residual = hidden_states
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states) + scale * self.to_q_lora(hidden_states)
        query = attn.head_to_batch_dim(query).contiguous()
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        key = attn.to_k(encoder_hidden_states) + scale * self.to_k_lora(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states) + scale * self.to_v_lora(encoder_hidden_states)
        key = attn.head_to_batch_dim(key).contiguous()
        value = attn.head_to_batch_dim(value).contiguous()
        hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask, op=self.attention_op, scale=attn.scale)
        hidden_states = attn.batch_to_head_dim(hidden_states)
        hidden_states = attn.to_out[0](hidden_states) + scale * self.to_out_lora(hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


class LoRAAttnProcessor2_0(nn.Module):
    """
    Processor for implementing the LoRA attention mechanism using PyTorch 2.0's memory-efficient scaled dot-product
    attention.

    Args:
        hidden_size (`int`):
            The hidden size of the attention layer.
        cross_attention_dim (`int`, *optional*):
            The number of channels in the `encoder_hidden_states`.
        rank (`int`, defaults to 4):
            The dimension of the LoRA update matrices.
        network_alpha (`int`, *optional*):
            Equivalent to `alpha` but it's usage is specific to Kohya (A1111) style LoRAs.
    """

    def __init__(self, hidden_size, cross_attention_dim=None, rank=4, network_alpha=None):
        super().__init__()
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError('AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.')
        self.hidden_size = hidden_size
        self.cross_attention_dim = cross_attention_dim
        self.rank = rank
        self.to_q_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)
        self.to_k_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)
        self.to_v_lora = LoRALinearLayer(cross_attention_dim or hidden_size, hidden_size, rank, network_alpha)
        self.to_out_lora = LoRALinearLayer(hidden_size, hidden_size, rank, network_alpha)

    def __call__(self, attn: 'Attention', hidden_states, encoder_hidden_states=None, attention_mask=None, scale=1.0):
        residual = hidden_states
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        inner_dim = hidden_states.shape[-1]
        if attention_mask is not None:
            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states) + scale * self.to_q_lora(hidden_states)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        key = attn.to_k(encoder_hidden_states) + scale * self.to_k_lora(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states) + scale * self.to_v_lora(encoder_hidden_states)
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)
        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states = hidden_states
        hidden_states = attn.to_out[0](hidden_states) + scale * self.to_out_lora(hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


class CustomDiffusionXFormersAttnProcessor(nn.Module):
    """
    Processor for implementing memory efficient attention using xFormers for the Custom Diffusion method.

    Args:
    train_kv (`bool`, defaults to `True`):
        Whether to newly train the key and value matrices corresponding to the text features.
    train_q_out (`bool`, defaults to `True`):
        Whether to newly train query matrices corresponding to the latent image features.
    hidden_size (`int`, *optional*, defaults to `None`):
        The hidden size of the attention layer.
    cross_attention_dim (`int`, *optional*, defaults to `None`):
        The number of channels in the `encoder_hidden_states`.
    out_bias (`bool`, defaults to `True`):
        Whether to include the bias parameter in `train_q_out`.
    dropout (`float`, *optional*, defaults to 0.0):
        The dropout probability to use.
    attention_op (`Callable`, *optional*, defaults to `None`):
        The base
        [operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase) to use
        as the attention operator. It is recommended to set to `None`, and allow xFormers to choose the best operator.
    """

    def __init__(self, train_kv=True, train_q_out=False, hidden_size=None, cross_attention_dim=None, out_bias=True, dropout=0.0, attention_op: 'Optional[Callable]'=None):
        super().__init__()
        self.train_kv = train_kv
        self.train_q_out = train_q_out
        self.hidden_size = hidden_size
        self.cross_attention_dim = cross_attention_dim
        self.attention_op = attention_op
        if self.train_kv:
            self.to_k_custom_diffusion = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)
            self.to_v_custom_diffusion = nn.Linear(cross_attention_dim or hidden_size, hidden_size, bias=False)
        if self.train_q_out:
            self.to_q_custom_diffusion = nn.Linear(hidden_size, hidden_size, bias=False)
            self.to_out_custom_diffusion = nn.ModuleList([])
            self.to_out_custom_diffusion.append(nn.Linear(hidden_size, hidden_size, bias=out_bias))
            self.to_out_custom_diffusion.append(nn.Dropout(dropout))

    def __call__(self, attn: 'Attention', hidden_states, encoder_hidden_states=None, attention_mask=None):
        batch_size, sequence_length, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        if self.train_q_out:
            query = self.to_q_custom_diffusion(hidden_states)
        else:
            query = attn.to_q(hidden_states)
        if encoder_hidden_states is None:
            crossattn = False
            encoder_hidden_states = hidden_states
        else:
            crossattn = True
            if attn.norm_cross:
                encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        if self.train_kv:
            key = self.to_k_custom_diffusion(encoder_hidden_states)
            value = self.to_v_custom_diffusion(encoder_hidden_states)
        else:
            key = attn.to_k(encoder_hidden_states)
            value = attn.to_v(encoder_hidden_states)
        if crossattn:
            detach = torch.ones_like(key)
            detach[:, :1, :] = detach[:, :1, :] * 0.0
            key = detach * key + (1 - detach) * key.detach()
            value = detach * value + (1 - detach) * value.detach()
        query = attn.head_to_batch_dim(query).contiguous()
        key = attn.head_to_batch_dim(key).contiguous()
        value = attn.head_to_batch_dim(value).contiguous()
        hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask, op=self.attention_op, scale=attn.scale)
        hidden_states = hidden_states
        hidden_states = attn.batch_to_head_dim(hidden_states)
        if self.train_q_out:
            hidden_states = self.to_out_custom_diffusion[0](hidden_states)
            hidden_states = self.to_out_custom_diffusion[1](hidden_states)
        else:
            hidden_states = attn.to_out[0](hidden_states)
            hidden_states = attn.to_out[1](hidden_states)
        return hidden_states


class SpatialNorm(nn.Module):
    """
    Spatially conditioned normalization as defined in https://arxiv.org/abs/2209.09002
    """

    def __init__(self, f_channels, zq_channels):
        super().__init__()
        self.norm_layer = nn.GroupNorm(num_channels=f_channels, num_groups=32, eps=1e-06, affine=True)
        self.conv_y = nn.Conv2d(zq_channels, f_channels, kernel_size=1, stride=1, padding=0)
        self.conv_b = nn.Conv2d(zq_channels, f_channels, kernel_size=1, stride=1, padding=0)

    def forward(self, f, zq):
        f_size = f.shape[-2:]
        zq = F.interpolate(zq, size=f_size, mode='nearest')
        norm_f = self.norm_layer(f)
        new_f = norm_f * self.conv_y(zq) + self.conv_b(zq)
        return new_f


class ControlNetConditioningEmbedding(nn.Module):
    """
    Quoting from https://arxiv.org/abs/2302.05543: "Stable Diffusion uses a pre-processing method similar to VQ-GAN
    [11] to convert the entire dataset of 512 × 512 images into smaller 64 × 64 “latent images” for stabilized
    training. This requires ControlNets to convert image-based conditions to 64 × 64 feature space to match the
    convolution size. We use a tiny network E(·) of four convolution layers with 4 × 4 kernels and 2 × 2 strides
    (activated by ReLU, channels are 16, 32, 64, 128, initialized with Gaussian weights, trained jointly with the full
    model) to encode image-space conditions ... into feature maps ..."
    """

    def __init__(self, conditioning_embedding_channels: 'int', conditioning_channels: 'int'=3, block_out_channels: 'Tuple[int]'=(16, 32, 96, 256)):
        super().__init__()
        self.conv_in = nn.Conv2d(conditioning_channels, block_out_channels[0], kernel_size=3, padding=1)
        self.blocks = nn.ModuleList([])
        for i in range(len(block_out_channels) - 1):
            channel_in = block_out_channels[i]
            channel_out = block_out_channels[i + 1]
            self.blocks.append(nn.Conv2d(channel_in, channel_in, kernel_size=3, padding=1))
            self.blocks.append(nn.Conv2d(channel_in, channel_out, kernel_size=3, padding=1, stride=2))
        self.conv_out = zero_module(nn.Conv2d(block_out_channels[-1], conditioning_embedding_channels, kernel_size=3, padding=1))

    def forward(self, conditioning):
        embedding = self.conv_in(conditioning)
        embedding = F.silu(embedding)
        for block in self.blocks:
            embedding = block(embedding)
            embedding = F.silu(embedding)
        embedding = self.conv_out(embedding)
        return embedding


BS4_IMPORT_ERROR = """
{0} requires the Beautiful Soup library but it was not found in your environment. You can install it with pip:
`pip install beautifulsoup4`. Please note that you may need to restart your runtime after installation.
"""


COMPEL_IMPORT_ERROR = """
{0} requires the compel library but it was not found in your environment. You can install it with pip: `pip install compel`
"""


FLAX_IMPORT_ERROR = """
{0} requires the FLAX library but it was not found in your environment. Checkout the instructions on the
installation page: https://github.com/google/flax and follow the ones that match your environment.
"""


FTFY_IMPORT_ERROR = """
{0} requires the ftfy library but it was not found in your environment. Checkout the instructions on the
installation section: https://github.com/rspeer/python-ftfy/tree/master#installing and follow the ones
that match your environment. Please note that you may need to restart your runtime after installation.
"""


INFLECT_IMPORT_ERROR = """
{0} requires the inflect library but it was not found in your environment. You can install it with pip: `pip install
inflect`
"""


K_DIFFUSION_IMPORT_ERROR = """
{0} requires the k-diffusion library but it was not found in your environment. You can install it with pip: `pip
install k-diffusion`
"""


LIBROSA_IMPORT_ERROR = """
{0} requires the librosa library but it was not found in your environment.  Checkout the instructions on the
installation page: https://librosa.org/doc/latest/install.html and follow the ones that match your environment.
"""


NOTE_SEQ_IMPORT_ERROR = """
{0} requires the note-seq library but it was not found in your environment. You can install it with pip: `pip
install note-seq`
"""


OMEGACONF_IMPORT_ERROR = """
{0} requires the omegaconf library but it was not found in your environment. You can install it with pip: `pip
install omegaconf`
"""


ONNX_IMPORT_ERROR = """
{0} requires the onnxruntime library but it was not found in your environment. You can install it with pip: `pip
install onnxruntime`
"""


OPENCV_IMPORT_ERROR = """
{0} requires the OpenCV library but it was not found in your environment. You can install it with pip: `pip
install opencv-python`
"""


PYTORCH_IMPORT_ERROR = """
{0} requires the PyTorch library but it was not found in your environment. Checkout the instructions on the
installation page: https://pytorch.org/get-started/locally/ and follow the ones that match your environment.
"""


SCIPY_IMPORT_ERROR = """
{0} requires the scipy library but it was not found in your environment. You can install it with pip: `pip install
scipy`
"""


TENSORBOARD_IMPORT_ERROR = """
{0} requires the tensorboard library but it was not found in your environment. You can install it with pip: `pip
install tensorboard`
"""


TORCHSDE_IMPORT_ERROR = """
{0} requires the torchsde library but it was not found in your environment. You can install it with pip: `pip install torchsde`
"""


TRANSFORMERS_IMPORT_ERROR = """
{0} requires the transformers library but it was not found in your environment. You can install it with pip: `pip
install transformers`
"""


UNIDECODE_IMPORT_ERROR = """
{0} requires the unidecode library but it was not found in your environment. You can install it with pip: `pip install
Unidecode`
"""


WANDB_IMPORT_ERROR = """
{0} requires the wandb library but it was not found in your environment. You can install it with pip: `pip
install wandb`
"""


def is_bs4_available():
    return _bs4_available


def is_flax_available():
    return _flax_available


def is_ftfy_available():
    return _ftfy_available


def is_inflect_available():
    return _inflect_available


def is_k_diffusion_available():
    return _k_diffusion_available


def is_librosa_available():
    return _librosa_available


def is_note_seq_available():
    return _note_seq_available


def is_omegaconf_available():
    return _omegaconf_available


def is_onnx_available():
    return _onnx_available


def is_opencv_available():
    return _opencv_available


def is_scipy_available():
    return _scipy_available


def is_torch_available():
    return _torch_available


def is_transformers_available():
    return _transformers_available


def is_unidecode_available():
    return _unidecode_available


def is_wandb_available():
    return _wandb_available


STR_OPERATION_TO_FUNC = {'>': op.gt, '>=': op.ge, '==': op.eq, '!=': op.ne, '<=': op.le, '<': op.lt}


def compare_versions(library_or_version: 'Union[str, Version]', operation: 'str', requirement_version: 'str'):
    """
    Args:
    Compares a library version to some requirement using a given operation.
        library_or_version (`str` or `packaging.version.Version`):
            A library name or a version to check.
        operation (`str`):
            A string representation of an operator, such as `">"` or `"<="`.
        requirement_version (`str`):
            The version to compare the library version against
    """
    if operation not in STR_OPERATION_TO_FUNC.keys():
        raise ValueError(f'`operation` must be one of {list(STR_OPERATION_TO_FUNC.keys())}, received {operation}')
    operation = STR_OPERATION_TO_FUNC[operation]
    if isinstance(library_or_version, str):
        library_or_version = parse(importlib_metadata.version(library_or_version))
    return operation(library_or_version, parse(requirement_version))


def is_transformers_version(operation: 'str', version: 'str'):
    """
    Args:
    Compares the current Transformers version to a given reference with an operation.
        operation (`str`):
            A string representation of an operator, such as `">"` or `"<="`
        version (`str`):
            A version string
    """
    if not _transformers_available:
        return False
    return compare_versions(parse(_transformers_version), operation, version)


def requires_backends(obj, backends):
    if not isinstance(backends, (list, tuple)):
        backends = [backends]
    name = obj.__name__ if hasattr(obj, '__name__') else obj.__class__.__name__
    checks = (BACKENDS_MAPPING[backend] for backend in backends)
    failed = [msg.format(name) for available, msg in checks if not available()]
    if failed:
        raise ImportError(''.join(failed))
    if name in ['VersatileDiffusionTextToImagePipeline', 'VersatileDiffusionPipeline', 'VersatileDiffusionDualGuidedPipeline', 'StableDiffusionImageVariationPipeline', 'UnCLIPPipeline'] and is_transformers_version('<', '4.25.0'):
        raise ImportError(f'You need to install `transformers>=4.25` in order to use {name}: \n```\n pip install --upgrade transformers \n```')
    if name in ['StableDiffusionDepth2ImgPipeline', 'StableDiffusionPix2PixZeroPipeline'] and is_transformers_version('<', '4.26.0'):
        raise ImportError(f'You need to install `transformers>=4.26` in order to use {name}: \n```\n pip install --upgrade transformers \n```')


class DummyObject(type):
    """
    Metaclass for the dummy objects. Any class inheriting from it will return the ImportError generated by
    `requires_backend` each time a user tries to access any method of that class.
    """

    def __getattr__(cls, key):
        if key.startswith('_'):
            return super().__getattr__(cls, key)
        requires_backends(cls, cls._backends)


class FrozenDict(OrderedDict):

    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        for key, value in self.items():
            setattr(self, key, value)
        self.__frozen = True

    def __delitem__(self, *args, **kwargs):
        raise Exception(f'You cannot use ``__delitem__`` on a {self.__class__.__name__} instance.')

    def setdefault(self, *args, **kwargs):
        raise Exception(f'You cannot use ``setdefault`` on a {self.__class__.__name__} instance.')

    def pop(self, *args, **kwargs):
        raise Exception(f'You cannot use ``pop`` on a {self.__class__.__name__} instance.')

    def update(self, *args, **kwargs):
        raise Exception(f'You cannot use ``update`` on a {self.__class__.__name__} instance.')

    def __setattr__(self, name, value):
        if hasattr(self, '__frozen') and self.__frozen:
            raise Exception(f'You cannot use ``__setattr__`` on a {self.__class__.__name__} instance.')
        super().__setattr__(name, value)

    def __setitem__(self, name, value):
        if hasattr(self, '__frozen') and self.__frozen:
            raise Exception(f'You cannot use ``__setattr__`` on a {self.__class__.__name__} instance.')
        super().__setitem__(name, value)


HUGGINGFACE_CO_RESOLVE_ENDPOINT = 'https://huggingface.co'


def deprecate(*args, take_from: Optional[Union[Dict, Any]]=None, standard_warn=True, stacklevel=2):
    deprecated_kwargs = take_from
    values = ()
    if not isinstance(args[0], tuple):
        args = args,
    for attribute, version_name, message in args:
        if version.parse(version.parse(__version__).base_version) >= version.parse(version_name):
            raise ValueError(f"The deprecation tuple {attribute, version_name, message} should be removed since diffusers' version {__version__} is >= {version_name}")
        warning = None
        if isinstance(deprecated_kwargs, dict) and attribute in deprecated_kwargs:
            values += deprecated_kwargs.pop(attribute),
            warning = f'The `{attribute}` argument is deprecated and will be removed in version {version_name}.'
        elif hasattr(deprecated_kwargs, attribute):
            values += getattr(deprecated_kwargs, attribute),
            warning = f'The `{attribute}` attribute is deprecated and will be removed in version {version_name}.'
        elif deprecated_kwargs is None:
            warning = f'`{attribute}` is deprecated and will be removed in version {version_name}.'
        if warning is not None:
            warning = warning + ' ' if standard_warn else ''
            warnings.warn(warning + message, FutureWarning, stacklevel=stacklevel)
    if isinstance(deprecated_kwargs, dict) and len(deprecated_kwargs) > 0:
        call_frame = inspect.getouterframes(inspect.currentframe())[1]
        filename = call_frame.filename
        line_number = call_frame.lineno
        function = call_frame.function
        key, value = next(iter(deprecated_kwargs.items()))
        raise TypeError(f'{function} in {filename} line {line_number - 1} got an unexpected keyword argument `{key}`')
    if len(values) == 0:
        return
    elif len(values) == 1:
        return values[0]
    return values


def extract_commit_hash(resolved_file: 'Optional[str]', commit_hash: 'Optional[str]'=None):
    """
    Extracts the commit hash from a resolved filename toward a cache file.
    """
    if resolved_file is None or commit_hash is not None:
        return commit_hash
    resolved_file = str(Path(resolved_file).as_posix())
    search = re.search('snapshots/([^/]+)/', resolved_file)
    if search is None:
        return None
    commit_hash = search.groups()[0]
    return commit_hash if REGEX_COMMIT_HASH.match(commit_hash) else None


ENV_VARS_TRUE_VALUES = {'1', 'ON', 'YES', 'TRUE'}


SESSION_ID = uuid4().hex


_flax_version = 'N/A'


_jax_version = 'N/A'


_onnxruntime_version = 'N/A'


_torch_version = 'N/A'


def http_user_agent(user_agent: 'Union[Dict, str, None]'=None) ->str:
    """
    Formats a user-agent string with basic info about a request.
    """
    ua = f'diffusers/{__version__}; python/{sys.version.split()[0]}; session_id/{SESSION_ID}'
    if DISABLE_TELEMETRY or HF_HUB_OFFLINE:
        return ua + '; telemetry/off'
    if is_torch_available():
        ua += f'; torch/{_torch_version}'
    if is_flax_available():
        ua += f'; jax/{_jax_version}'
        ua += f'; flax/{_flax_version}'
    if is_onnx_available():
        ua += f'; onnxruntime/{_onnxruntime_version}'
    if os.environ.get('DIFFUSERS_IS_CI', '').upper() in ENV_VARS_TRUE_VALUES:
        ua += '; is_ci/true'
    if isinstance(user_agent, dict):
        ua += '; ' + '; '.join(f'{k}/{v}' for k, v in user_agent.items())
    elif isinstance(user_agent, str):
        ua += '; ' + user_agent
    return ua


logger = logging.getLogger('xformers')


class ConfigMixin:
    """
    Base class for all configuration classes. Stores all configuration parameters under `self.config` Also handles all
    methods for loading/downloading/saving classes inheriting from [`ConfigMixin`] with
        - [`~ConfigMixin.from_config`]
        - [`~ConfigMixin.save_config`]

    Class attributes:
        - **config_name** (`str`) -- A filename under which the config should stored when calling
          [`~ConfigMixin.save_config`] (should be overridden by parent class).
        - **ignore_for_config** (`List[str]`) -- A list of attributes that should not be saved in the config (should be
          overridden by subclass).
        - **has_compatibles** (`bool`) -- Whether the class has compatible classes (should be overridden by subclass).
        - **_deprecated_kwargs** (`List[str]`) -- Keyword arguments that are deprecated. Note that the init function
          should only have a `kwargs` argument if at least one argument is deprecated (should be overridden by
          subclass).
    """
    config_name = None
    ignore_for_config = []
    has_compatibles = False
    _deprecated_kwargs = []

    def register_to_config(self, **kwargs):
        if self.config_name is None:
            raise NotImplementedError(f'Make sure that {self.__class__} has defined a class name `config_name`')
        kwargs.pop('kwargs', None)
        if not hasattr(self, '_internal_dict'):
            internal_dict = kwargs
        else:
            previous_dict = dict(self._internal_dict)
            internal_dict = {**self._internal_dict, **kwargs}
            logger.debug(f'Updating config from {previous_dict} to {internal_dict}')
        self._internal_dict = FrozenDict(internal_dict)

    def __getattr__(self, name: 'str') ->Any:
        """The only reason we overwrite `getattr` here is to gracefully deprecate accessing
        config attributes directly. See https://github.com/huggingface/diffusers/pull/3129

        Tihs funtion is mostly copied from PyTorch's __getattr__ overwrite:
        https://pytorch.org/docs/stable/_modules/torch/nn/modules/module.html#Module
        """
        is_in_config = '_internal_dict' in self.__dict__ and hasattr(self.__dict__['_internal_dict'], name)
        is_attribute = name in self.__dict__
        if is_in_config and not is_attribute:
            deprecation_message = f"Accessing config attribute `{name}` directly via '{type(self).__name__}' object attribute is deprecated. Please access '{name}' over '{type(self).__name__}'s config object instead, e.g. 'scheduler.config.{name}'."
            deprecate('direct config name access', '1.0.0', deprecation_message, standard_warn=False)
            return self._internal_dict[name]
        raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")

    def save_config(self, save_directory: 'Union[str, os.PathLike]', push_to_hub: 'bool'=False, **kwargs):
        """
        Save a configuration object to the directory `save_directory`, so that it can be re-loaded using the
        [`~ConfigMixin.from_config`] class method.

        Args:
            save_directory (`str` or `os.PathLike`):
                Directory where the configuration JSON file will be saved (will be created if it does not exist).
        """
        if os.path.isfile(save_directory):
            raise AssertionError(f'Provided path ({save_directory}) should be a directory, not a file')
        os.makedirs(save_directory, exist_ok=True)
        output_config_file = os.path.join(save_directory, self.config_name)
        self.to_json_file(output_config_file)
        logger.info(f'Configuration saved in {output_config_file}')

    @classmethod
    def from_config(cls, config: 'Union[FrozenDict, Dict[str, Any]]'=None, return_unused_kwargs=False, **kwargs):
        """
        Instantiate a Python class from a config dictionary.

        Parameters:
            config (`Dict[str, Any]`):
                A config dictionary from which the Python class will be instantiated. Make sure to only load
                configuration files of compatible classes.
            return_unused_kwargs (`bool`, *optional*, defaults to `False`):
                Whether kwargs that are not consumed by the Python class should be returned or not.

            kwargs (remaining dictionary of keyword arguments, *optional*):
                Can be used to update the configuration object (after it is loaded) and initiate the Python class.
                `**kwargs` are directly passed to the underlying scheduler/model's `__init__` method and eventually
                overwrite same named arguments in `config`.

        Returns:
            [`ModelMixin`] or [`SchedulerMixin`]:
                A model or scheduler object instantiated from a config dictionary.

        Examples:

        ```python
        >>> from diffusers import DDPMScheduler, DDIMScheduler, PNDMScheduler

        >>> # Download scheduler from huggingface.co and cache.
        >>> scheduler = DDPMScheduler.from_pretrained("google/ddpm-cifar10-32")

        >>> # Instantiate DDIM scheduler class with same config as DDPM
        >>> scheduler = DDIMScheduler.from_config(scheduler.config)

        >>> # Instantiate PNDM scheduler class with same config as DDPM
        >>> scheduler = PNDMScheduler.from_config(scheduler.config)
        ```
        """
        if 'pretrained_model_name_or_path' in kwargs:
            config = kwargs.pop('pretrained_model_name_or_path')
        if config is None:
            raise ValueError('Please make sure to provide a config as the first positional argument.')
        if not isinstance(config, dict):
            deprecation_message = 'It is deprecated to pass a pretrained model name or path to `from_config`.'
            if 'Scheduler' in cls.__name__:
                deprecation_message += f'If you were trying to load a scheduler, please use {cls}.from_pretrained(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.'
            elif 'Model' in cls.__name__:
                deprecation_message += f'If you were trying to load a model, please use {cls}.load_config(...) followed by {cls}.from_config(...) instead. Otherwise, please make sure to pass a configuration dictionary instead. This functionality will be removed in v1.0.0.'
            deprecate('config-passed-as-path', '1.0.0', deprecation_message, standard_warn=False)
            config, kwargs = cls.load_config(pretrained_model_name_or_path=config, return_unused_kwargs=True, **kwargs)
        init_dict, unused_kwargs, hidden_dict = cls.extract_init_dict(config, **kwargs)
        if 'dtype' in unused_kwargs:
            init_dict['dtype'] = unused_kwargs.pop('dtype')
        for deprecated_kwarg in cls._deprecated_kwargs:
            if deprecated_kwarg in unused_kwargs:
                init_dict[deprecated_kwarg] = unused_kwargs.pop(deprecated_kwarg)
        model = cls(**init_dict)
        model.register_to_config(**hidden_dict)
        unused_kwargs = {**unused_kwargs, **hidden_dict}
        if return_unused_kwargs:
            return model, unused_kwargs
        else:
            return model

    @classmethod
    def get_config_dict(cls, *args, **kwargs):
        deprecation_message = f' The function get_config_dict is deprecated. Please use {cls}.load_config instead. This function will be removed in version v1.0.0'
        deprecate('get_config_dict', '1.0.0', deprecation_message, standard_warn=False)
        return cls.load_config(*args, **kwargs)

    @classmethod
    def load_config(cls, pretrained_model_name_or_path: 'Union[str, os.PathLike]', return_unused_kwargs=False, return_commit_hash=False, **kwargs) ->Tuple[Dict[str, Any], Dict[str, Any]]:
        """
        Load a model or scheduler configuration.

        Parameters:
            pretrained_model_name_or_path (`str` or `os.PathLike`, *optional*):
                Can be either:

                    - A string, the *model id* (for example `google/ddpm-celebahq-256`) of a pretrained model hosted on
                      the Hub.
                    - A path to a *directory* (for example `./my_model_directory`) containing model weights saved with
                      [`~ConfigMixin.save_config`].

            cache_dir (`Union[str, os.PathLike]`, *optional*):
                Path to a directory where a downloaded pretrained model configuration is cached if the standard cache
                is not used.
            force_download (`bool`, *optional*, defaults to `False`):
                Whether or not to force the (re-)download of the model weights and configuration files, overriding the
                cached versions if they exist.
            resume_download (`bool`, *optional*, defaults to `False`):
                Whether or not to resume downloading the model weights and configuration files. If set to False, any
                incompletely downloaded files are deleted.
            proxies (`Dict[str, str]`, *optional*):
                A dictionary of proxy servers to use by protocol or endpoint, for example, `{'http': 'foo.bar:3128',
                'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.
            output_loading_info(`bool`, *optional*, defaults to `False`):
                Whether or not to also return a dictionary containing missing keys, unexpected keys and error messages.
            local_files_only(`bool`, *optional*, defaults to `False`):
                Whether to only load local model weights and configuration files or not. If set to True, the model
                won’t be downloaded from the Hub.
            use_auth_token (`str` or *bool*, *optional*):
                The token to use as HTTP bearer authorization for remote files. If `True`, the token generated from
                `diffusers-cli login` (stored in `~/.huggingface`) is used.
            revision (`str`, *optional*, defaults to `"main"`):
                The specific model version to use. It can be a branch name, a tag name, a commit id, or any identifier
                allowed by Git.
            subfolder (`str`, *optional*, defaults to `""`):
                The subfolder location of a model file within a larger model repository on the Hub or locally.
            return_unused_kwargs (`bool`, *optional*, defaults to `False):
                Whether unused keyword arguments of the config are returned.
            return_commit_hash (`bool`, *optional*, defaults to `False):
                Whether the `commit_hash` of the loaded configuration are returned.

        Returns:
            `dict`:
                A dictionary of all the parameters stored in a JSON configuration file.

        <Tip>

        To use private or [gated models](https://huggingface.co/docs/hub/models-gated#gated-models), log-in with
        `huggingface-cli login`. You can also activate the special
        ["offline-mode"](https://huggingface.co/transformers/installation.html#offline-mode) to use this method in a
        firewalled environment.

        </Tip>
        """
        cache_dir = kwargs.pop('cache_dir', DIFFUSERS_CACHE)
        force_download = kwargs.pop('force_download', False)
        resume_download = kwargs.pop('resume_download', False)
        proxies = kwargs.pop('proxies', None)
        use_auth_token = kwargs.pop('use_auth_token', None)
        local_files_only = kwargs.pop('local_files_only', False)
        revision = kwargs.pop('revision', None)
        _ = kwargs.pop('mirror', None)
        subfolder = kwargs.pop('subfolder', None)
        user_agent = kwargs.pop('user_agent', {})
        user_agent = {**user_agent, 'file_type': 'config'}
        user_agent = http_user_agent(user_agent)
        pretrained_model_name_or_path = str(pretrained_model_name_or_path)
        if cls.config_name is None:
            raise ValueError('`self.config_name` is not defined. Note that one should not load a config from `ConfigMixin`. Please make sure to define `config_name` in a class inheriting from `ConfigMixin`')
        if os.path.isfile(pretrained_model_name_or_path):
            config_file = pretrained_model_name_or_path
        elif os.path.isdir(pretrained_model_name_or_path):
            if os.path.isfile(os.path.join(pretrained_model_name_or_path, cls.config_name)):
                config_file = os.path.join(pretrained_model_name_or_path, cls.config_name)
            elif subfolder is not None and os.path.isfile(os.path.join(pretrained_model_name_or_path, subfolder, cls.config_name)):
                config_file = os.path.join(pretrained_model_name_or_path, subfolder, cls.config_name)
            else:
                raise EnvironmentError(f'Error no file named {cls.config_name} found in directory {pretrained_model_name_or_path}.')
        else:
            try:
                config_file = hf_hub_download(pretrained_model_name_or_path, filename=cls.config_name, cache_dir=cache_dir, force_download=force_download, proxies=proxies, resume_download=resume_download, local_files_only=local_files_only, use_auth_token=use_auth_token, user_agent=user_agent, subfolder=subfolder, revision=revision)
            except RepositoryNotFoundError:
                raise EnvironmentError(f"{pretrained_model_name_or_path} is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo with `use_auth_token` or log in with `huggingface-cli login`.")
            except RevisionNotFoundError:
                raise EnvironmentError(f"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists for this model name. Check the model page at 'https://huggingface.co/{pretrained_model_name_or_path}' for available revisions.")
            except EntryNotFoundError:
                raise EnvironmentError(f'{pretrained_model_name_or_path} does not appear to have a file named {cls.config_name}.')
            except HTTPError as err:
                raise EnvironmentError(f'There was a specific connection error when trying to load {pretrained_model_name_or_path}:\n{err}')
            except ValueError:
                raise EnvironmentError(f"We couldn't connect to '{HUGGINGFACE_CO_RESOLVE_ENDPOINT}' to load this model, couldn't find it in the cached files and it looks like {pretrained_model_name_or_path} is not the path to a directory containing a {cls.config_name} file.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/diffusers/installation#offline-mode'.")
            except EnvironmentError:
                raise EnvironmentError(f"Can't load config for '{pretrained_model_name_or_path}'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure '{pretrained_model_name_or_path}' is the correct path to a directory containing a {cls.config_name} file")
        try:
            config_dict = cls._dict_from_json_file(config_file)
            commit_hash = extract_commit_hash(config_file)
        except (json.JSONDecodeError, UnicodeDecodeError):
            raise EnvironmentError(f"It looks like the config file at '{config_file}' is not a valid JSON file.")
        if not (return_unused_kwargs or return_commit_hash):
            return config_dict
        outputs = config_dict,
        if return_unused_kwargs:
            outputs += kwargs,
        if return_commit_hash:
            outputs += commit_hash,
        return outputs

    @staticmethod
    def _get_init_keys(cls):
        return set(dict(inspect.signature(cls.__init__).parameters).keys())

    @classmethod
    def extract_init_dict(cls, config_dict, **kwargs):
        original_dict = dict(config_dict.items())
        expected_keys = cls._get_init_keys(cls)
        expected_keys.remove('self')
        if 'kwargs' in expected_keys:
            expected_keys.remove('kwargs')
        if hasattr(cls, '_flax_internal_args'):
            for arg in cls._flax_internal_args:
                expected_keys.remove(arg)
        if len(cls.ignore_for_config) > 0:
            expected_keys = expected_keys - set(cls.ignore_for_config)
        diffusers_library = importlib.import_module(__name__.split('.')[0])
        if cls.has_compatibles:
            compatible_classes = [c for c in cls._get_compatibles() if not isinstance(c, DummyObject)]
        else:
            compatible_classes = []
        expected_keys_comp_cls = set()
        for c in compatible_classes:
            expected_keys_c = cls._get_init_keys(c)
            expected_keys_comp_cls = expected_keys_comp_cls.union(expected_keys_c)
        expected_keys_comp_cls = expected_keys_comp_cls - cls._get_init_keys(cls)
        config_dict = {k: v for k, v in config_dict.items() if k not in expected_keys_comp_cls}
        orig_cls_name = config_dict.pop('_class_name', cls.__name__)
        if orig_cls_name != cls.__name__ and hasattr(diffusers_library, orig_cls_name):
            orig_cls = getattr(diffusers_library, orig_cls_name)
            unexpected_keys_from_orig = cls._get_init_keys(orig_cls) - expected_keys
            config_dict = {k: v for k, v in config_dict.items() if k not in unexpected_keys_from_orig}
        config_dict = {k: v for k, v in config_dict.items() if not k.startswith('_')}
        init_dict = {}
        for key in expected_keys:
            if key in kwargs and key in config_dict:
                config_dict[key] = kwargs.pop(key)
            if key in kwargs:
                init_dict[key] = kwargs.pop(key)
            elif key in config_dict:
                init_dict[key] = config_dict.pop(key)
        if len(config_dict) > 0:
            logger.warning(f'The config attributes {config_dict} were passed to {cls.__name__}, but are not expected and will be ignored. Please verify your {cls.config_name} configuration file.')
        passed_keys = set(init_dict.keys())
        if len(expected_keys - passed_keys) > 0:
            logger.info(f'{expected_keys - passed_keys} was not found in config. Values will be initialized to default values.')
        unused_kwargs = {**config_dict, **kwargs}
        hidden_config_dict = {k: v for k, v in original_dict.items() if k not in init_dict}
        return init_dict, unused_kwargs, hidden_config_dict

    @classmethod
    def _dict_from_json_file(cls, json_file: 'Union[str, os.PathLike]'):
        with open(json_file, 'r', encoding='utf-8') as reader:
            text = reader.read()
        return json.loads(text)

    def __repr__(self):
        return f'{self.__class__.__name__} {self.to_json_string()}'

    @property
    def config(self) ->Dict[str, Any]:
        """
        Returns the config of the class as a frozen dictionary

        Returns:
            `Dict[str, Any]`: Config of the class.
        """
        return self._internal_dict

    def to_json_string(self) ->str:
        """
        Serializes this instance to a JSON string.

        Returns:
            `str`: String containing all the attributes that make up this configuration instance in JSON format.
        """
        config_dict = self._internal_dict if hasattr(self, '_internal_dict') else {}
        config_dict['_class_name'] = self.__class__.__name__
        config_dict['_diffusers_version'] = __version__

        def to_json_saveable(value):
            if isinstance(value, np.ndarray):
                value = value.tolist()
            elif isinstance(value, PosixPath):
                value = str(value)
            return value
        config_dict = {k: to_json_saveable(v) for k, v in config_dict.items()}
        config_dict.pop('_ignore_files', None)
        return json.dumps(config_dict, indent=2, sort_keys=True) + '\n'

    def to_json_file(self, json_file_path: 'Union[str, os.PathLike]'):
        """
        Save this instance to a JSON file.

        Args:
            json_file_path (`str` or `os.PathLike`):
                Path to the JSON file in which this configuration instance's parameters will be saved.
        """
        with open(json_file_path, 'w', encoding='utf-8') as writer:
            writer.write(self.to_json_string())


class BaseOutput(OrderedDict):
    """
    Base class for all model outputs as dataclass. Has a `__getitem__` that allows indexing by integer or slice (like a
    tuple) or strings (like a dictionary) that will ignore the `None` attributes. Otherwise behaves like a regular
    python dictionary.

    <Tip warning={true}>

    You can't unpack a `BaseOutput` directly. Use the [`~utils.BaseOutput.to_tuple`] method to convert it to a tuple
    before.

    </Tip>
    """

    def __post_init__(self):
        class_fields = fields(self)
        if not len(class_fields):
            raise ValueError(f'{self.__class__.__name__} has no fields.')
        first_field = getattr(self, class_fields[0].name)
        other_fields_are_none = all(getattr(self, field.name) is None for field in class_fields[1:])
        if other_fields_are_none and isinstance(first_field, dict):
            for key, value in first_field.items():
                self[key] = value
        else:
            for field in class_fields:
                v = getattr(self, field.name)
                if v is not None:
                    self[field.name] = v

    def __delitem__(self, *args, **kwargs):
        raise Exception(f'You cannot use ``__delitem__`` on a {self.__class__.__name__} instance.')

    def setdefault(self, *args, **kwargs):
        raise Exception(f'You cannot use ``setdefault`` on a {self.__class__.__name__} instance.')

    def pop(self, *args, **kwargs):
        raise Exception(f'You cannot use ``pop`` on a {self.__class__.__name__} instance.')

    def update(self, *args, **kwargs):
        raise Exception(f'You cannot use ``update`` on a {self.__class__.__name__} instance.')

    def __getitem__(self, k):
        if isinstance(k, str):
            inner_dict = dict(self.items())
            return inner_dict[k]
        else:
            return self.to_tuple()[k]

    def __setattr__(self, name, value):
        if name in self.keys() and value is not None:
            super().__setitem__(name, value)
        super().__setattr__(name, value)

    def __setitem__(self, key, value):
        super().__setitem__(key, value)
        super().__setattr__(key, value)

    def to_tuple(self) ->Tuple[Any]:
        """
        Convert self to a tuple containing all the attributes/keys that are not `None`.
        """
        return tuple(self[k] for k in self.keys())


CONFIG_NAME = 'config.json'


FLAX_WEIGHTS_NAME = 'diffusion_flax_model.msgpack'


WEIGHTS_NAME = 'diffusion_pytorch_model.bin'


def rename_key(key):
    regex = '\\w+[.]\\d+'
    pats = re.findall(regex, key)
    for pat in pats:
        key = key.replace(pat, '_'.join(pat.split('.')))
    return key


def rename_key_and_reshape_tensor(pt_tuple_key, pt_tensor, random_flax_state_dict):
    """Rename PT weight names to corresponding Flax weight names and reshape tensor if necessary"""
    renamed_pt_tuple_key = pt_tuple_key[:-1] + ('scale',)
    if any('norm' in str_ for str_ in pt_tuple_key) and pt_tuple_key[-1] == 'bias' and pt_tuple_key[:-1] + ('bias',) not in random_flax_state_dict and pt_tuple_key[:-1] + ('scale',) in random_flax_state_dict:
        renamed_pt_tuple_key = pt_tuple_key[:-1] + ('scale',)
        return renamed_pt_tuple_key, pt_tensor
    elif pt_tuple_key[-1] in ['weight', 'gamma'] and pt_tuple_key[:-1] + ('scale',) in random_flax_state_dict:
        renamed_pt_tuple_key = pt_tuple_key[:-1] + ('scale',)
        return renamed_pt_tuple_key, pt_tensor
    if pt_tuple_key[-1] == 'weight' and pt_tuple_key[:-1] + ('embedding',) in random_flax_state_dict:
        pt_tuple_key = pt_tuple_key[:-1] + ('embedding',)
        return renamed_pt_tuple_key, pt_tensor
    renamed_pt_tuple_key = pt_tuple_key[:-1] + ('kernel',)
    if pt_tuple_key[-1] == 'weight' and pt_tensor.ndim == 4:
        pt_tensor = pt_tensor.transpose(2, 3, 1, 0)
        return renamed_pt_tuple_key, pt_tensor
    renamed_pt_tuple_key = pt_tuple_key[:-1] + ('kernel',)
    if pt_tuple_key[-1] == 'weight':
        pt_tensor = pt_tensor.T
        return renamed_pt_tuple_key, pt_tensor
    renamed_pt_tuple_key = pt_tuple_key[:-1] + ('weight',)
    if pt_tuple_key[-1] == 'gamma':
        return renamed_pt_tuple_key, pt_tensor
    renamed_pt_tuple_key = pt_tuple_key[:-1] + ('bias',)
    if pt_tuple_key[-1] == 'beta':
        return renamed_pt_tuple_key, pt_tensor
    return pt_tuple_key, pt_tensor


def convert_pytorch_state_dict_to_flax(pt_state_dict, flax_model, init_key=42):
    pt_state_dict = {k: v.numpy() for k, v in pt_state_dict.items()}
    random_flax_params = flax_model.init_weights(PRNGKey(init_key))
    random_flax_state_dict = flatten_dict(random_flax_params)
    flax_state_dict = {}
    for pt_key, pt_tensor in pt_state_dict.items():
        renamed_pt_key = rename_key(pt_key)
        pt_tuple_key = tuple(renamed_pt_key.split('.'))
        flax_key, flax_tensor = rename_key_and_reshape_tensor(pt_tuple_key, pt_tensor, random_flax_state_dict)
        if flax_key in random_flax_state_dict:
            if flax_tensor.shape != random_flax_state_dict[flax_key].shape:
                raise ValueError(f'PyTorch checkpoint seems to be incorrect. Weight {pt_key} was expected to be of shape {random_flax_state_dict[flax_key].shape}, but is {flax_tensor.shape}.')
        flax_state_dict[flax_key] = jnp.asarray(flax_tensor)
    return unflatten_dict(flax_state_dict)


def _add_variant(weights_name: 'str', variant: 'Optional[str]'=None) ->str:
    if variant is not None:
        splits = weights_name.split('.')
        splits = splits[:-1] + [variant] + splits[-1:]
        weights_name = '.'.join(splits)
    return weights_name


def load_state_dict(checkpoint_file: 'Union[str, os.PathLike]', variant: 'Optional[str]'=None):
    """
    Reads a checkpoint file, returning properly formatted errors if they arise.
    """
    try:
        if os.path.basename(checkpoint_file) == _add_variant(WEIGHTS_NAME, variant):
            return torch.load(checkpoint_file, map_location='cpu')
        else:
            return safetensors.torch.load_file(checkpoint_file, device='cpu')
    except Exception as e:
        try:
            with open(checkpoint_file) as f:
                if f.read().startswith('version'):
                    raise OSError('You seem to have cloned a repository without having git-lfs installed. Please install git-lfs and run `git lfs install` followed by `git lfs pull` in the folder you cloned.')
                else:
                    raise ValueError(f'Unable to locate the file {checkpoint_file} which is necessary to load this pretrained model. Make sure you have saved the model properly.') from e
        except (UnicodeDecodeError, ValueError):
            raise OSError(f"Unable to load weights from checkpoint file for '{checkpoint_file}' at '{checkpoint_file}'. If you tried to load a PyTorch model from a TF 2.0 checkpoint, please set from_tf=True.")


class Transformer2DModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


@dataclass
class Transformer2DModelOutput(BaseOutput):
    """
    Args:
        sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)` or `(batch size, num_vector_embeds - 1, num_latent_pixels)` if [`Transformer2DModel`] is discrete):
            Hidden states conditioned on `encoder_hidden_states` input. If discrete, returns probability distributions
            for the unnoised latent pixels.
    """
    sample: 'torch.FloatTensor'


class DualTransformer2DModel(nn.Module):
    """
    Dual transformer wrapper that combines two `Transformer2DModel`s for mixed inference.

    Parameters:
        num_attention_heads (`int`, *optional*, defaults to 16): The number of heads to use for multi-head attention.
        attention_head_dim (`int`, *optional*, defaults to 88): The number of channels in each head.
        in_channels (`int`, *optional*):
            Pass if the input is continuous. The number of channels in the input and output.
        num_layers (`int`, *optional*, defaults to 1): The number of layers of Transformer blocks to use.
        dropout (`float`, *optional*, defaults to 0.1): The dropout probability to use.
        cross_attention_dim (`int`, *optional*): The number of encoder_hidden_states dimensions to use.
        sample_size (`int`, *optional*): Pass if the input is discrete. The width of the latent images.
            Note that this is fixed at training time as it is used for learning a number of position embeddings. See
            `ImagePositionalEmbeddings`.
        num_vector_embeds (`int`, *optional*):
            Pass if the input is discrete. The number of classes of the vector embeddings of the latent pixels.
            Includes the class for the masked latent pixel.
        activation_fn (`str`, *optional*, defaults to `"geglu"`): Activation function to be used in feed-forward.
        num_embeds_ada_norm ( `int`, *optional*): Pass if at least one of the norm_layers is `AdaLayerNorm`.
            The number of diffusion steps used during training. Note that this is fixed at training time as it is used
            to learn a number of embeddings that are added to the hidden states. During inference, you can denoise for
            up to but not more than steps than `num_embeds_ada_norm`.
        attention_bias (`bool`, *optional*):
            Configure if the TransformerBlocks' attention should contain a bias parameter.
    """

    def __init__(self, num_attention_heads: 'int'=16, attention_head_dim: 'int'=88, in_channels: 'Optional[int]'=None, num_layers: 'int'=1, dropout: 'float'=0.0, norm_num_groups: 'int'=32, cross_attention_dim: 'Optional[int]'=None, attention_bias: 'bool'=False, sample_size: 'Optional[int]'=None, num_vector_embeds: 'Optional[int]'=None, activation_fn: 'str'='geglu', num_embeds_ada_norm: 'Optional[int]'=None):
        super().__init__()
        self.transformers = nn.ModuleList([Transformer2DModel(num_attention_heads=num_attention_heads, attention_head_dim=attention_head_dim, in_channels=in_channels, num_layers=num_layers, dropout=dropout, norm_num_groups=norm_num_groups, cross_attention_dim=cross_attention_dim, attention_bias=attention_bias, sample_size=sample_size, num_vector_embeds=num_vector_embeds, activation_fn=activation_fn, num_embeds_ada_norm=num_embeds_ada_norm) for _ in range(2)])
        self.mix_ratio = 0.5
        self.condition_lengths = [77, 257]
        self.transformer_index_for_condition = [1, 0]

    def forward(self, hidden_states, encoder_hidden_states, timestep=None, attention_mask=None, cross_attention_kwargs=None, return_dict: 'bool'=True):
        """
        Args:
            hidden_states ( When discrete, `torch.LongTensor` of shape `(batch size, num latent pixels)`.
                When continuous, `torch.FloatTensor` of shape `(batch size, channel, height, width)`): Input
                hidden_states
            encoder_hidden_states ( `torch.LongTensor` of shape `(batch size, encoder_hidden_states dim)`, *optional*):
                Conditional embeddings for cross attention layer. If not given, cross-attention defaults to
                self-attention.
            timestep ( `torch.long`, *optional*):
                Optional timestep to be applied as an embedding in AdaLayerNorm's. Used to indicate denoising step.
            attention_mask (`torch.FloatTensor`, *optional*):
                Optional attention mask to be applied in Attention
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`models.unet_2d_condition.UNet2DConditionOutput`] instead of a plain tuple.

        Returns:
            [`~models.transformer_2d.Transformer2DModelOutput`] or `tuple`:
            [`~models.transformer_2d.Transformer2DModelOutput`] if `return_dict` is True, otherwise a `tuple`. When
            returning a tuple, the first element is the sample tensor.
        """
        input_states = hidden_states
        encoded_states = []
        tokens_start = 0
        for i in range(2):
            condition_state = encoder_hidden_states[:, tokens_start:tokens_start + self.condition_lengths[i]]
            transformer_index = self.transformer_index_for_condition[i]
            encoded_state = self.transformers[transformer_index](input_states, encoder_hidden_states=condition_state, timestep=timestep, cross_attention_kwargs=cross_attention_kwargs, return_dict=False)[0]
            encoded_states.append(encoded_state - input_states)
            tokens_start += self.condition_lengths[i]
        output_states = encoded_states[0] * self.mix_ratio + encoded_states[1] * (1 - self.mix_ratio)
        output_states = output_states + input_states
        if not return_dict:
            return output_states,
        return Transformer2DModelOutput(sample=output_states)


def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):
    """
    embed_dim: output dimension for each position pos: a list of positions to be encoded: size (M,) out: (M, D)
    """
    if embed_dim % 2 != 0:
        raise ValueError('embed_dim must be divisible by 2')
    omega = np.arange(embed_dim // 2, dtype=np.float64)
    omega /= embed_dim / 2.0
    omega = 1.0 / 10000 ** omega
    pos = pos.reshape(-1)
    out = np.einsum('m,d->md', pos, omega)
    emb_sin = np.sin(out)
    emb_cos = np.cos(out)
    emb = np.concatenate([emb_sin, emb_cos], axis=1)
    return emb


def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):
    if embed_dim % 2 != 0:
        raise ValueError('embed_dim must be divisible by 2')
    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])
    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])
    emb = np.concatenate([emb_h, emb_w], axis=1)
    return emb


def get_2d_sincos_pos_embed(embed_dim, grid_size, cls_token=False, extra_tokens=0):
    """
    grid_size: int of the grid height and width return: pos_embed: [grid_size*grid_size, embed_dim] or
    [1+grid_size*grid_size, embed_dim] (w/ or w/o cls_token)
    """
    grid_h = np.arange(grid_size, dtype=np.float32)
    grid_w = np.arange(grid_size, dtype=np.float32)
    grid = np.meshgrid(grid_w, grid_h)
    grid = np.stack(grid, axis=0)
    grid = grid.reshape([2, 1, grid_size, grid_size])
    pos_embed = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)
    if cls_token and extra_tokens > 0:
        pos_embed = np.concatenate([np.zeros([extra_tokens, embed_dim]), pos_embed], axis=0)
    return pos_embed


class PatchEmbed(nn.Module):
    """2D Image to Patch Embedding"""

    def __init__(self, height=224, width=224, patch_size=16, in_channels=3, embed_dim=768, layer_norm=False, flatten=True, bias=True, use_pos_embed=True):
        super().__init__()
        num_patches = height // patch_size * (width // patch_size)
        self.flatten = flatten
        self.layer_norm = layer_norm
        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=(patch_size, patch_size), stride=patch_size, bias=bias)
        if layer_norm:
            self.norm = nn.LayerNorm(embed_dim, elementwise_affine=False, eps=1e-06)
        else:
            self.norm = None
        self.use_pos_embed = use_pos_embed
        if self.use_pos_embed:
            pos_embed = get_2d_sincos_pos_embed(embed_dim, int(num_patches ** 0.5))
            self.register_buffer('pos_embed', torch.from_numpy(pos_embed).float().unsqueeze(0), persistent=False)

    def forward(self, latent):
        latent = self.proj(latent)
        if self.flatten:
            latent = latent.flatten(2).transpose(1, 2)
        if self.layer_norm:
            latent = self.norm(latent)
        if self.use_pos_embed:
            return latent + self.pos_embed
        else:
            return latent


class GaussianFourierProjection(nn.Module):
    """Gaussian Fourier embeddings for noise levels."""

    def __init__(self, embedding_size: 'int'=256, scale: 'float'=1.0, set_W_to_weight=True, log=True, flip_sin_to_cos=False):
        super().__init__()
        self.weight = nn.Parameter(torch.randn(embedding_size) * scale, requires_grad=False)
        self.log = log
        self.flip_sin_to_cos = flip_sin_to_cos
        if set_W_to_weight:
            self.W = nn.Parameter(torch.randn(embedding_size) * scale, requires_grad=False)
            self.weight = self.W

    def forward(self, x):
        if self.log:
            x = torch.log(x)
        x_proj = x[:, None] * self.weight[None, :] * 2 * np.pi
        if self.flip_sin_to_cos:
            out = torch.cat([torch.cos(x_proj), torch.sin(x_proj)], dim=-1)
        else:
            out = torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)
        return out


class ImagePositionalEmbeddings(nn.Module):
    """
    Converts latent image classes into vector embeddings. Sums the vector embeddings with positional embeddings for the
    height and width of the latent space.

    For more details, see figure 10 of the dall-e paper: https://arxiv.org/abs/2102.12092

    For VQ-diffusion:

    Output vector embeddings are used as input for the transformer.

    Note that the vector embeddings for the transformer are different than the vector embeddings from the VQVAE.

    Args:
        num_embed (`int`):
            Number of embeddings for the latent pixels embeddings.
        height (`int`):
            Height of the latent image i.e. the number of height embeddings.
        width (`int`):
            Width of the latent image i.e. the number of width embeddings.
        embed_dim (`int`):
            Dimension of the produced vector embeddings. Used for the latent pixel, height, and width embeddings.
    """

    def __init__(self, num_embed: 'int', height: 'int', width: 'int', embed_dim: 'int'):
        super().__init__()
        self.height = height
        self.width = width
        self.num_embed = num_embed
        self.embed_dim = embed_dim
        self.emb = nn.Embedding(self.num_embed, embed_dim)
        self.height_emb = nn.Embedding(self.height, embed_dim)
        self.width_emb = nn.Embedding(self.width, embed_dim)

    def forward(self, index):
        emb = self.emb(index)
        height_emb = self.height_emb(torch.arange(self.height, device=index.device).view(1, self.height))
        height_emb = height_emb.unsqueeze(2)
        width_emb = self.width_emb(torch.arange(self.width, device=index.device).view(1, self.width))
        width_emb = width_emb.unsqueeze(1)
        pos_emb = height_emb + width_emb
        pos_emb = pos_emb.view(1, self.height * self.width, -1)
        emb = emb + pos_emb[:, :emb.shape[1], :]
        return emb


class TextImageProjection(nn.Module):

    def __init__(self, text_embed_dim: 'int'=1024, image_embed_dim: 'int'=768, cross_attention_dim: 'int'=768, num_image_text_embeds: 'int'=10):
        super().__init__()
        self.num_image_text_embeds = num_image_text_embeds
        self.image_embeds = nn.Linear(image_embed_dim, self.num_image_text_embeds * cross_attention_dim)
        self.text_proj = nn.Linear(text_embed_dim, cross_attention_dim)

    def forward(self, text_embeds: 'torch.FloatTensor', image_embeds: 'torch.FloatTensor'):
        batch_size = text_embeds.shape[0]
        image_text_embeds = self.image_embeds(image_embeds)
        image_text_embeds = image_text_embeds.reshape(batch_size, self.num_image_text_embeds, -1)
        text_embeds = self.text_proj(text_embeds)
        return torch.cat([image_text_embeds, text_embeds], dim=1)


class AttentionPooling(nn.Module):

    def __init__(self, num_heads, embed_dim, dtype=None):
        super().__init__()
        self.dtype = dtype
        self.positional_embedding = nn.Parameter(torch.randn(1, embed_dim) / embed_dim ** 0.5)
        self.k_proj = nn.Linear(embed_dim, embed_dim, dtype=self.dtype)
        self.q_proj = nn.Linear(embed_dim, embed_dim, dtype=self.dtype)
        self.v_proj = nn.Linear(embed_dim, embed_dim, dtype=self.dtype)
        self.num_heads = num_heads
        self.dim_per_head = embed_dim // self.num_heads

    def forward(self, x):
        bs, length, width = x.size()

        def shape(x):
            x = x.view(bs, -1, self.num_heads, self.dim_per_head)
            x = x.transpose(1, 2)
            x = x.reshape(bs * self.num_heads, -1, self.dim_per_head)
            x = x.transpose(1, 2)
            return x
        class_token = x.mean(dim=1, keepdim=True) + self.positional_embedding
        x = torch.cat([class_token, x], dim=1)
        q = shape(self.q_proj(class_token))
        k = shape(self.k_proj(x))
        v = shape(self.v_proj(x))
        scale = 1 / math.sqrt(math.sqrt(self.dim_per_head))
        weight = torch.einsum('bct,bcs->bts', q * scale, k * scale)
        weight = torch.softmax(weight.float(), dim=-1).type(weight.dtype)
        a = torch.einsum('bts,bcs->bct', weight, v)
        a = a.reshape(bs, -1, 1).transpose(1, 2)
        return a[:, 0, :]


class TextTimeEmbedding(nn.Module):

    def __init__(self, encoder_dim: 'int', time_embed_dim: 'int', num_heads: 'int'=64):
        super().__init__()
        self.norm1 = nn.LayerNorm(encoder_dim)
        self.pool = AttentionPooling(num_heads, encoder_dim)
        self.proj = nn.Linear(encoder_dim, time_embed_dim)
        self.norm2 = nn.LayerNorm(time_embed_dim)

    def forward(self, hidden_states):
        hidden_states = self.norm1(hidden_states)
        hidden_states = self.pool(hidden_states)
        hidden_states = self.proj(hidden_states)
        hidden_states = self.norm2(hidden_states)
        return hidden_states


class TextImageTimeEmbedding(nn.Module):

    def __init__(self, text_embed_dim: 'int'=768, image_embed_dim: 'int'=768, time_embed_dim: 'int'=1536):
        super().__init__()
        self.text_proj = nn.Linear(text_embed_dim, time_embed_dim)
        self.text_norm = nn.LayerNorm(time_embed_dim)
        self.image_proj = nn.Linear(image_embed_dim, time_embed_dim)

    def forward(self, text_embeds: 'torch.FloatTensor', image_embeds: 'torch.FloatTensor'):
        time_text_embeds = self.text_proj(text_embeds)
        time_text_embeds = self.text_norm(time_text_embeds)
        time_image_embeds = self.image_proj(image_embeds)
        return time_image_embeds + time_text_embeds


class ModelMixin(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class PriorTransformer(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class Upsample1D(nn.Module):
    """A 1D upsampling layer with an optional convolution.

    Parameters:
        channels (`int`):
            number of channels in the inputs and outputs.
        use_conv (`bool`, default `False`):
            option to use a convolution.
        use_conv_transpose (`bool`, default `False`):
            option to use a convolution transpose.
        out_channels (`int`, optional):
            number of output channels. Defaults to `channels`.
    """

    def __init__(self, channels, use_conv=False, use_conv_transpose=False, out_channels=None, name='conv'):
        super().__init__()
        self.channels = channels
        self.out_channels = out_channels or channels
        self.use_conv = use_conv
        self.use_conv_transpose = use_conv_transpose
        self.name = name
        self.conv = None
        if use_conv_transpose:
            self.conv = nn.ConvTranspose1d(channels, self.out_channels, 4, 2, 1)
        elif use_conv:
            self.conv = nn.Conv1d(self.channels, self.out_channels, 3, padding=1)

    def forward(self, inputs):
        assert inputs.shape[1] == self.channels
        if self.use_conv_transpose:
            return self.conv(inputs)
        outputs = F.interpolate(inputs, scale_factor=2.0, mode='nearest')
        if self.use_conv:
            outputs = self.conv(outputs)
        return outputs


class Downsample1D(nn.Module):
    """A 1D downsampling layer with an optional convolution.

    Parameters:
        channels (`int`):
            number of channels in the inputs and outputs.
        use_conv (`bool`, default `False`):
            option to use a convolution.
        out_channels (`int`, optional):
            number of output channels. Defaults to `channels`.
        padding (`int`, default `1`):
            padding for the convolution.
    """

    def __init__(self, channels, use_conv=False, out_channels=None, padding=1, name='conv'):
        super().__init__()
        self.channels = channels
        self.out_channels = out_channels or channels
        self.use_conv = use_conv
        self.padding = padding
        stride = 2
        self.name = name
        if use_conv:
            self.conv = nn.Conv1d(self.channels, self.out_channels, 3, stride=stride, padding=padding)
        else:
            assert self.channels == self.out_channels
            self.conv = nn.AvgPool1d(kernel_size=stride, stride=stride)

    def forward(self, x):
        assert x.shape[1] == self.channels
        return self.conv(x)


class Upsample2D(nn.Module):
    """A 2D upsampling layer with an optional convolution.

    Parameters:
        channels (`int`):
            number of channels in the inputs and outputs.
        use_conv (`bool`, default `False`):
            option to use a convolution.
        use_conv_transpose (`bool`, default `False`):
            option to use a convolution transpose.
        out_channels (`int`, optional):
            number of output channels. Defaults to `channels`.
    """

    def __init__(self, channels, use_conv=False, use_conv_transpose=False, out_channels=None, name='conv'):
        super().__init__()
        self.channels = channels
        self.out_channels = out_channels or channels
        self.use_conv = use_conv
        self.use_conv_transpose = use_conv_transpose
        self.name = name
        conv = None
        if use_conv_transpose:
            conv = nn.ConvTranspose2d(channels, self.out_channels, 4, 2, 1)
        elif use_conv:
            conv = nn.Conv2d(self.channels, self.out_channels, 3, padding=1)
        if name == 'conv':
            self.conv = conv
        else:
            self.Conv2d_0 = conv

    def forward(self, hidden_states, output_size=None):
        assert hidden_states.shape[1] == self.channels
        if self.use_conv_transpose:
            return self.conv(hidden_states)
        dtype = hidden_states.dtype
        if dtype == torch.bfloat16:
            hidden_states = hidden_states
        if hidden_states.shape[0] >= 64:
            hidden_states = hidden_states.contiguous()
        if output_size is None:
            hidden_states = F.interpolate(hidden_states, scale_factor=2.0, mode='nearest')
        else:
            hidden_states = F.interpolate(hidden_states, size=output_size, mode='nearest')
        if dtype == torch.bfloat16:
            hidden_states = hidden_states
        if self.use_conv:
            if self.name == 'conv':
                hidden_states = self.conv(hidden_states)
            else:
                hidden_states = self.Conv2d_0(hidden_states)
        return hidden_states


class Downsample2D(nn.Module):
    """A 2D downsampling layer with an optional convolution.

    Parameters:
        channels (`int`):
            number of channels in the inputs and outputs.
        use_conv (`bool`, default `False`):
            option to use a convolution.
        out_channels (`int`, optional):
            number of output channels. Defaults to `channels`.
        padding (`int`, default `1`):
            padding for the convolution.
    """

    def __init__(self, channels, use_conv=False, out_channels=None, padding=1, name='conv'):
        super().__init__()
        self.channels = channels
        self.out_channels = out_channels or channels
        self.use_conv = use_conv
        self.padding = padding
        stride = 2
        self.name = name
        if use_conv:
            conv = nn.Conv2d(self.channels, self.out_channels, 3, stride=stride, padding=padding)
        else:
            assert self.channels == self.out_channels
            conv = nn.AvgPool2d(kernel_size=stride, stride=stride)
        if name == 'conv':
            self.Conv2d_0 = conv
            self.conv = conv
        elif name == 'Conv2d_0':
            self.conv = conv
        else:
            self.conv = conv

    def forward(self, hidden_states):
        assert hidden_states.shape[1] == self.channels
        if self.use_conv and self.padding == 0:
            pad = 0, 1, 0, 1
            hidden_states = F.pad(hidden_states, pad, mode='constant', value=0)
        assert hidden_states.shape[1] == self.channels
        hidden_states = self.conv(hidden_states)
        return hidden_states


def upfirdn2d_native(tensor, kernel, up=1, down=1, pad=(0, 0)):
    up_x = up_y = up
    down_x = down_y = down
    pad_x0 = pad_y0 = pad[0]
    pad_x1 = pad_y1 = pad[1]
    _, channel, in_h, in_w = tensor.shape
    tensor = tensor.reshape(-1, in_h, in_w, 1)
    _, in_h, in_w, minor = tensor.shape
    kernel_h, kernel_w = kernel.shape
    out = tensor.view(-1, in_h, 1, in_w, 1, minor)
    out = F.pad(out, [0, 0, 0, up_x - 1, 0, 0, 0, up_y - 1])
    out = out.view(-1, in_h * up_y, in_w * up_x, minor)
    out = F.pad(out, [0, 0, max(pad_x0, 0), max(pad_x1, 0), max(pad_y0, 0), max(pad_y1, 0)])
    out = out
    out = out[:, max(-pad_y0, 0):out.shape[1] - max(-pad_y1, 0), max(-pad_x0, 0):out.shape[2] - max(-pad_x1, 0), :]
    out = out.permute(0, 3, 1, 2)
    out = out.reshape([-1, 1, in_h * up_y + pad_y0 + pad_y1, in_w * up_x + pad_x0 + pad_x1])
    w = torch.flip(kernel, [0, 1]).view(1, 1, kernel_h, kernel_w)
    out = F.conv2d(out, w)
    out = out.reshape(-1, minor, in_h * up_y + pad_y0 + pad_y1 - kernel_h + 1, in_w * up_x + pad_x0 + pad_x1 - kernel_w + 1)
    out = out.permute(0, 2, 3, 1)
    out = out[:, ::down_y, ::down_x, :]
    out_h = (in_h * up_y + pad_y0 + pad_y1 - kernel_h) // down_y + 1
    out_w = (in_w * up_x + pad_x0 + pad_x1 - kernel_w) // down_x + 1
    return out.view(-1, channel, out_h, out_w)


class FirUpsample2D(nn.Module):
    """A 2D FIR upsampling layer with an optional convolution.

    Parameters:
        channels (`int`):
            number of channels in the inputs and outputs.
        use_conv (`bool`, default `False`):
            option to use a convolution.
        out_channels (`int`, optional):
            number of output channels. Defaults to `channels`.
        fir_kernel (`tuple`, default `(1, 3, 3, 1)`):
            kernel for the FIR filter.
    """

    def __init__(self, channels=None, out_channels=None, use_conv=False, fir_kernel=(1, 3, 3, 1)):
        super().__init__()
        out_channels = out_channels if out_channels else channels
        if use_conv:
            self.Conv2d_0 = nn.Conv2d(channels, out_channels, kernel_size=3, stride=1, padding=1)
        self.use_conv = use_conv
        self.fir_kernel = fir_kernel
        self.out_channels = out_channels

    def _upsample_2d(self, hidden_states, weight=None, kernel=None, factor=2, gain=1):
        """Fused `upsample_2d()` followed by `Conv2d()`.

        Padding is performed only once at the beginning, not between the operations. The fused op is considerably more
        efficient than performing the same calculation using standard TensorFlow ops. It supports gradients of
        arbitrary order.

        Args:
            hidden_states: Input tensor of the shape `[N, C, H, W]` or `[N, H, W, C]`.
            weight: Weight tensor of the shape `[filterH, filterW, inChannels,
                outChannels]`. Grouped convolution can be performed by `inChannels = x.shape[0] // numGroups`.
            kernel: FIR filter of the shape `[firH, firW]` or `[firN]`
                (separable). The default is `[1] * factor`, which corresponds to nearest-neighbor upsampling.
            factor: Integer upsampling factor (default: 2).
            gain: Scaling factor for signal magnitude (default: 1.0).

        Returns:
            output: Tensor of the shape `[N, C, H * factor, W * factor]` or `[N, H * factor, W * factor, C]`, and same
            datatype as `hidden_states`.
        """
        assert isinstance(factor, int) and factor >= 1
        if kernel is None:
            kernel = [1] * factor
        kernel = torch.tensor(kernel, dtype=torch.float32)
        if kernel.ndim == 1:
            kernel = torch.outer(kernel, kernel)
        kernel /= torch.sum(kernel)
        kernel = kernel * (gain * factor ** 2)
        if self.use_conv:
            convH = weight.shape[2]
            convW = weight.shape[3]
            inC = weight.shape[1]
            pad_value = kernel.shape[0] - factor - (convW - 1)
            stride = factor, factor
            output_shape = (hidden_states.shape[2] - 1) * factor + convH, (hidden_states.shape[3] - 1) * factor + convW
            output_padding = output_shape[0] - (hidden_states.shape[2] - 1) * stride[0] - convH, output_shape[1] - (hidden_states.shape[3] - 1) * stride[1] - convW
            assert output_padding[0] >= 0 and output_padding[1] >= 0
            num_groups = hidden_states.shape[1] // inC
            weight = torch.reshape(weight, (num_groups, -1, inC, convH, convW))
            weight = torch.flip(weight, dims=[3, 4]).permute(0, 2, 1, 3, 4)
            weight = torch.reshape(weight, (num_groups * inC, -1, convH, convW))
            inverse_conv = F.conv_transpose2d(hidden_states, weight, stride=stride, output_padding=output_padding, padding=0)
            output = upfirdn2d_native(inverse_conv, torch.tensor(kernel, device=inverse_conv.device), pad=((pad_value + 1) // 2 + factor - 1, pad_value // 2 + 1))
        else:
            pad_value = kernel.shape[0] - factor
            output = upfirdn2d_native(hidden_states, torch.tensor(kernel, device=hidden_states.device), up=factor, pad=((pad_value + 1) // 2 + factor - 1, pad_value // 2))
        return output

    def forward(self, hidden_states):
        if self.use_conv:
            height = self._upsample_2d(hidden_states, self.Conv2d_0.weight, kernel=self.fir_kernel)
            height = height + self.Conv2d_0.bias.reshape(1, -1, 1, 1)
        else:
            height = self._upsample_2d(hidden_states, kernel=self.fir_kernel, factor=2)
        return height


class FirDownsample2D(nn.Module):
    """A 2D FIR downsampling layer with an optional convolution.

    Parameters:
        channels (`int`):
            number of channels in the inputs and outputs.
        use_conv (`bool`, default `False`):
            option to use a convolution.
        out_channels (`int`, optional):
            number of output channels. Defaults to `channels`.
        fir_kernel (`tuple`, default `(1, 3, 3, 1)`):
            kernel for the FIR filter.
    """

    def __init__(self, channels=None, out_channels=None, use_conv=False, fir_kernel=(1, 3, 3, 1)):
        super().__init__()
        out_channels = out_channels if out_channels else channels
        if use_conv:
            self.Conv2d_0 = nn.Conv2d(channels, out_channels, kernel_size=3, stride=1, padding=1)
        self.fir_kernel = fir_kernel
        self.use_conv = use_conv
        self.out_channels = out_channels

    def _downsample_2d(self, hidden_states, weight=None, kernel=None, factor=2, gain=1):
        """Fused `Conv2d()` followed by `downsample_2d()`.
        Padding is performed only once at the beginning, not between the operations. The fused op is considerably more
        efficient than performing the same calculation using standard TensorFlow ops. It supports gradients of
        arbitrary order.

        Args:
            hidden_states: Input tensor of the shape `[N, C, H, W]` or `[N, H, W, C]`.
            weight:
                Weight tensor of the shape `[filterH, filterW, inChannels, outChannels]`. Grouped convolution can be
                performed by `inChannels = x.shape[0] // numGroups`.
            kernel: FIR filter of the shape `[firH, firW]` or `[firN]` (separable). The default is `[1] *
            factor`, which corresponds to average pooling.
            factor: Integer downsampling factor (default: 2).
            gain: Scaling factor for signal magnitude (default: 1.0).

        Returns:
            output: Tensor of the shape `[N, C, H // factor, W // factor]` or `[N, H // factor, W // factor, C]`, and
            same datatype as `x`.
        """
        assert isinstance(factor, int) and factor >= 1
        if kernel is None:
            kernel = [1] * factor
        kernel = torch.tensor(kernel, dtype=torch.float32)
        if kernel.ndim == 1:
            kernel = torch.outer(kernel, kernel)
        kernel /= torch.sum(kernel)
        kernel = kernel * gain
        if self.use_conv:
            _, _, convH, convW = weight.shape
            pad_value = kernel.shape[0] - factor + (convW - 1)
            stride_value = [factor, factor]
            upfirdn_input = upfirdn2d_native(hidden_states, torch.tensor(kernel, device=hidden_states.device), pad=((pad_value + 1) // 2, pad_value // 2))
            output = F.conv2d(upfirdn_input, weight, stride=stride_value, padding=0)
        else:
            pad_value = kernel.shape[0] - factor
            output = upfirdn2d_native(hidden_states, torch.tensor(kernel, device=hidden_states.device), down=factor, pad=((pad_value + 1) // 2, pad_value // 2))
        return output

    def forward(self, hidden_states):
        if self.use_conv:
            downsample_input = self._downsample_2d(hidden_states, weight=self.Conv2d_0.weight, kernel=self.fir_kernel)
            hidden_states = downsample_input + self.Conv2d_0.bias.reshape(1, -1, 1, 1)
        else:
            hidden_states = self._downsample_2d(hidden_states, kernel=self.fir_kernel, factor=2)
        return hidden_states


class KDownsample2D(nn.Module):

    def __init__(self, pad_mode='reflect'):
        super().__init__()
        self.pad_mode = pad_mode
        kernel_1d = torch.tensor([[1 / 8, 3 / 8, 3 / 8, 1 / 8]])
        self.pad = kernel_1d.shape[1] // 2 - 1
        self.register_buffer('kernel', kernel_1d.T @ kernel_1d, persistent=False)

    def forward(self, x):
        x = F.pad(x, (self.pad,) * 4, self.pad_mode)
        weight = x.new_zeros([x.shape[1], x.shape[1], self.kernel.shape[0], self.kernel.shape[1]])
        indices = torch.arange(x.shape[1], device=x.device)
        kernel = self.kernel[None, :].expand(x.shape[1], -1, -1)
        weight[indices, indices] = kernel
        return F.conv2d(x, weight, stride=2)


class KUpsample2D(nn.Module):

    def __init__(self, pad_mode='reflect'):
        super().__init__()
        self.pad_mode = pad_mode
        kernel_1d = torch.tensor([[1 / 8, 3 / 8, 3 / 8, 1 / 8]]) * 2
        self.pad = kernel_1d.shape[1] // 2 - 1
        self.register_buffer('kernel', kernel_1d.T @ kernel_1d, persistent=False)

    def forward(self, x):
        x = F.pad(x, ((self.pad + 1) // 2,) * 4, self.pad_mode)
        weight = x.new_zeros([x.shape[1], x.shape[1], self.kernel.shape[0], self.kernel.shape[1]])
        indices = torch.arange(x.shape[1], device=x.device)
        kernel = self.kernel[None, :].expand(x.shape[1], -1, -1)
        weight[indices, indices] = kernel
        return F.conv_transpose2d(x, weight, stride=2, padding=self.pad * 2 + 1)


def downsample_2d(hidden_states, kernel=None, factor=2, gain=1):
    """Downsample2D a batch of 2D images with the given filter.
    Accepts a batch of 2D images of the shape `[N, C, H, W]` or `[N, H, W, C]` and downsamples each image with the
    given filter. The filter is normalized so that if the input pixels are constant, they will be scaled by the
    specified `gain`. Pixels outside the image are assumed to be zero, and the filter is padded with zeros so that its
    shape is a multiple of the downsampling factor.

    Args:
        hidden_states: Input tensor of the shape `[N, C, H, W]` or `[N, H, W, C]`.
        kernel: FIR filter of the shape `[firH, firW]` or `[firN]`
          (separable). The default is `[1] * factor`, which corresponds to average pooling.
        factor: Integer downsampling factor (default: 2).
        gain: Scaling factor for signal magnitude (default: 1.0).

    Returns:
        output: Tensor of the shape `[N, C, H // factor, W // factor]`
    """
    assert isinstance(factor, int) and factor >= 1
    if kernel is None:
        kernel = [1] * factor
    kernel = torch.tensor(kernel, dtype=torch.float32)
    if kernel.ndim == 1:
        kernel = torch.outer(kernel, kernel)
    kernel /= torch.sum(kernel)
    kernel = kernel * gain
    pad_value = kernel.shape[0] - factor
    output = upfirdn2d_native(hidden_states, kernel, down=factor, pad=((pad_value + 1) // 2, pad_value // 2))
    return output


def upsample_2d(hidden_states, kernel=None, factor=2, gain=1):
    """Upsample2D a batch of 2D images with the given filter.
    Accepts a batch of 2D images of the shape `[N, C, H, W]` or `[N, H, W, C]` and upsamples each image with the given
    filter. The filter is normalized so that if the input pixels are constant, they will be scaled by the specified
    `gain`. Pixels outside the image are assumed to be zero, and the filter is padded with zeros so that its shape is
    a: multiple of the upsampling factor.

    Args:
        hidden_states: Input tensor of the shape `[N, C, H, W]` or `[N, H, W, C]`.
        kernel: FIR filter of the shape `[firH, firW]` or `[firN]`
          (separable). The default is `[1] * factor`, which corresponds to nearest-neighbor upsampling.
        factor: Integer upsampling factor (default: 2).
        gain: Scaling factor for signal magnitude (default: 1.0).

    Returns:
        output: Tensor of the shape `[N, C, H * factor, W * factor]`
    """
    assert isinstance(factor, int) and factor >= 1
    if kernel is None:
        kernel = [1] * factor
    kernel = torch.tensor(kernel, dtype=torch.float32)
    if kernel.ndim == 1:
        kernel = torch.outer(kernel, kernel)
    kernel /= torch.sum(kernel)
    kernel = kernel * (gain * factor ** 2)
    pad_value = kernel.shape[0] - factor
    output = upfirdn2d_native(hidden_states, kernel, up=factor, pad=((pad_value + 1) // 2 + factor - 1, pad_value // 2))
    return output


class ResnetBlock2D(nn.Module):
    """
    A Resnet block.

    Parameters:
        in_channels (`int`): The number of channels in the input.
        out_channels (`int`, *optional*, default to be `None`):
            The number of output channels for the first conv2d layer. If None, same as `in_channels`.
        dropout (`float`, *optional*, defaults to `0.0`): The dropout probability to use.
        temb_channels (`int`, *optional*, default to `512`): the number of channels in timestep embedding.
        groups (`int`, *optional*, default to `32`): The number of groups to use for the first normalization layer.
        groups_out (`int`, *optional*, default to None):
            The number of groups to use for the second normalization layer. if set to None, same as `groups`.
        eps (`float`, *optional*, defaults to `1e-6`): The epsilon to use for the normalization.
        non_linearity (`str`, *optional*, default to `"swish"`): the activation function to use.
        time_embedding_norm (`str`, *optional*, default to `"default"` ): Time scale shift config.
            By default, apply timestep embedding conditioning with a simple shift mechanism. Choose "scale_shift" or
            "ada_group" for a stronger conditioning with scale and shift.
        kernel (`torch.FloatTensor`, optional, default to None): FIR filter, see
            [`~models.resnet.FirUpsample2D`] and [`~models.resnet.FirDownsample2D`].
        output_scale_factor (`float`, *optional*, default to be `1.0`): the scale factor to use for the output.
        use_in_shortcut (`bool`, *optional*, default to `True`):
            If `True`, add a 1x1 nn.conv2d layer for skip-connection.
        up (`bool`, *optional*, default to `False`): If `True`, add an upsample layer.
        down (`bool`, *optional*, default to `False`): If `True`, add a downsample layer.
        conv_shortcut_bias (`bool`, *optional*, default to `True`):  If `True`, adds a learnable bias to the
            `conv_shortcut` output.
        conv_2d_out_channels (`int`, *optional*, default to `None`): the number of channels in the output.
            If None, same as `out_channels`.
    """

    def __init__(self, *, in_channels, out_channels=None, conv_shortcut=False, dropout=0.0, temb_channels=512, groups=32, groups_out=None, pre_norm=True, eps=1e-06, non_linearity='swish', skip_time_act=False, time_embedding_norm='default', kernel=None, output_scale_factor=1.0, use_in_shortcut=None, up=False, down=False, conv_shortcut_bias: bool=True, conv_2d_out_channels: Optional[int]=None):
        super().__init__()
        self.pre_norm = pre_norm
        self.pre_norm = True
        self.in_channels = in_channels
        out_channels = in_channels if out_channels is None else out_channels
        self.out_channels = out_channels
        self.use_conv_shortcut = conv_shortcut
        self.up = up
        self.down = down
        self.output_scale_factor = output_scale_factor
        self.time_embedding_norm = time_embedding_norm
        self.skip_time_act = skip_time_act
        if groups_out is None:
            groups_out = groups
        if self.time_embedding_norm == 'ada_group':
            self.norm1 = AdaGroupNorm(temb_channels, in_channels, groups, eps=eps)
        elif self.time_embedding_norm == 'spatial':
            self.norm1 = SpatialNorm(in_channels, temb_channels)
        else:
            self.norm1 = torch.nn.GroupNorm(num_groups=groups, num_channels=in_channels, eps=eps, affine=True)
        self.conv1 = torch.nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1)
        if temb_channels is not None:
            if self.time_embedding_norm == 'default':
                self.time_emb_proj = torch.nn.Linear(temb_channels, out_channels)
            elif self.time_embedding_norm == 'scale_shift':
                self.time_emb_proj = torch.nn.Linear(temb_channels, 2 * out_channels)
            elif self.time_embedding_norm == 'ada_group' or self.time_embedding_norm == 'spatial':
                self.time_emb_proj = None
            else:
                raise ValueError(f'unknown time_embedding_norm : {self.time_embedding_norm} ')
        else:
            self.time_emb_proj = None
        if self.time_embedding_norm == 'ada_group':
            self.norm2 = AdaGroupNorm(temb_channels, out_channels, groups_out, eps=eps)
        elif self.time_embedding_norm == 'spatial':
            self.norm2 = SpatialNorm(out_channels, temb_channels)
        else:
            self.norm2 = torch.nn.GroupNorm(num_groups=groups_out, num_channels=out_channels, eps=eps, affine=True)
        self.dropout = torch.nn.Dropout(dropout)
        conv_2d_out_channels = conv_2d_out_channels or out_channels
        self.conv2 = torch.nn.Conv2d(out_channels, conv_2d_out_channels, kernel_size=3, stride=1, padding=1)
        self.nonlinearity = get_activation(non_linearity)
        self.upsample = self.downsample = None
        if self.up:
            if kernel == 'fir':
                fir_kernel = 1, 3, 3, 1
                self.upsample = lambda x: upsample_2d(x, kernel=fir_kernel)
            elif kernel == 'sde_vp':
                self.upsample = partial(F.interpolate, scale_factor=2.0, mode='nearest')
            else:
                self.upsample = Upsample2D(in_channels, use_conv=False)
        elif self.down:
            if kernel == 'fir':
                fir_kernel = 1, 3, 3, 1
                self.downsample = lambda x: downsample_2d(x, kernel=fir_kernel)
            elif kernel == 'sde_vp':
                self.downsample = partial(F.avg_pool2d, kernel_size=2, stride=2)
            else:
                self.downsample = Downsample2D(in_channels, use_conv=False, padding=1, name='op')
        self.use_in_shortcut = self.in_channels != conv_2d_out_channels if use_in_shortcut is None else use_in_shortcut
        self.conv_shortcut = None
        if self.use_in_shortcut:
            self.conv_shortcut = torch.nn.Conv2d(in_channels, conv_2d_out_channels, kernel_size=1, stride=1, padding=0, bias=conv_shortcut_bias)

    def forward(self, input_tensor, temb):
        hidden_states = input_tensor
        if self.time_embedding_norm == 'ada_group' or self.time_embedding_norm == 'spatial':
            hidden_states = self.norm1(hidden_states, temb)
        else:
            hidden_states = self.norm1(hidden_states)
        hidden_states = self.nonlinearity(hidden_states)
        if self.upsample is not None:
            if hidden_states.shape[0] >= 64:
                input_tensor = input_tensor.contiguous()
                hidden_states = hidden_states.contiguous()
            input_tensor = self.upsample(input_tensor)
            hidden_states = self.upsample(hidden_states)
        elif self.downsample is not None:
            input_tensor = self.downsample(input_tensor)
            hidden_states = self.downsample(hidden_states)
        hidden_states = self.conv1(hidden_states)
        if self.time_emb_proj is not None:
            if not self.skip_time_act:
                temb = self.nonlinearity(temb)
            temb = self.time_emb_proj(temb)[:, :, None, None]
        if temb is not None and self.time_embedding_norm == 'default':
            hidden_states = hidden_states + temb
        if self.time_embedding_norm == 'ada_group' or self.time_embedding_norm == 'spatial':
            hidden_states = self.norm2(hidden_states, temb)
        else:
            hidden_states = self.norm2(hidden_states)
        if temb is not None and self.time_embedding_norm == 'scale_shift':
            scale, shift = torch.chunk(temb, 2, dim=1)
            hidden_states = hidden_states * (1 + scale) + shift
        hidden_states = self.nonlinearity(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.conv2(hidden_states)
        if self.conv_shortcut is not None:
            input_tensor = self.conv_shortcut(input_tensor)
        output_tensor = (input_tensor + hidden_states) / self.output_scale_factor
        return output_tensor


def rearrange_dims(tensor):
    if len(tensor.shape) == 2:
        return tensor[:, :, None]
    if len(tensor.shape) == 3:
        return tensor[:, :, None, :]
    elif len(tensor.shape) == 4:
        return tensor[:, :, 0, :]
    else:
        raise ValueError(f'`len(tensor)`: {len(tensor)} has to be 2, 3 or 4.')


class Conv1dBlock(nn.Module):
    """
    Conv1d --> GroupNorm --> Mish
    """

    def __init__(self, inp_channels, out_channels, kernel_size, n_groups=8):
        super().__init__()
        self.conv1d = nn.Conv1d(inp_channels, out_channels, kernel_size, padding=kernel_size // 2)
        self.group_norm = nn.GroupNorm(n_groups, out_channels)
        self.mish = nn.Mish()

    def forward(self, x):
        x = self.conv1d(x)
        x = rearrange_dims(x)
        x = self.group_norm(x)
        x = rearrange_dims(x)
        x = self.mish(x)
        return x


class ResidualTemporalBlock1D(nn.Module):

    def __init__(self, inp_channels, out_channels, embed_dim, kernel_size=5):
        super().__init__()
        self.conv_in = Conv1dBlock(inp_channels, out_channels, kernel_size)
        self.conv_out = Conv1dBlock(out_channels, out_channels, kernel_size)
        self.time_emb_act = nn.Mish()
        self.time_emb = nn.Linear(embed_dim, out_channels)
        self.residual_conv = nn.Conv1d(inp_channels, out_channels, 1) if inp_channels != out_channels else nn.Identity()

    def forward(self, x, t):
        """
        Args:
            x : [ batch_size x inp_channels x horizon ]
            t : [ batch_size x embed_dim ]

        returns:
            out : [ batch_size x out_channels x horizon ]
        """
        t = self.time_emb_act(t)
        t = self.time_emb(t)
        out = self.conv_in(x) + rearrange_dims(t)
        out = self.conv_out(out)
        return out + self.residual_conv(x)


class TemporalConvLayer(nn.Module):
    """
    Temporal convolutional layer that can be used for video (sequence of images) input Code mostly copied from:
    https://github.com/modelscope/modelscope/blob/1509fdb973e5871f37148a4b5e5964cafd43e64d/modelscope/models/multi_modal/video_synthesis/unet_sd.py#L1016
    """

    def __init__(self, in_dim, out_dim=None, dropout=0.0):
        super().__init__()
        out_dim = out_dim or in_dim
        self.in_dim = in_dim
        self.out_dim = out_dim
        self.conv1 = nn.Sequential(nn.GroupNorm(32, in_dim), nn.SiLU(), nn.Conv3d(in_dim, out_dim, (3, 1, 1), padding=(1, 0, 0)))
        self.conv2 = nn.Sequential(nn.GroupNorm(32, out_dim), nn.SiLU(), nn.Dropout(dropout), nn.Conv3d(out_dim, in_dim, (3, 1, 1), padding=(1, 0, 0)))
        self.conv3 = nn.Sequential(nn.GroupNorm(32, out_dim), nn.SiLU(), nn.Dropout(dropout), nn.Conv3d(out_dim, in_dim, (3, 1, 1), padding=(1, 0, 0)))
        self.conv4 = nn.Sequential(nn.GroupNorm(32, out_dim), nn.SiLU(), nn.Dropout(dropout), nn.Conv3d(out_dim, in_dim, (3, 1, 1), padding=(1, 0, 0)))
        nn.init.zeros_(self.conv4[-1].weight)
        nn.init.zeros_(self.conv4[-1].bias)

    def forward(self, hidden_states, num_frames=1):
        hidden_states = hidden_states[None, :].reshape((-1, num_frames) + hidden_states.shape[1:]).permute(0, 2, 1, 3, 4)
        identity = hidden_states
        hidden_states = self.conv1(hidden_states)
        hidden_states = self.conv2(hidden_states)
        hidden_states = self.conv3(hidden_states)
        hidden_states = self.conv4(hidden_states)
        hidden_states = identity + hidden_states
        hidden_states = hidden_states.permute(0, 2, 1, 3, 4).reshape((hidden_states.shape[0] * hidden_states.shape[2], -1) + hidden_states.shape[3:])
        return hidden_states


class T5FilmDecoder(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class T5LayerNorm(nn.Module):

    def __init__(self, hidden_size, eps=1e-06):
        """
        Construct a layernorm module in the T5 style. No bias and no subtraction of mean.
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        if self.weight.dtype in [torch.float16, torch.bfloat16]:
            hidden_states = hidden_states
        return self.weight * hidden_states


class T5LayerCrossAttention(nn.Module):

    def __init__(self, d_model, d_kv, num_heads, dropout_rate, layer_norm_epsilon):
        super().__init__()
        self.attention = Attention(query_dim=d_model, heads=num_heads, dim_head=d_kv, out_bias=False, scale_qk=False)
        self.layer_norm = T5LayerNorm(d_model, eps=layer_norm_epsilon)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, hidden_states, key_value_states=None, attention_mask=None):
        normed_hidden_states = self.layer_norm(hidden_states)
        attention_output = self.attention(normed_hidden_states, encoder_hidden_states=key_value_states, attention_mask=attention_mask.squeeze(1))
        layer_output = hidden_states + self.dropout(attention_output)
        return layer_output


class NewGELUActivation(nn.Module):
    """
    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see
    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415
    """

    def forward(self, input: 'torch.Tensor') ->torch.Tensor:
        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))


class T5DenseGatedActDense(nn.Module):

    def __init__(self, d_model, d_ff, dropout_rate):
        super().__init__()
        self.wi_0 = nn.Linear(d_model, d_ff, bias=False)
        self.wi_1 = nn.Linear(d_model, d_ff, bias=False)
        self.wo = nn.Linear(d_ff, d_model, bias=False)
        self.dropout = nn.Dropout(dropout_rate)
        self.act = NewGELUActivation()

    def forward(self, hidden_states):
        hidden_gelu = self.act(self.wi_0(hidden_states))
        hidden_linear = self.wi_1(hidden_states)
        hidden_states = hidden_gelu * hidden_linear
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.wo(hidden_states)
        return hidden_states


class T5FiLMLayer(nn.Module):
    """
    FiLM Layer
    """

    def __init__(self, in_features, out_features):
        super().__init__()
        self.scale_bias = nn.Linear(in_features, out_features * 2, bias=False)

    def forward(self, x, conditioning_emb):
        emb = self.scale_bias(conditioning_emb)
        scale, shift = torch.chunk(emb, 2, -1)
        x = x * (1 + scale) + shift
        return x


class T5LayerFFCond(nn.Module):

    def __init__(self, d_model, d_ff, dropout_rate, layer_norm_epsilon):
        super().__init__()
        self.DenseReluDense = T5DenseGatedActDense(d_model=d_model, d_ff=d_ff, dropout_rate=dropout_rate)
        self.film = T5FiLMLayer(in_features=d_model * 4, out_features=d_model)
        self.layer_norm = T5LayerNorm(d_model, eps=layer_norm_epsilon)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, hidden_states, conditioning_emb=None):
        forwarded_states = self.layer_norm(hidden_states)
        if conditioning_emb is not None:
            forwarded_states = self.film(forwarded_states, conditioning_emb)
        forwarded_states = self.DenseReluDense(forwarded_states)
        hidden_states = hidden_states + self.dropout(forwarded_states)
        return hidden_states


class T5LayerSelfAttentionCond(nn.Module):

    def __init__(self, d_model, d_kv, num_heads, dropout_rate):
        super().__init__()
        self.layer_norm = T5LayerNorm(d_model)
        self.FiLMLayer = T5FiLMLayer(in_features=d_model * 4, out_features=d_model)
        self.attention = Attention(query_dim=d_model, heads=num_heads, dim_head=d_kv, out_bias=False, scale_qk=False)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, hidden_states, conditioning_emb=None, attention_mask=None):
        normed_hidden_states = self.layer_norm(hidden_states)
        if conditioning_emb is not None:
            normed_hidden_states = self.FiLMLayer(normed_hidden_states, conditioning_emb)
        attention_output = self.attention(normed_hidden_states)
        hidden_states = hidden_states + self.dropout(attention_output)
        return hidden_states


class DecoderLayer(nn.Module):

    def __init__(self, d_model, d_kv, num_heads, d_ff, dropout_rate, layer_norm_epsilon=1e-06):
        super().__init__()
        self.layer = nn.ModuleList()
        self.layer.append(T5LayerSelfAttentionCond(d_model=d_model, d_kv=d_kv, num_heads=num_heads, dropout_rate=dropout_rate))
        self.layer.append(T5LayerCrossAttention(d_model=d_model, d_kv=d_kv, num_heads=num_heads, dropout_rate=dropout_rate, layer_norm_epsilon=layer_norm_epsilon))
        self.layer.append(T5LayerFFCond(d_model=d_model, d_ff=d_ff, dropout_rate=dropout_rate, layer_norm_epsilon=layer_norm_epsilon))

    def forward(self, hidden_states, conditioning_emb=None, attention_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, encoder_decoder_position_bias=None):
        hidden_states = self.layer[0](hidden_states, conditioning_emb=conditioning_emb, attention_mask=attention_mask)
        if encoder_hidden_states is not None:
            encoder_extended_attention_mask = torch.where(encoder_attention_mask > 0, 0, -10000000000.0)
            hidden_states = self.layer[1](hidden_states, key_value_states=encoder_hidden_states, attention_mask=encoder_extended_attention_mask)
        hidden_states = self.layer[-1](hidden_states, conditioning_emb)
        return hidden_states,


@dataclass
class TransformerTemporalModelOutput(BaseOutput):
    """
    Args:
        sample (`torch.FloatTensor` of shape `(batch_size x num_frames, num_channels, height, width)`)
            Hidden states conditioned on `encoder_hidden_states` input.
    """
    sample: 'torch.FloatTensor'


def register_to_config(init):
    """
    Decorator to apply on the init of classes inheriting from [`ConfigMixin`] so that all the arguments are
    automatically sent to `self.register_for_config`. To ignore a specific argument accepted by the init but that
    shouldn't be registered in the config, use the `ignore_for_config` class variable

    Warning: Once decorated, all private arguments (beginning with an underscore) are trashed and not sent to the init!
    """

    @functools.wraps(init)
    def inner_init(self, *args, **kwargs):
        init_kwargs = {k: v for k, v in kwargs.items() if not k.startswith('_')}
        config_init_kwargs = {k: v for k, v in kwargs.items() if k.startswith('_')}
        if not isinstance(self, ConfigMixin):
            raise RuntimeError(f'`@register_for_config` was applied to {self.__class__.__name__} init method, but this class does not inherit from `ConfigMixin`.')
        ignore = getattr(self, 'ignore_for_config', [])
        new_kwargs = {}
        signature = inspect.signature(init)
        parameters = {name: p.default for i, (name, p) in enumerate(signature.parameters.items()) if i > 0 and name not in ignore}
        for arg, name in zip(args, parameters.keys()):
            new_kwargs[name] = arg
        new_kwargs.update({k: init_kwargs.get(k, default) for k, default in parameters.items() if k not in ignore and k not in new_kwargs})
        new_kwargs = {**config_init_kwargs, **new_kwargs}
        getattr(self, 'register_to_config')(**new_kwargs)
        init(self, *args, **init_kwargs)
    return inner_init


class TransformerTemporalModel(ModelMixin, ConfigMixin):
    """
    Transformer model for video-like data.

    Parameters:
        num_attention_heads (`int`, *optional*, defaults to 16): The number of heads to use for multi-head attention.
        attention_head_dim (`int`, *optional*, defaults to 88): The number of channels in each head.
        in_channels (`int`, *optional*):
            Pass if the input is continuous. The number of channels in the input and output.
        num_layers (`int`, *optional*, defaults to 1): The number of layers of Transformer blocks to use.
        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.
        cross_attention_dim (`int`, *optional*): The number of encoder_hidden_states dimensions to use.
        sample_size (`int`, *optional*): Pass if the input is discrete. The width of the latent images.
            Note that this is fixed at training time as it is used for learning a number of position embeddings. See
            `ImagePositionalEmbeddings`.
        activation_fn (`str`, *optional*, defaults to `"geglu"`): Activation function to be used in feed-forward.
        attention_bias (`bool`, *optional*):
            Configure if the TransformerBlocks' attention should contain a bias parameter.
        double_self_attention (`bool`, *optional*):
            Configure if each TransformerBlock should contain two self-attention layers
    """

    @register_to_config
    def __init__(self, num_attention_heads: 'int'=16, attention_head_dim: 'int'=88, in_channels: 'Optional[int]'=None, out_channels: 'Optional[int]'=None, num_layers: 'int'=1, dropout: 'float'=0.0, norm_num_groups: 'int'=32, cross_attention_dim: 'Optional[int]'=None, attention_bias: 'bool'=False, sample_size: 'Optional[int]'=None, activation_fn: 'str'='geglu', norm_elementwise_affine: 'bool'=True, double_self_attention: 'bool'=True):
        super().__init__()
        self.num_attention_heads = num_attention_heads
        self.attention_head_dim = attention_head_dim
        inner_dim = num_attention_heads * attention_head_dim
        self.in_channels = in_channels
        self.norm = torch.nn.GroupNorm(num_groups=norm_num_groups, num_channels=in_channels, eps=1e-06, affine=True)
        self.proj_in = nn.Linear(in_channels, inner_dim)
        self.transformer_blocks = nn.ModuleList([BasicTransformerBlock(inner_dim, num_attention_heads, attention_head_dim, dropout=dropout, cross_attention_dim=cross_attention_dim, activation_fn=activation_fn, attention_bias=attention_bias, double_self_attention=double_self_attention, norm_elementwise_affine=norm_elementwise_affine) for d in range(num_layers)])
        self.proj_out = nn.Linear(inner_dim, in_channels)

    def forward(self, hidden_states, encoder_hidden_states=None, timestep=None, class_labels=None, num_frames=1, cross_attention_kwargs=None, return_dict: 'bool'=True):
        """
        Args:
            hidden_states ( When discrete, `torch.LongTensor` of shape `(batch size, num latent pixels)`.
                When continous, `torch.FloatTensor` of shape `(batch size, channel, height, width)`): Input
                hidden_states
            encoder_hidden_states ( `torch.LongTensor` of shape `(batch size, encoder_hidden_states dim)`, *optional*):
                Conditional embeddings for cross attention layer. If not given, cross-attention defaults to
                self-attention.
            timestep ( `torch.long`, *optional*):
                Optional timestep to be applied as an embedding in AdaLayerNorm's. Used to indicate denoising step.
            class_labels ( `torch.LongTensor` of shape `(batch size, num classes)`, *optional*):
                Optional class labels to be applied as an embedding in AdaLayerZeroNorm. Used to indicate class labels
                conditioning.
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`models.unet_2d_condition.UNet2DConditionOutput`] instead of a plain tuple.

        Returns:
            [`~models.transformer_2d.TransformerTemporalModelOutput`] or `tuple`:
            [`~models.transformer_2d.TransformerTemporalModelOutput`] if `return_dict` is True, otherwise a `tuple`.
            When returning a tuple, the first element is the sample tensor.
        """
        batch_frames, channel, height, width = hidden_states.shape
        batch_size = batch_frames // num_frames
        residual = hidden_states
        hidden_states = hidden_states[None, :].reshape(batch_size, num_frames, channel, height, width)
        hidden_states = hidden_states.permute(0, 2, 1, 3, 4)
        hidden_states = self.norm(hidden_states)
        hidden_states = hidden_states.permute(0, 3, 4, 2, 1).reshape(batch_size * height * width, num_frames, channel)
        hidden_states = self.proj_in(hidden_states)
        for block in self.transformer_blocks:
            hidden_states = block(hidden_states, encoder_hidden_states=encoder_hidden_states, timestep=timestep, cross_attention_kwargs=cross_attention_kwargs, class_labels=class_labels)
        hidden_states = self.proj_out(hidden_states)
        hidden_states = hidden_states[None, None, :].reshape(batch_size, height, width, channel, num_frames).permute(0, 3, 4, 1, 2).contiguous()
        hidden_states = hidden_states.reshape(batch_frames, channel, height, width)
        output = hidden_states + residual
        if not return_dict:
            return output,
        return TransformerTemporalModelOutput(sample=output)


class UNet1DModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class DownResnetBlock1D(nn.Module):

    def __init__(self, in_channels, out_channels=None, num_layers=1, conv_shortcut=False, temb_channels=32, groups=32, groups_out=None, non_linearity=None, time_embedding_norm='default', output_scale_factor=1.0, add_downsample=True):
        super().__init__()
        self.in_channels = in_channels
        out_channels = in_channels if out_channels is None else out_channels
        self.out_channels = out_channels
        self.use_conv_shortcut = conv_shortcut
        self.time_embedding_norm = time_embedding_norm
        self.add_downsample = add_downsample
        self.output_scale_factor = output_scale_factor
        if groups_out is None:
            groups_out = groups
        resnets = [ResidualTemporalBlock1D(in_channels, out_channels, embed_dim=temb_channels)]
        for _ in range(num_layers):
            resnets.append(ResidualTemporalBlock1D(out_channels, out_channels, embed_dim=temb_channels))
        self.resnets = nn.ModuleList(resnets)
        if non_linearity is None:
            self.nonlinearity = None
        else:
            self.nonlinearity = get_activation(non_linearity)
        self.downsample = None
        if add_downsample:
            self.downsample = Downsample1D(out_channels, use_conv=True, padding=1)

    def forward(self, hidden_states, temb=None):
        output_states = ()
        hidden_states = self.resnets[0](hidden_states, temb)
        for resnet in self.resnets[1:]:
            hidden_states = resnet(hidden_states, temb)
        output_states += hidden_states,
        if self.nonlinearity is not None:
            hidden_states = self.nonlinearity(hidden_states)
        if self.downsample is not None:
            hidden_states = self.downsample(hidden_states)
        return hidden_states, output_states


class UpResnetBlock1D(nn.Module):

    def __init__(self, in_channels, out_channels=None, num_layers=1, temb_channels=32, groups=32, groups_out=None, non_linearity=None, time_embedding_norm='default', output_scale_factor=1.0, add_upsample=True):
        super().__init__()
        self.in_channels = in_channels
        out_channels = in_channels if out_channels is None else out_channels
        self.out_channels = out_channels
        self.time_embedding_norm = time_embedding_norm
        self.add_upsample = add_upsample
        self.output_scale_factor = output_scale_factor
        if groups_out is None:
            groups_out = groups
        resnets = [ResidualTemporalBlock1D(2 * in_channels, out_channels, embed_dim=temb_channels)]
        for _ in range(num_layers):
            resnets.append(ResidualTemporalBlock1D(out_channels, out_channels, embed_dim=temb_channels))
        self.resnets = nn.ModuleList(resnets)
        if non_linearity is None:
            self.nonlinearity = None
        else:
            self.nonlinearity = get_activation(non_linearity)
        self.upsample = None
        if add_upsample:
            self.upsample = Upsample1D(out_channels, use_conv_transpose=True)

    def forward(self, hidden_states, res_hidden_states_tuple=None, temb=None):
        if res_hidden_states_tuple is not None:
            res_hidden_states = res_hidden_states_tuple[-1]
            hidden_states = torch.cat((hidden_states, res_hidden_states), dim=1)
        hidden_states = self.resnets[0](hidden_states, temb)
        for resnet in self.resnets[1:]:
            hidden_states = resnet(hidden_states, temb)
        if self.nonlinearity is not None:
            hidden_states = self.nonlinearity(hidden_states)
        if self.upsample is not None:
            hidden_states = self.upsample(hidden_states)
        return hidden_states


class ValueFunctionMidBlock1D(nn.Module):

    def __init__(self, in_channels, out_channels, embed_dim):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.embed_dim = embed_dim
        self.res1 = ResidualTemporalBlock1D(in_channels, in_channels // 2, embed_dim=embed_dim)
        self.down1 = Downsample1D(out_channels // 2, use_conv=True)
        self.res2 = ResidualTemporalBlock1D(in_channels // 2, in_channels // 4, embed_dim=embed_dim)
        self.down2 = Downsample1D(out_channels // 4, use_conv=True)

    def forward(self, x, temb=None):
        x = self.res1(x, temb)
        x = self.down1(x)
        x = self.res2(x, temb)
        x = self.down2(x)
        return x


class MidResTemporalBlock1D(nn.Module):

    def __init__(self, in_channels, out_channels, embed_dim, num_layers: 'int'=1, add_downsample: 'bool'=False, add_upsample: 'bool'=False, non_linearity=None):
        super().__init__()
        self.in_channels = in_channels
        self.out_channels = out_channels
        self.add_downsample = add_downsample
        resnets = [ResidualTemporalBlock1D(in_channels, out_channels, embed_dim=embed_dim)]
        for _ in range(num_layers):
            resnets.append(ResidualTemporalBlock1D(out_channels, out_channels, embed_dim=embed_dim))
        self.resnets = nn.ModuleList(resnets)
        if non_linearity is None:
            self.nonlinearity = None
        else:
            self.nonlinearity = get_activation(non_linearity)
        self.upsample = None
        if add_upsample:
            self.upsample = Downsample1D(out_channels, use_conv=True)
        self.downsample = None
        if add_downsample:
            self.downsample = Downsample1D(out_channels, use_conv=True)
        if self.upsample and self.downsample:
            raise ValueError('Block cannot downsample and upsample')

    def forward(self, hidden_states, temb):
        hidden_states = self.resnets[0](hidden_states, temb)
        for resnet in self.resnets[1:]:
            hidden_states = resnet(hidden_states, temb)
        if self.upsample:
            hidden_states = self.upsample(hidden_states)
        if self.downsample:
            self.downsample = self.downsample(hidden_states)
        return hidden_states


class OutConv1DBlock(nn.Module):

    def __init__(self, num_groups_out, out_channels, embed_dim, act_fn):
        super().__init__()
        self.final_conv1d_1 = nn.Conv1d(embed_dim, embed_dim, 5, padding=2)
        self.final_conv1d_gn = nn.GroupNorm(num_groups_out, embed_dim)
        self.final_conv1d_act = get_activation(act_fn)
        self.final_conv1d_2 = nn.Conv1d(embed_dim, out_channels, 1)

    def forward(self, hidden_states, temb=None):
        hidden_states = self.final_conv1d_1(hidden_states)
        hidden_states = rearrange_dims(hidden_states)
        hidden_states = self.final_conv1d_gn(hidden_states)
        hidden_states = rearrange_dims(hidden_states)
        hidden_states = self.final_conv1d_act(hidden_states)
        hidden_states = self.final_conv1d_2(hidden_states)
        return hidden_states


class OutValueFunctionBlock(nn.Module):

    def __init__(self, fc_dim, embed_dim):
        super().__init__()
        self.final_block = nn.ModuleList([nn.Linear(fc_dim + embed_dim, fc_dim // 2), nn.Mish(), nn.Linear(fc_dim // 2, 1)])

    def forward(self, hidden_states, temb):
        hidden_states = hidden_states.view(hidden_states.shape[0], -1)
        hidden_states = torch.cat((hidden_states, temb), dim=-1)
        for layer in self.final_block:
            hidden_states = layer(hidden_states)
        return hidden_states


_kernels = {'linear': [1 / 8, 3 / 8, 3 / 8, 1 / 8], 'cubic': [-0.01171875, -0.03515625, 0.11328125, 0.43359375, 0.43359375, 0.11328125, -0.03515625, -0.01171875], 'lanczos3': [0.003689131001010537, 0.015056144446134567, -0.03399861603975296, -0.066637322306633, 0.13550527393817902, 0.44638532400131226, 0.44638532400131226, 0.13550527393817902, -0.066637322306633, -0.03399861603975296, 0.015056144446134567, 0.003689131001010537]}


class Downsample1d(nn.Module):

    def __init__(self, kernel='linear', pad_mode='reflect'):
        super().__init__()
        self.pad_mode = pad_mode
        kernel_1d = torch.tensor(_kernels[kernel])
        self.pad = kernel_1d.shape[0] // 2 - 1
        self.register_buffer('kernel', kernel_1d)

    def forward(self, hidden_states):
        hidden_states = F.pad(hidden_states, (self.pad,) * 2, self.pad_mode)
        weight = hidden_states.new_zeros([hidden_states.shape[1], hidden_states.shape[1], self.kernel.shape[0]])
        indices = torch.arange(hidden_states.shape[1], device=hidden_states.device)
        kernel = self.kernel[None, :].expand(hidden_states.shape[1], -1)
        weight[indices, indices] = kernel
        return F.conv1d(hidden_states, weight, stride=2)


class Upsample1d(nn.Module):

    def __init__(self, kernel='linear', pad_mode='reflect'):
        super().__init__()
        self.pad_mode = pad_mode
        kernel_1d = torch.tensor(_kernels[kernel]) * 2
        self.pad = kernel_1d.shape[0] // 2 - 1
        self.register_buffer('kernel', kernel_1d)

    def forward(self, hidden_states, temb=None):
        hidden_states = F.pad(hidden_states, ((self.pad + 1) // 2,) * 2, self.pad_mode)
        weight = hidden_states.new_zeros([hidden_states.shape[1], hidden_states.shape[1], self.kernel.shape[0]])
        indices = torch.arange(hidden_states.shape[1], device=hidden_states.device)
        kernel = self.kernel[None, :].expand(hidden_states.shape[1], -1)
        weight[indices, indices] = kernel
        return F.conv_transpose1d(hidden_states, weight, stride=2, padding=self.pad * 2 + 1)


class SelfAttention1d(nn.Module):

    def __init__(self, in_channels, n_head=1, dropout_rate=0.0):
        super().__init__()
        self.channels = in_channels
        self.group_norm = nn.GroupNorm(1, num_channels=in_channels)
        self.num_heads = n_head
        self.query = nn.Linear(self.channels, self.channels)
        self.key = nn.Linear(self.channels, self.channels)
        self.value = nn.Linear(self.channels, self.channels)
        self.proj_attn = nn.Linear(self.channels, self.channels, bias=True)
        self.dropout = nn.Dropout(dropout_rate, inplace=True)

    def transpose_for_scores(self, projection: 'torch.Tensor') ->torch.Tensor:
        new_projection_shape = projection.size()[:-1] + (self.num_heads, -1)
        new_projection = projection.view(new_projection_shape).permute(0, 2, 1, 3)
        return new_projection

    def forward(self, hidden_states):
        residual = hidden_states
        batch, channel_dim, seq = hidden_states.shape
        hidden_states = self.group_norm(hidden_states)
        hidden_states = hidden_states.transpose(1, 2)
        query_proj = self.query(hidden_states)
        key_proj = self.key(hidden_states)
        value_proj = self.value(hidden_states)
        query_states = self.transpose_for_scores(query_proj)
        key_states = self.transpose_for_scores(key_proj)
        value_states = self.transpose_for_scores(value_proj)
        scale = 1 / math.sqrt(math.sqrt(key_states.shape[-1]))
        attention_scores = torch.matmul(query_states * scale, key_states.transpose(-1, -2) * scale)
        attention_probs = torch.softmax(attention_scores, dim=-1)
        hidden_states = torch.matmul(attention_probs, value_states)
        hidden_states = hidden_states.permute(0, 2, 1, 3).contiguous()
        new_hidden_states_shape = hidden_states.size()[:-2] + (self.channels,)
        hidden_states = hidden_states.view(new_hidden_states_shape)
        hidden_states = self.proj_attn(hidden_states)
        hidden_states = hidden_states.transpose(1, 2)
        hidden_states = self.dropout(hidden_states)
        output = hidden_states + residual
        return output


class ResConvBlock(nn.Module):

    def __init__(self, in_channels, mid_channels, out_channels, is_last=False):
        super().__init__()
        self.is_last = is_last
        self.has_conv_skip = in_channels != out_channels
        if self.has_conv_skip:
            self.conv_skip = nn.Conv1d(in_channels, out_channels, 1, bias=False)
        self.conv_1 = nn.Conv1d(in_channels, mid_channels, 5, padding=2)
        self.group_norm_1 = nn.GroupNorm(1, mid_channels)
        self.gelu_1 = nn.GELU()
        self.conv_2 = nn.Conv1d(mid_channels, out_channels, 5, padding=2)
        if not self.is_last:
            self.group_norm_2 = nn.GroupNorm(1, out_channels)
            self.gelu_2 = nn.GELU()

    def forward(self, hidden_states):
        residual = self.conv_skip(hidden_states) if self.has_conv_skip else hidden_states
        hidden_states = self.conv_1(hidden_states)
        hidden_states = self.group_norm_1(hidden_states)
        hidden_states = self.gelu_1(hidden_states)
        hidden_states = self.conv_2(hidden_states)
        if not self.is_last:
            hidden_states = self.group_norm_2(hidden_states)
            hidden_states = self.gelu_2(hidden_states)
        output = hidden_states + residual
        return output


class UNetMidBlock1D(nn.Module):

    def __init__(self, mid_channels, in_channels, out_channels=None):
        super().__init__()
        out_channels = in_channels if out_channels is None else out_channels
        self.down = Downsample1d('cubic')
        resnets = [ResConvBlock(in_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, out_channels)]
        attentions = [SelfAttention1d(mid_channels, mid_channels // 32), SelfAttention1d(mid_channels, mid_channels // 32), SelfAttention1d(mid_channels, mid_channels // 32), SelfAttention1d(mid_channels, mid_channels // 32), SelfAttention1d(mid_channels, mid_channels // 32), SelfAttention1d(out_channels, out_channels // 32)]
        self.up = Upsample1d(kernel='cubic')
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)

    def forward(self, hidden_states, temb=None):
        hidden_states = self.down(hidden_states)
        for attn, resnet in zip(self.attentions, self.resnets):
            hidden_states = resnet(hidden_states)
            hidden_states = attn(hidden_states)
        hidden_states = self.up(hidden_states)
        return hidden_states


class AttnDownBlock1D(nn.Module):

    def __init__(self, out_channels, in_channels, mid_channels=None):
        super().__init__()
        mid_channels = out_channels if mid_channels is None else mid_channels
        self.down = Downsample1d('cubic')
        resnets = [ResConvBlock(in_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, out_channels)]
        attentions = [SelfAttention1d(mid_channels, mid_channels // 32), SelfAttention1d(mid_channels, mid_channels // 32), SelfAttention1d(out_channels, out_channels // 32)]
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)

    def forward(self, hidden_states, temb=None):
        hidden_states = self.down(hidden_states)
        for resnet, attn in zip(self.resnets, self.attentions):
            hidden_states = resnet(hidden_states)
            hidden_states = attn(hidden_states)
        return hidden_states, (hidden_states,)


class DownBlock1D(nn.Module):

    def __init__(self, out_channels, in_channels, mid_channels=None):
        super().__init__()
        mid_channels = out_channels if mid_channels is None else mid_channels
        self.down = Downsample1d('cubic')
        resnets = [ResConvBlock(in_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, out_channels)]
        self.resnets = nn.ModuleList(resnets)

    def forward(self, hidden_states, temb=None):
        hidden_states = self.down(hidden_states)
        for resnet in self.resnets:
            hidden_states = resnet(hidden_states)
        return hidden_states, (hidden_states,)


class DownBlock1DNoSkip(nn.Module):

    def __init__(self, out_channels, in_channels, mid_channels=None):
        super().__init__()
        mid_channels = out_channels if mid_channels is None else mid_channels
        resnets = [ResConvBlock(in_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, out_channels)]
        self.resnets = nn.ModuleList(resnets)

    def forward(self, hidden_states, temb=None):
        hidden_states = torch.cat([hidden_states, temb], dim=1)
        for resnet in self.resnets:
            hidden_states = resnet(hidden_states)
        return hidden_states, (hidden_states,)


class AttnUpBlock1D(nn.Module):

    def __init__(self, in_channels, out_channels, mid_channels=None):
        super().__init__()
        mid_channels = out_channels if mid_channels is None else mid_channels
        resnets = [ResConvBlock(2 * in_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, out_channels)]
        attentions = [SelfAttention1d(mid_channels, mid_channels // 32), SelfAttention1d(mid_channels, mid_channels // 32), SelfAttention1d(out_channels, out_channels // 32)]
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        self.up = Upsample1d(kernel='cubic')

    def forward(self, hidden_states, res_hidden_states_tuple, temb=None):
        res_hidden_states = res_hidden_states_tuple[-1]
        hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
        for resnet, attn in zip(self.resnets, self.attentions):
            hidden_states = resnet(hidden_states)
            hidden_states = attn(hidden_states)
        hidden_states = self.up(hidden_states)
        return hidden_states


class UpBlock1D(nn.Module):

    def __init__(self, in_channels, out_channels, mid_channels=None):
        super().__init__()
        mid_channels = in_channels if mid_channels is None else mid_channels
        resnets = [ResConvBlock(2 * in_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, out_channels)]
        self.resnets = nn.ModuleList(resnets)
        self.up = Upsample1d(kernel='cubic')

    def forward(self, hidden_states, res_hidden_states_tuple, temb=None):
        res_hidden_states = res_hidden_states_tuple[-1]
        hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
        for resnet in self.resnets:
            hidden_states = resnet(hidden_states)
        hidden_states = self.up(hidden_states)
        return hidden_states


class UpBlock1DNoSkip(nn.Module):

    def __init__(self, in_channels, out_channels, mid_channels=None):
        super().__init__()
        mid_channels = in_channels if mid_channels is None else mid_channels
        resnets = [ResConvBlock(2 * in_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, mid_channels), ResConvBlock(mid_channels, mid_channels, out_channels, is_last=True)]
        self.resnets = nn.ModuleList(resnets)

    def forward(self, hidden_states, res_hidden_states_tuple, temb=None):
        res_hidden_states = res_hidden_states_tuple[-1]
        hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
        for resnet in self.resnets:
            hidden_states = resnet(hidden_states)
        return hidden_states


class UNet2DModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class UNetMidBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, add_attention: 'bool'=True, attn_num_head_channels=1, output_scale_factor=1.0):
        super().__init__()
        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)
        self.add_attention = add_attention
        resnets = [ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm)]
        attentions = []
        for _ in range(num_layers):
            if self.add_attention:
                attentions.append(Attention(in_channels, heads=in_channels // attn_num_head_channels if attn_num_head_channels is not None else 1, dim_head=attn_num_head_channels if attn_num_head_channels is not None else in_channels, rescale_output_factor=output_scale_factor, eps=resnet_eps, norm_num_groups=resnet_groups if resnet_time_scale_shift == 'default' else None, spatial_norm_dim=temb_channels if resnet_time_scale_shift == 'spatial' else None, residual_connection=True, bias=True, upcast_softmax=True, _from_deprecated_attn_block=True))
            else:
                attentions.append(None)
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)

    def forward(self, hidden_states, temb=None):
        hidden_states = self.resnets[0](hidden_states, temb)
        for attn, resnet in zip(self.attentions, self.resnets[1:]):
            if attn is not None:
                hidden_states = attn(hidden_states, temb=temb)
            hidden_states = resnet(hidden_states, temb)
        return hidden_states


class UNetMidBlock2DCrossAttn(nn.Module):

    def __init__(self, in_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attn_num_head_channels=1, output_scale_factor=1.0, cross_attention_dim=1280, dual_cross_attention=False, use_linear_projection=False, upcast_attention=False):
        super().__init__()
        self.has_cross_attention = True
        self.attn_num_head_channels = attn_num_head_channels
        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)
        resnets = [ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm)]
        attentions = []
        for _ in range(num_layers):
            if not dual_cross_attention:
                attentions.append(Transformer2DModel(attn_num_head_channels, in_channels // attn_num_head_channels, in_channels=in_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups, use_linear_projection=use_linear_projection, upcast_attention=upcast_attention))
            else:
                attentions.append(DualTransformer2DModel(attn_num_head_channels, in_channels // attn_num_head_channels, in_channels=in_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups))
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)

    def forward(self, hidden_states: 'torch.FloatTensor', temb: 'Optional[torch.FloatTensor]'=None, encoder_hidden_states: 'Optional[torch.FloatTensor]'=None, attention_mask: 'Optional[torch.FloatTensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[torch.FloatTensor]'=None) ->torch.FloatTensor:
        hidden_states = self.resnets[0](hidden_states, temb)
        for attn, resnet in zip(self.attentions, self.resnets[1:]):
            hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, attention_mask=attention_mask, encoder_attention_mask=encoder_attention_mask, return_dict=False)[0]
            hidden_states = resnet(hidden_states, temb)
        return hidden_states


class AttnAddedKVProcessor:
    """
    Processor for performing attention-related computations with extra learnable key and value matrices for the text
    encoder.
    """

    def __call__(self, attn: 'Attention', hidden_states, encoder_hidden_states=None, attention_mask=None):
        residual = hidden_states
        hidden_states = hidden_states.view(hidden_states.shape[0], hidden_states.shape[1], -1).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        query = attn.head_to_batch_dim(query)
        encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)
        encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)
        encoder_hidden_states_key_proj = attn.head_to_batch_dim(encoder_hidden_states_key_proj)
        encoder_hidden_states_value_proj = attn.head_to_batch_dim(encoder_hidden_states_value_proj)
        if not attn.only_cross_attention:
            key = attn.to_k(hidden_states)
            value = attn.to_v(hidden_states)
            key = attn.head_to_batch_dim(key)
            value = attn.head_to_batch_dim(value)
            key = torch.cat([encoder_hidden_states_key_proj, key], dim=1)
            value = torch.cat([encoder_hidden_states_value_proj, value], dim=1)
        else:
            key = encoder_hidden_states_key_proj
            value = encoder_hidden_states_value_proj
        attention_probs = attn.get_attention_scores(query, key, attention_mask)
        hidden_states = torch.bmm(attention_probs, value)
        hidden_states = attn.batch_to_head_dim(hidden_states)
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        hidden_states = hidden_states.transpose(-1, -2).reshape(residual.shape)
        hidden_states = hidden_states + residual
        return hidden_states


class AttnAddedKVProcessor2_0:
    """
    Processor for performing scaled dot-product attention (enabled by default if you're using PyTorch 2.0), with extra
    learnable key and value matrices for the text encoder.
    """

    def __init__(self):
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError('AttnAddedKVProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.')

    def __call__(self, attn: 'Attention', hidden_states, encoder_hidden_states=None, attention_mask=None):
        residual = hidden_states
        hidden_states = hidden_states.view(hidden_states.shape[0], hidden_states.shape[1], -1).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size, out_dim=4)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        query = attn.head_to_batch_dim(query, out_dim=4)
        encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)
        encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)
        encoder_hidden_states_key_proj = attn.head_to_batch_dim(encoder_hidden_states_key_proj, out_dim=4)
        encoder_hidden_states_value_proj = attn.head_to_batch_dim(encoder_hidden_states_value_proj, out_dim=4)
        if not attn.only_cross_attention:
            key = attn.to_k(hidden_states)
            value = attn.to_v(hidden_states)
            key = attn.head_to_batch_dim(key, out_dim=4)
            value = attn.head_to_batch_dim(value, out_dim=4)
            key = torch.cat([encoder_hidden_states_key_proj, key], dim=2)
            value = torch.cat([encoder_hidden_states_value_proj, value], dim=2)
        else:
            key = encoder_hidden_states_key_proj
            value = encoder_hidden_states_value_proj
        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)
        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, residual.shape[1])
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        hidden_states = hidden_states.transpose(-1, -2).reshape(residual.shape)
        hidden_states = hidden_states + residual
        return hidden_states


class UNetMidBlock2DSimpleCrossAttn(nn.Module):

    def __init__(self, in_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attn_num_head_channels=1, output_scale_factor=1.0, cross_attention_dim=1280, skip_time_act=False, only_cross_attention=False, cross_attention_norm=None):
        super().__init__()
        self.has_cross_attention = True
        self.attn_num_head_channels = attn_num_head_channels
        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)
        self.num_heads = in_channels // self.attn_num_head_channels
        resnets = [ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act)]
        attentions = []
        for _ in range(num_layers):
            processor = AttnAddedKVProcessor2_0() if hasattr(F, 'scaled_dot_product_attention') else AttnAddedKVProcessor()
            attentions.append(Attention(query_dim=in_channels, cross_attention_dim=in_channels, heads=self.num_heads, dim_head=attn_num_head_channels, added_kv_proj_dim=cross_attention_dim, norm_num_groups=resnet_groups, bias=True, upcast_softmax=True, only_cross_attention=only_cross_attention, cross_attention_norm=cross_attention_norm, processor=processor))
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)

    def forward(self, hidden_states: 'torch.FloatTensor', temb: 'Optional[torch.FloatTensor]'=None, encoder_hidden_states: 'Optional[torch.FloatTensor]'=None, attention_mask: 'Optional[torch.FloatTensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[torch.FloatTensor]'=None):
        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
        if attention_mask is None:
            mask = None if encoder_hidden_states is None else encoder_attention_mask
        else:
            mask = attention_mask
        hidden_states = self.resnets[0](hidden_states, temb)
        for attn, resnet in zip(self.attentions, self.resnets[1:]):
            hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, attention_mask=mask, **cross_attention_kwargs)
            hidden_states = resnet(hidden_states, temb)
        return hidden_states


class AttnDownBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attn_num_head_channels=1, output_scale_factor=1.0, downsample_padding=1, add_downsample=True):
        super().__init__()
        resnets = []
        attentions = []
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            attentions.append(Attention(out_channels, heads=out_channels // attn_num_head_channels if attn_num_head_channels is not None else 1, dim_head=attn_num_head_channels if attn_num_head_channels is not None else out_channels, rescale_output_factor=output_scale_factor, eps=resnet_eps, norm_num_groups=resnet_groups, residual_connection=True, bias=True, upcast_softmax=True, _from_deprecated_attn_block=True))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if add_downsample:
            self.downsamplers = nn.ModuleList([Downsample2D(out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name='op')])
        else:
            self.downsamplers = None

    def forward(self, hidden_states, temb=None, upsample_size=None):
        output_states = ()
        for resnet, attn in zip(self.resnets, self.attentions):
            hidden_states = resnet(hidden_states, temb)
            hidden_states = attn(hidden_states)
            output_states += hidden_states,
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
            output_states += hidden_states,
        return hidden_states, output_states


def is_torch_version(operation: 'str', version: 'str'):
    """
    Args:
    Compares the current PyTorch version to a given reference with an operation.
        operation (`str`):
            A string representation of an operator, such as `">"` or `"<="`
        version (`str`):
            A string version of PyTorch
    """
    return compare_versions(parse(_torch_version), operation, version)


class CrossAttnDownBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attn_num_head_channels=1, cross_attention_dim=1280, output_scale_factor=1.0, downsample_padding=1, add_downsample=True, dual_cross_attention=False, use_linear_projection=False, only_cross_attention=False, upcast_attention=False):
        super().__init__()
        resnets = []
        attentions = []
        self.has_cross_attention = True
        self.attn_num_head_channels = attn_num_head_channels
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            if not dual_cross_attention:
                attentions.append(Transformer2DModel(attn_num_head_channels, out_channels // attn_num_head_channels, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups, use_linear_projection=use_linear_projection, only_cross_attention=only_cross_attention, upcast_attention=upcast_attention))
            else:
                attentions.append(DualTransformer2DModel(attn_num_head_channels, out_channels // attn_num_head_channels, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if add_downsample:
            self.downsamplers = nn.ModuleList([Downsample2D(out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name='op')])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.FloatTensor', temb: 'Optional[torch.FloatTensor]'=None, encoder_hidden_states: 'Optional[torch.FloatTensor]'=None, attention_mask: 'Optional[torch.FloatTensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[torch.FloatTensor]'=None):
        output_states = ()
        for resnet, attn in zip(self.resnets, self.attentions):
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, **ckpt_kwargs)
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(attn, return_dict=False), hidden_states, encoder_hidden_states, None, None, cross_attention_kwargs, attention_mask, encoder_attention_mask, **ckpt_kwargs)[0]
            else:
                hidden_states = resnet(hidden_states, temb)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, attention_mask=attention_mask, encoder_attention_mask=encoder_attention_mask, return_dict=False)[0]
            output_states = output_states + (hidden_states,)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
            output_states = output_states + (hidden_states,)
        return hidden_states, output_states


class DownBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor=1.0, add_downsample=True, downsample_padding=1):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.resnets = nn.ModuleList(resnets)
        if add_downsample:
            self.downsamplers = nn.ModuleList([Downsample2D(out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name='op')])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states, temb=None):
        output_states = ()
        for resnet in self.resnets:
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs)
                    return custom_forward
                if is_torch_version('>=', '1.11.0'):
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, use_reentrant=False)
                else:
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
            else:
                hidden_states = resnet(hidden_states, temb)
            output_states = output_states + (hidden_states,)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
            output_states = output_states + (hidden_states,)
        return hidden_states, output_states


class DownEncoderBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor=1.0, add_downsample=True, downsample_padding=1):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=None, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.resnets = nn.ModuleList(resnets)
        if add_downsample:
            self.downsamplers = nn.ModuleList([Downsample2D(out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name='op')])
        else:
            self.downsamplers = None

    def forward(self, hidden_states):
        for resnet in self.resnets:
            hidden_states = resnet(hidden_states, temb=None)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
        return hidden_states


class AttnDownEncoderBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attn_num_head_channels=1, output_scale_factor=1.0, add_downsample=True, downsample_padding=1):
        super().__init__()
        resnets = []
        attentions = []
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=None, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            attentions.append(Attention(out_channels, heads=out_channels // attn_num_head_channels if attn_num_head_channels is not None else 1, dim_head=attn_num_head_channels if attn_num_head_channels is not None else out_channels, rescale_output_factor=output_scale_factor, eps=resnet_eps, norm_num_groups=resnet_groups, residual_connection=True, bias=True, upcast_softmax=True, _from_deprecated_attn_block=True))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if add_downsample:
            self.downsamplers = nn.ModuleList([Downsample2D(out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name='op')])
        else:
            self.downsamplers = None

    def forward(self, hidden_states):
        for resnet, attn in zip(self.resnets, self.attentions):
            hidden_states = resnet(hidden_states, temb=None)
            hidden_states = attn(hidden_states)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
        return hidden_states


class AttnSkipDownBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_pre_norm: 'bool'=True, attn_num_head_channels=1, output_scale_factor=np.sqrt(2.0), downsample_padding=1, add_downsample=True):
        super().__init__()
        self.attentions = nn.ModuleList([])
        self.resnets = nn.ModuleList([])
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            self.resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=min(in_channels // 4, 32), groups_out=min(out_channels // 4, 32), dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            self.attentions.append(Attention(out_channels, heads=out_channels // attn_num_head_channels if attn_num_head_channels is not None else 1, dim_head=attn_num_head_channels if attn_num_head_channels is not None else out_channels, rescale_output_factor=output_scale_factor, eps=resnet_eps, norm_num_groups=32, residual_connection=True, bias=True, upcast_softmax=True, _from_deprecated_attn_block=True))
        if add_downsample:
            self.resnet_down = ResnetBlock2D(in_channels=out_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=min(out_channels // 4, 32), dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, use_in_shortcut=True, down=True, kernel='fir')
            self.downsamplers = nn.ModuleList([FirDownsample2D(out_channels, out_channels=out_channels)])
            self.skip_conv = nn.Conv2d(3, out_channels, kernel_size=(1, 1), stride=(1, 1))
        else:
            self.resnet_down = None
            self.downsamplers = None
            self.skip_conv = None

    def forward(self, hidden_states, temb=None, skip_sample=None):
        output_states = ()
        for resnet, attn in zip(self.resnets, self.attentions):
            hidden_states = resnet(hidden_states, temb)
            hidden_states = attn(hidden_states)
            output_states += hidden_states,
        if self.downsamplers is not None:
            hidden_states = self.resnet_down(hidden_states, temb)
            for downsampler in self.downsamplers:
                skip_sample = downsampler(skip_sample)
            hidden_states = self.skip_conv(skip_sample) + hidden_states
            output_states += hidden_states,
        return hidden_states, output_states, skip_sample


class SkipDownBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_pre_norm: 'bool'=True, output_scale_factor=np.sqrt(2.0), add_downsample=True, downsample_padding=1):
        super().__init__()
        self.resnets = nn.ModuleList([])
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            self.resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=min(in_channels // 4, 32), groups_out=min(out_channels // 4, 32), dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        if add_downsample:
            self.resnet_down = ResnetBlock2D(in_channels=out_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=min(out_channels // 4, 32), dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, use_in_shortcut=True, down=True, kernel='fir')
            self.downsamplers = nn.ModuleList([FirDownsample2D(out_channels, out_channels=out_channels)])
            self.skip_conv = nn.Conv2d(3, out_channels, kernel_size=(1, 1), stride=(1, 1))
        else:
            self.resnet_down = None
            self.downsamplers = None
            self.skip_conv = None

    def forward(self, hidden_states, temb=None, skip_sample=None):
        output_states = ()
        for resnet in self.resnets:
            hidden_states = resnet(hidden_states, temb)
            output_states += hidden_states,
        if self.downsamplers is not None:
            hidden_states = self.resnet_down(hidden_states, temb)
            for downsampler in self.downsamplers:
                skip_sample = downsampler(skip_sample)
            hidden_states = self.skip_conv(skip_sample) + hidden_states
            output_states += hidden_states,
        return hidden_states, output_states, skip_sample


class ResnetDownsampleBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor=1.0, add_downsample=True, skip_time_act=False):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act))
        self.resnets = nn.ModuleList(resnets)
        if add_downsample:
            self.downsamplers = nn.ModuleList([ResnetBlock2D(in_channels=out_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act, down=True)])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states, temb=None):
        output_states = ()
        for resnet in self.resnets:
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs)
                    return custom_forward
                if is_torch_version('>=', '1.11.0'):
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, use_reentrant=False)
                else:
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
            else:
                hidden_states = resnet(hidden_states, temb)
            output_states = output_states + (hidden_states,)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states, temb)
            output_states = output_states + (hidden_states,)
        return hidden_states, output_states


class SimpleCrossAttnDownBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attn_num_head_channels=1, cross_attention_dim=1280, output_scale_factor=1.0, add_downsample=True, skip_time_act=False, only_cross_attention=False, cross_attention_norm=None):
        super().__init__()
        self.has_cross_attention = True
        resnets = []
        attentions = []
        self.attn_num_head_channels = attn_num_head_channels
        self.num_heads = out_channels // self.attn_num_head_channels
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act))
            processor = AttnAddedKVProcessor2_0() if hasattr(F, 'scaled_dot_product_attention') else AttnAddedKVProcessor()
            attentions.append(Attention(query_dim=out_channels, cross_attention_dim=out_channels, heads=self.num_heads, dim_head=attn_num_head_channels, added_kv_proj_dim=cross_attention_dim, norm_num_groups=resnet_groups, bias=True, upcast_softmax=True, only_cross_attention=only_cross_attention, cross_attention_norm=cross_attention_norm, processor=processor))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if add_downsample:
            self.downsamplers = nn.ModuleList([ResnetBlock2D(in_channels=out_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act, down=True)])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.FloatTensor', temb: 'Optional[torch.FloatTensor]'=None, encoder_hidden_states: 'Optional[torch.FloatTensor]'=None, attention_mask: 'Optional[torch.FloatTensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[torch.FloatTensor]'=None):
        output_states = ()
        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
        if attention_mask is None:
            mask = None if encoder_hidden_states is None else encoder_attention_mask
        else:
            mask = attention_mask
        for resnet, attn in zip(self.resnets, self.attentions):
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(attn, return_dict=False), hidden_states, encoder_hidden_states, mask, cross_attention_kwargs)[0]
            else:
                hidden_states = resnet(hidden_states, temb)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, attention_mask=mask, **cross_attention_kwargs)
            output_states = output_states + (hidden_states,)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states, temb)
            output_states = output_states + (hidden_states,)
        return hidden_states, output_states


class KDownBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=4, resnet_eps: 'float'=1e-05, resnet_act_fn: 'str'='gelu', resnet_group_size: 'int'=32, add_downsample=False):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            groups = in_channels // resnet_group_size
            groups_out = out_channels // resnet_group_size
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, dropout=dropout, temb_channels=temb_channels, groups=groups, groups_out=groups_out, eps=resnet_eps, non_linearity=resnet_act_fn, time_embedding_norm='ada_group', conv_shortcut_bias=False))
        self.resnets = nn.ModuleList(resnets)
        if add_downsample:
            self.downsamplers = nn.ModuleList([KDownsample2D()])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states, temb=None):
        output_states = ()
        for resnet in self.resnets:
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs)
                    return custom_forward
                if is_torch_version('>=', '1.11.0'):
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, use_reentrant=False)
                else:
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
            else:
                hidden_states = resnet(hidden_states, temb)
            output_states += hidden_states,
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
        return hidden_states, output_states


class KAttentionBlock(nn.Module):
    """
    A basic Transformer block.

    Parameters:
        dim (`int`): The number of channels in the input and output.
        num_attention_heads (`int`): The number of heads to use for multi-head attention.
        attention_head_dim (`int`): The number of channels in each head.
        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.
        cross_attention_dim (`int`, *optional*): The size of the encoder_hidden_states vector for cross attention.
        activation_fn (`str`, *optional*, defaults to `"geglu"`): Activation function to be used in feed-forward.
        num_embeds_ada_norm (:
            obj: `int`, *optional*): The number of diffusion steps used during training. See `Transformer2DModel`.
        attention_bias (:
            obj: `bool`, *optional*, defaults to `False`): Configure if the attentions should contain a bias parameter.
    """

    def __init__(self, dim: 'int', num_attention_heads: 'int', attention_head_dim: 'int', dropout: 'float'=0.0, cross_attention_dim: 'Optional[int]'=None, attention_bias: 'bool'=False, upcast_attention: 'bool'=False, temb_channels: 'int'=768, add_self_attention: 'bool'=False, cross_attention_norm: 'Optional[str]'=None, group_size: 'int'=32):
        super().__init__()
        self.add_self_attention = add_self_attention
        if add_self_attention:
            self.norm1 = AdaGroupNorm(temb_channels, dim, max(1, dim // group_size))
            self.attn1 = Attention(query_dim=dim, heads=num_attention_heads, dim_head=attention_head_dim, dropout=dropout, bias=attention_bias, cross_attention_dim=None, cross_attention_norm=None)
        self.norm2 = AdaGroupNorm(temb_channels, dim, max(1, dim // group_size))
        self.attn2 = Attention(query_dim=dim, cross_attention_dim=cross_attention_dim, heads=num_attention_heads, dim_head=attention_head_dim, dropout=dropout, bias=attention_bias, upcast_attention=upcast_attention, cross_attention_norm=cross_attention_norm)

    def _to_3d(self, hidden_states, height, weight):
        return hidden_states.permute(0, 2, 3, 1).reshape(hidden_states.shape[0], height * weight, -1)

    def _to_4d(self, hidden_states, height, weight):
        return hidden_states.permute(0, 2, 1).reshape(hidden_states.shape[0], -1, height, weight)

    def forward(self, hidden_states: 'torch.FloatTensor', encoder_hidden_states: 'Optional[torch.FloatTensor]'=None, emb: 'Optional[torch.FloatTensor]'=None, attention_mask: 'Optional[torch.FloatTensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[torch.FloatTensor]'=None):
        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
        if self.add_self_attention:
            norm_hidden_states = self.norm1(hidden_states, emb)
            height, weight = norm_hidden_states.shape[2:]
            norm_hidden_states = self._to_3d(norm_hidden_states, height, weight)
            attn_output = self.attn1(norm_hidden_states, encoder_hidden_states=None, attention_mask=attention_mask, **cross_attention_kwargs)
            attn_output = self._to_4d(attn_output, height, weight)
            hidden_states = attn_output + hidden_states
        norm_hidden_states = self.norm2(hidden_states, emb)
        height, weight = norm_hidden_states.shape[2:]
        norm_hidden_states = self._to_3d(norm_hidden_states, height, weight)
        attn_output = self.attn2(norm_hidden_states, encoder_hidden_states=encoder_hidden_states, attention_mask=attention_mask if encoder_hidden_states is None else encoder_attention_mask, **cross_attention_kwargs)
        attn_output = self._to_4d(attn_output, height, weight)
        hidden_states = attn_output + hidden_states
        return hidden_states


class KCrossAttnDownBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', cross_attention_dim: 'int', dropout: 'float'=0.0, num_layers: 'int'=4, resnet_group_size: 'int'=32, add_downsample=True, attn_num_head_channels: 'int'=64, add_self_attention: 'bool'=False, resnet_eps: 'float'=1e-05, resnet_act_fn: 'str'='gelu'):
        super().__init__()
        resnets = []
        attentions = []
        self.has_cross_attention = True
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            groups = in_channels // resnet_group_size
            groups_out = out_channels // resnet_group_size
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, dropout=dropout, temb_channels=temb_channels, groups=groups, groups_out=groups_out, eps=resnet_eps, non_linearity=resnet_act_fn, time_embedding_norm='ada_group', conv_shortcut_bias=False))
            attentions.append(KAttentionBlock(out_channels, out_channels // attn_num_head_channels, attn_num_head_channels, cross_attention_dim=cross_attention_dim, temb_channels=temb_channels, attention_bias=True, add_self_attention=add_self_attention, cross_attention_norm='layer_norm', group_size=resnet_group_size))
        self.resnets = nn.ModuleList(resnets)
        self.attentions = nn.ModuleList(attentions)
        if add_downsample:
            self.downsamplers = nn.ModuleList([KDownsample2D()])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.FloatTensor', temb: 'Optional[torch.FloatTensor]'=None, encoder_hidden_states: 'Optional[torch.FloatTensor]'=None, attention_mask: 'Optional[torch.FloatTensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[torch.FloatTensor]'=None):
        output_states = ()
        for resnet, attn in zip(self.resnets, self.attentions):
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, **ckpt_kwargs)
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(attn, return_dict=False), hidden_states, encoder_hidden_states, temb, attention_mask, cross_attention_kwargs, encoder_attention_mask, **ckpt_kwargs)
            else:
                hidden_states = resnet(hidden_states, temb)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, emb=temb, attention_mask=attention_mask, cross_attention_kwargs=cross_attention_kwargs, encoder_attention_mask=encoder_attention_mask)
            if self.downsamplers is None:
                output_states += None,
            else:
                output_states += hidden_states,
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
        return hidden_states, output_states


class AttnUpBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', prev_output_channel: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attn_num_head_channels=1, output_scale_factor=1.0, add_upsample=True):
        super().__init__()
        resnets = []
        attentions = []
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            attentions.append(Attention(out_channels, heads=out_channels // attn_num_head_channels if attn_num_head_channels is not None else 1, dim_head=attn_num_head_channels if attn_num_head_channels is not None else out_channels, rescale_output_factor=output_scale_factor, eps=resnet_eps, norm_num_groups=resnet_groups, residual_connection=True, bias=True, upcast_softmax=True, _from_deprecated_attn_block=True))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None

    def forward(self, hidden_states, res_hidden_states_tuple, temb=None, upsample_size=None):
        for resnet, attn in zip(self.resnets, self.attentions):
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            hidden_states = resnet(hidden_states, temb)
            hidden_states = attn(hidden_states)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states)
        return hidden_states


class CrossAttnUpBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', prev_output_channel: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attn_num_head_channels=1, cross_attention_dim=1280, output_scale_factor=1.0, add_upsample=True, dual_cross_attention=False, use_linear_projection=False, only_cross_attention=False, upcast_attention=False):
        super().__init__()
        resnets = []
        attentions = []
        self.has_cross_attention = True
        self.attn_num_head_channels = attn_num_head_channels
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            if not dual_cross_attention:
                attentions.append(Transformer2DModel(attn_num_head_channels, out_channels // attn_num_head_channels, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups, use_linear_projection=use_linear_projection, only_cross_attention=only_cross_attention, upcast_attention=upcast_attention))
            else:
                attentions.append(DualTransformer2DModel(attn_num_head_channels, out_channels // attn_num_head_channels, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.FloatTensor', res_hidden_states_tuple: 'Tuple[torch.FloatTensor, ...]', temb: 'Optional[torch.FloatTensor]'=None, encoder_hidden_states: 'Optional[torch.FloatTensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, upsample_size: 'Optional[int]'=None, attention_mask: 'Optional[torch.FloatTensor]'=None, encoder_attention_mask: 'Optional[torch.FloatTensor]'=None):
        for resnet, attn in zip(self.resnets, self.attentions):
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, **ckpt_kwargs)
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(attn, return_dict=False), hidden_states, encoder_hidden_states, None, None, cross_attention_kwargs, attention_mask, encoder_attention_mask, **ckpt_kwargs)[0]
            else:
                hidden_states = resnet(hidden_states, temb)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, attention_mask=attention_mask, encoder_attention_mask=encoder_attention_mask, return_dict=False)[0]
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states, upsample_size)
        return hidden_states


class UpBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', prev_output_channel: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor=1.0, add_upsample=True):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states, res_hidden_states_tuple, temb=None, upsample_size=None):
        for resnet in self.resnets:
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs)
                    return custom_forward
                if is_torch_version('>=', '1.11.0'):
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, use_reentrant=False)
                else:
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
            else:
                hidden_states = resnet(hidden_states, temb)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states, upsample_size)
        return hidden_states


class UpDecoderBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor=1.0, add_upsample=True, temb_channels=None):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            input_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=input_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None

    def forward(self, hidden_states, temb=None):
        for resnet in self.resnets:
            hidden_states = resnet(hidden_states, temb=temb)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states)
        return hidden_states


class AttnUpDecoderBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attn_num_head_channels=1, output_scale_factor=1.0, add_upsample=True, temb_channels=None):
        super().__init__()
        resnets = []
        attentions = []
        for i in range(num_layers):
            input_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=input_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            attentions.append(Attention(out_channels, heads=out_channels // attn_num_head_channels if attn_num_head_channels is not None else 1, dim_head=attn_num_head_channels if attn_num_head_channels is not None else out_channels, rescale_output_factor=output_scale_factor, eps=resnet_eps, norm_num_groups=resnet_groups if resnet_time_scale_shift != 'spatial' else None, spatial_norm_dim=temb_channels if resnet_time_scale_shift == 'spatial' else None, residual_connection=True, bias=True, upcast_softmax=True, _from_deprecated_attn_block=True))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None

    def forward(self, hidden_states, temb=None):
        for resnet, attn in zip(self.resnets, self.attentions):
            hidden_states = resnet(hidden_states, temb=temb)
            hidden_states = attn(hidden_states, temb=temb)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states)
        return hidden_states


class AttnSkipUpBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', prev_output_channel: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_pre_norm: 'bool'=True, attn_num_head_channels=1, output_scale_factor=np.sqrt(2.0), upsample_padding=1, add_upsample=True):
        super().__init__()
        self.attentions = nn.ModuleList([])
        self.resnets = nn.ModuleList([])
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            self.resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=min(resnet_in_channels + res_skip_channels // 4, 32), groups_out=min(out_channels // 4, 32), dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.attentions.append(Attention(out_channels, heads=out_channels // attn_num_head_channels if attn_num_head_channels is not None else 1, dim_head=attn_num_head_channels if attn_num_head_channels is not None else out_channels, rescale_output_factor=output_scale_factor, eps=resnet_eps, norm_num_groups=32, residual_connection=True, bias=True, upcast_softmax=True, _from_deprecated_attn_block=True))
        self.upsampler = FirUpsample2D(in_channels, out_channels=out_channels)
        if add_upsample:
            self.resnet_up = ResnetBlock2D(in_channels=out_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=min(out_channels // 4, 32), groups_out=min(out_channels // 4, 32), dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, use_in_shortcut=True, up=True, kernel='fir')
            self.skip_conv = nn.Conv2d(out_channels, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            self.skip_norm = torch.nn.GroupNorm(num_groups=min(out_channels // 4, 32), num_channels=out_channels, eps=resnet_eps, affine=True)
            self.act = nn.SiLU()
        else:
            self.resnet_up = None
            self.skip_conv = None
            self.skip_norm = None
            self.act = None

    def forward(self, hidden_states, res_hidden_states_tuple, temb=None, skip_sample=None):
        for resnet in self.resnets:
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            hidden_states = resnet(hidden_states, temb)
        hidden_states = self.attentions[0](hidden_states)
        if skip_sample is not None:
            skip_sample = self.upsampler(skip_sample)
        else:
            skip_sample = 0
        if self.resnet_up is not None:
            skip_sample_states = self.skip_norm(hidden_states)
            skip_sample_states = self.act(skip_sample_states)
            skip_sample_states = self.skip_conv(skip_sample_states)
            skip_sample = skip_sample + skip_sample_states
            hidden_states = self.resnet_up(hidden_states, temb)
        return hidden_states, skip_sample


class SkipUpBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', prev_output_channel: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_pre_norm: 'bool'=True, output_scale_factor=np.sqrt(2.0), add_upsample=True, upsample_padding=1):
        super().__init__()
        self.resnets = nn.ModuleList([])
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            self.resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=min((resnet_in_channels + res_skip_channels) // 4, 32), groups_out=min(out_channels // 4, 32), dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.upsampler = FirUpsample2D(in_channels, out_channels=out_channels)
        if add_upsample:
            self.resnet_up = ResnetBlock2D(in_channels=out_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=min(out_channels // 4, 32), groups_out=min(out_channels // 4, 32), dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, use_in_shortcut=True, up=True, kernel='fir')
            self.skip_conv = nn.Conv2d(out_channels, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
            self.skip_norm = torch.nn.GroupNorm(num_groups=min(out_channels // 4, 32), num_channels=out_channels, eps=resnet_eps, affine=True)
            self.act = nn.SiLU()
        else:
            self.resnet_up = None
            self.skip_conv = None
            self.skip_norm = None
            self.act = None

    def forward(self, hidden_states, res_hidden_states_tuple, temb=None, skip_sample=None):
        for resnet in self.resnets:
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            hidden_states = resnet(hidden_states, temb)
        if skip_sample is not None:
            skip_sample = self.upsampler(skip_sample)
        else:
            skip_sample = 0
        if self.resnet_up is not None:
            skip_sample_states = self.skip_norm(hidden_states)
            skip_sample_states = self.act(skip_sample_states)
            skip_sample_states = self.skip_conv(skip_sample_states)
            skip_sample = skip_sample + skip_sample_states
            hidden_states = self.resnet_up(hidden_states, temb)
        return hidden_states, skip_sample


class ResnetUpsampleBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', prev_output_channel: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor=1.0, add_upsample=True, skip_time_act=False):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act))
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([ResnetBlock2D(in_channels=out_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act, up=True)])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states, res_hidden_states_tuple, temb=None, upsample_size=None):
        for resnet in self.resnets:
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs)
                    return custom_forward
                if is_torch_version('>=', '1.11.0'):
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, use_reentrant=False)
                else:
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
            else:
                hidden_states = resnet(hidden_states, temb)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states, temb)
        return hidden_states


class SimpleCrossAttnUpBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', prev_output_channel: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attn_num_head_channels=1, cross_attention_dim=1280, output_scale_factor=1.0, add_upsample=True, skip_time_act=False, only_cross_attention=False, cross_attention_norm=None):
        super().__init__()
        resnets = []
        attentions = []
        self.has_cross_attention = True
        self.attn_num_head_channels = attn_num_head_channels
        self.num_heads = out_channels // self.attn_num_head_channels
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act))
            processor = AttnAddedKVProcessor2_0() if hasattr(F, 'scaled_dot_product_attention') else AttnAddedKVProcessor()
            attentions.append(Attention(query_dim=out_channels, cross_attention_dim=out_channels, heads=self.num_heads, dim_head=attn_num_head_channels, added_kv_proj_dim=cross_attention_dim, norm_num_groups=resnet_groups, bias=True, upcast_softmax=True, only_cross_attention=only_cross_attention, cross_attention_norm=cross_attention_norm, processor=processor))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([ResnetBlock2D(in_channels=out_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act, up=True)])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.FloatTensor', res_hidden_states_tuple: 'Tuple[torch.FloatTensor, ...]', temb: 'Optional[torch.FloatTensor]'=None, encoder_hidden_states: 'Optional[torch.FloatTensor]'=None, upsample_size: 'Optional[int]'=None, attention_mask: 'Optional[torch.FloatTensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[torch.FloatTensor]'=None):
        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
        if attention_mask is None:
            mask = None if encoder_hidden_states is None else encoder_attention_mask
        else:
            mask = attention_mask
        for resnet, attn in zip(self.resnets, self.attentions):
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(attn, return_dict=False), hidden_states, encoder_hidden_states, mask, cross_attention_kwargs)[0]
            else:
                hidden_states = resnet(hidden_states, temb)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, attention_mask=mask, **cross_attention_kwargs)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states, temb)
        return hidden_states


class KUpBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=5, resnet_eps: 'float'=1e-05, resnet_act_fn: 'str'='gelu', resnet_group_size: 'Optional[int]'=32, add_upsample=True):
        super().__init__()
        resnets = []
        k_in_channels = 2 * out_channels
        k_out_channels = in_channels
        num_layers = num_layers - 1
        for i in range(num_layers):
            in_channels = k_in_channels if i == 0 else out_channels
            groups = in_channels // resnet_group_size
            groups_out = out_channels // resnet_group_size
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=k_out_channels if i == num_layers - 1 else out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=groups, groups_out=groups_out, dropout=dropout, non_linearity=resnet_act_fn, time_embedding_norm='ada_group', conv_shortcut_bias=False))
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([KUpsample2D()])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states, res_hidden_states_tuple, temb=None, upsample_size=None):
        res_hidden_states_tuple = res_hidden_states_tuple[-1]
        if res_hidden_states_tuple is not None:
            hidden_states = torch.cat([hidden_states, res_hidden_states_tuple], dim=1)
        for resnet in self.resnets:
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs)
                    return custom_forward
                if is_torch_version('>=', '1.11.0'):
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, use_reentrant=False)
                else:
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
            else:
                hidden_states = resnet(hidden_states, temb)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states)
        return hidden_states


class KCrossAttnUpBlock2D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=4, resnet_eps: 'float'=1e-05, resnet_act_fn: 'str'='gelu', resnet_group_size: 'int'=32, attn_num_head_channels=1, cross_attention_dim: 'int'=768, add_upsample: 'bool'=True, upcast_attention: 'bool'=False):
        super().__init__()
        resnets = []
        attentions = []
        is_first_block = in_channels == out_channels == temb_channels
        is_middle_block = in_channels != out_channels
        add_self_attention = True if is_first_block else False
        self.has_cross_attention = True
        self.attn_num_head_channels = attn_num_head_channels
        k_in_channels = out_channels if is_first_block else 2 * out_channels
        k_out_channels = in_channels
        num_layers = num_layers - 1
        for i in range(num_layers):
            in_channels = k_in_channels if i == 0 else out_channels
            groups = in_channels // resnet_group_size
            groups_out = out_channels // resnet_group_size
            if is_middle_block and i == num_layers - 1:
                conv_2d_out_channels = k_out_channels
            else:
                conv_2d_out_channels = None
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, conv_2d_out_channels=conv_2d_out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=groups, groups_out=groups_out, dropout=dropout, non_linearity=resnet_act_fn, time_embedding_norm='ada_group', conv_shortcut_bias=False))
            attentions.append(KAttentionBlock(k_out_channels if i == num_layers - 1 else out_channels, k_out_channels // attn_num_head_channels if i == num_layers - 1 else out_channels // attn_num_head_channels, attn_num_head_channels, cross_attention_dim=cross_attention_dim, temb_channels=temb_channels, attention_bias=True, add_self_attention=add_self_attention, cross_attention_norm='layer_norm', upcast_attention=upcast_attention))
        self.resnets = nn.ModuleList(resnets)
        self.attentions = nn.ModuleList(attentions)
        if add_upsample:
            self.upsamplers = nn.ModuleList([KUpsample2D()])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.FloatTensor', res_hidden_states_tuple: 'Tuple[torch.FloatTensor, ...]', temb: 'Optional[torch.FloatTensor]'=None, encoder_hidden_states: 'Optional[torch.FloatTensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, upsample_size: 'Optional[int]'=None, attention_mask: 'Optional[torch.FloatTensor]'=None, encoder_attention_mask: 'Optional[torch.FloatTensor]'=None):
        res_hidden_states_tuple = res_hidden_states_tuple[-1]
        if res_hidden_states_tuple is not None:
            hidden_states = torch.cat([hidden_states, res_hidden_states_tuple], dim=1)
        for resnet, attn in zip(self.resnets, self.attentions):
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, **ckpt_kwargs)
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(attn, return_dict=False), hidden_states, encoder_hidden_states, temb, attention_mask, cross_attention_kwargs, encoder_attention_mask, **ckpt_kwargs)[0]
            else:
                hidden_states = resnet(hidden_states, temb)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, emb=temb, attention_mask=attention_mask, cross_attention_kwargs=cross_attention_kwargs, encoder_attention_mask=encoder_attention_mask)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states)
        return hidden_states


class UNet2DConditionModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class UNetMidBlock3DCrossAttn(nn.Module):

    def __init__(self, in_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attn_num_head_channels=1, output_scale_factor=1.0, cross_attention_dim=1280, dual_cross_attention=False, use_linear_projection=True, upcast_attention=False):
        super().__init__()
        self.has_cross_attention = True
        self.attn_num_head_channels = attn_num_head_channels
        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)
        resnets = [ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm)]
        temp_convs = [TemporalConvLayer(in_channels, in_channels, dropout=0.1)]
        attentions = []
        temp_attentions = []
        for _ in range(num_layers):
            attentions.append(Transformer2DModel(in_channels // attn_num_head_channels, attn_num_head_channels, in_channels=in_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups, use_linear_projection=use_linear_projection, upcast_attention=upcast_attention))
            temp_attentions.append(TransformerTemporalModel(in_channels // attn_num_head_channels, attn_num_head_channels, in_channels=in_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups))
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            temp_convs.append(TemporalConvLayer(in_channels, in_channels, dropout=0.1))
        self.resnets = nn.ModuleList(resnets)
        self.temp_convs = nn.ModuleList(temp_convs)
        self.attentions = nn.ModuleList(attentions)
        self.temp_attentions = nn.ModuleList(temp_attentions)

    def forward(self, hidden_states, temb=None, encoder_hidden_states=None, attention_mask=None, num_frames=1, cross_attention_kwargs=None):
        hidden_states = self.resnets[0](hidden_states, temb)
        hidden_states = self.temp_convs[0](hidden_states, num_frames=num_frames)
        for attn, temp_attn, resnet, temp_conv in zip(self.attentions, self.temp_attentions, self.resnets[1:], self.temp_convs[1:]):
            hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs).sample
            hidden_states = temp_attn(hidden_states, num_frames=num_frames, cross_attention_kwargs=cross_attention_kwargs).sample
            hidden_states = resnet(hidden_states, temb)
            hidden_states = temp_conv(hidden_states, num_frames=num_frames)
        return hidden_states


class CrossAttnDownBlock3D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attn_num_head_channels=1, cross_attention_dim=1280, output_scale_factor=1.0, downsample_padding=1, add_downsample=True, dual_cross_attention=False, use_linear_projection=False, only_cross_attention=False, upcast_attention=False):
        super().__init__()
        resnets = []
        attentions = []
        temp_attentions = []
        temp_convs = []
        self.has_cross_attention = True
        self.attn_num_head_channels = attn_num_head_channels
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            temp_convs.append(TemporalConvLayer(out_channels, out_channels, dropout=0.1))
            attentions.append(Transformer2DModel(out_channels // attn_num_head_channels, attn_num_head_channels, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups, use_linear_projection=use_linear_projection, only_cross_attention=only_cross_attention, upcast_attention=upcast_attention))
            temp_attentions.append(TransformerTemporalModel(out_channels // attn_num_head_channels, attn_num_head_channels, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups))
        self.resnets = nn.ModuleList(resnets)
        self.temp_convs = nn.ModuleList(temp_convs)
        self.attentions = nn.ModuleList(attentions)
        self.temp_attentions = nn.ModuleList(temp_attentions)
        if add_downsample:
            self.downsamplers = nn.ModuleList([Downsample2D(out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name='op')])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states, temb=None, encoder_hidden_states=None, attention_mask=None, num_frames=1, cross_attention_kwargs=None):
        output_states = ()
        for resnet, temp_conv, attn, temp_attn in zip(self.resnets, self.temp_convs, self.attentions, self.temp_attentions):
            hidden_states = resnet(hidden_states, temb)
            hidden_states = temp_conv(hidden_states, num_frames=num_frames)
            hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs).sample
            hidden_states = temp_attn(hidden_states, num_frames=num_frames, cross_attention_kwargs=cross_attention_kwargs).sample
            output_states += hidden_states,
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
            output_states += hidden_states,
        return hidden_states, output_states


class DownBlock3D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor=1.0, add_downsample=True, downsample_padding=1):
        super().__init__()
        resnets = []
        temp_convs = []
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            temp_convs.append(TemporalConvLayer(out_channels, out_channels, dropout=0.1))
        self.resnets = nn.ModuleList(resnets)
        self.temp_convs = nn.ModuleList(temp_convs)
        if add_downsample:
            self.downsamplers = nn.ModuleList([Downsample2D(out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name='op')])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states, temb=None, num_frames=1):
        output_states = ()
        for resnet, temp_conv in zip(self.resnets, self.temp_convs):
            hidden_states = resnet(hidden_states, temb)
            hidden_states = temp_conv(hidden_states, num_frames=num_frames)
            output_states += hidden_states,
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
            output_states += hidden_states,
        return hidden_states, output_states


class CrossAttnUpBlock3D(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', prev_output_channel: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attn_num_head_channels=1, cross_attention_dim=1280, output_scale_factor=1.0, add_upsample=True, dual_cross_attention=False, use_linear_projection=False, only_cross_attention=False, upcast_attention=False):
        super().__init__()
        resnets = []
        temp_convs = []
        attentions = []
        temp_attentions = []
        self.has_cross_attention = True
        self.attn_num_head_channels = attn_num_head_channels
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            temp_convs.append(TemporalConvLayer(out_channels, out_channels, dropout=0.1))
            attentions.append(Transformer2DModel(out_channels // attn_num_head_channels, attn_num_head_channels, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups, use_linear_projection=use_linear_projection, only_cross_attention=only_cross_attention, upcast_attention=upcast_attention))
            temp_attentions.append(TransformerTemporalModel(out_channels // attn_num_head_channels, attn_num_head_channels, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups))
        self.resnets = nn.ModuleList(resnets)
        self.temp_convs = nn.ModuleList(temp_convs)
        self.attentions = nn.ModuleList(attentions)
        self.temp_attentions = nn.ModuleList(temp_attentions)
        if add_upsample:
            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states, res_hidden_states_tuple, temb=None, encoder_hidden_states=None, upsample_size=None, attention_mask=None, num_frames=1, cross_attention_kwargs=None):
        for resnet, temp_conv, attn, temp_attn in zip(self.resnets, self.temp_convs, self.attentions, self.temp_attentions):
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            hidden_states = resnet(hidden_states, temb)
            hidden_states = temp_conv(hidden_states, num_frames=num_frames)
            hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs).sample
            hidden_states = temp_attn(hidden_states, num_frames=num_frames, cross_attention_kwargs=cross_attention_kwargs).sample
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states, upsample_size)
        return hidden_states


class UpBlock3D(nn.Module):

    def __init__(self, in_channels: 'int', prev_output_channel: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor=1.0, add_upsample=True):
        super().__init__()
        resnets = []
        temp_convs = []
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(ResnetBlock2D(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            temp_convs.append(TemporalConvLayer(out_channels, out_channels, dropout=0.1))
        self.resnets = nn.ModuleList(resnets)
        self.temp_convs = nn.ModuleList(temp_convs)
        if add_upsample:
            self.upsamplers = nn.ModuleList([Upsample2D(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states, res_hidden_states_tuple, temb=None, upsample_size=None, num_frames=1):
        for resnet, temp_conv in zip(self.resnets, self.temp_convs):
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            hidden_states = resnet(hidden_states, temb)
            hidden_states = temp_conv(hidden_states, num_frames=num_frames)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states, upsample_size)
        return hidden_states


class UNet3DConditionModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class LinearMultiDim(nn.Linear):

    def __init__(self, in_features, out_features=None, second_dim=4, *args, **kwargs):
        in_features = [in_features, second_dim, 1] if isinstance(in_features, int) else list(in_features)
        if out_features is None:
            out_features = in_features
        out_features = [out_features, second_dim, 1] if isinstance(out_features, int) else list(out_features)
        self.in_features_multidim = in_features
        self.out_features_multidim = out_features
        super().__init__(np.array(in_features).prod(), np.array(out_features).prod())

    def forward(self, input_tensor, *args, **kwargs):
        shape = input_tensor.shape
        n_dim = len(self.in_features_multidim)
        input_tensor = input_tensor.reshape(*shape[0:-n_dim], self.in_features)
        output_tensor = super().forward(input_tensor)
        output_tensor = output_tensor.view(*shape[0:-n_dim], *self.out_features_multidim)
        return output_tensor


class ResnetBlockFlat(nn.Module):

    def __init__(self, *, in_channels, out_channels=None, dropout=0.0, temb_channels=512, groups=32, groups_out=None, pre_norm=True, eps=1e-06, time_embedding_norm='default', use_in_shortcut=None, second_dim=4, **kwargs):
        super().__init__()
        self.pre_norm = pre_norm
        self.pre_norm = True
        in_channels = [in_channels, second_dim, 1] if isinstance(in_channels, int) else list(in_channels)
        self.in_channels_prod = np.array(in_channels).prod()
        self.channels_multidim = in_channels
        if out_channels is not None:
            out_channels = [out_channels, second_dim, 1] if isinstance(out_channels, int) else list(out_channels)
            out_channels_prod = np.array(out_channels).prod()
            self.out_channels_multidim = out_channels
        else:
            out_channels_prod = self.in_channels_prod
            self.out_channels_multidim = self.channels_multidim
        self.time_embedding_norm = time_embedding_norm
        if groups_out is None:
            groups_out = groups
        self.norm1 = torch.nn.GroupNorm(num_groups=groups, num_channels=self.in_channels_prod, eps=eps, affine=True)
        self.conv1 = torch.nn.Conv2d(self.in_channels_prod, out_channels_prod, kernel_size=1, padding=0)
        if temb_channels is not None:
            self.time_emb_proj = torch.nn.Linear(temb_channels, out_channels_prod)
        else:
            self.time_emb_proj = None
        self.norm2 = torch.nn.GroupNorm(num_groups=groups_out, num_channels=out_channels_prod, eps=eps, affine=True)
        self.dropout = torch.nn.Dropout(dropout)
        self.conv2 = torch.nn.Conv2d(out_channels_prod, out_channels_prod, kernel_size=1, padding=0)
        self.nonlinearity = nn.SiLU()
        self.use_in_shortcut = self.in_channels_prod != out_channels_prod if use_in_shortcut is None else use_in_shortcut
        self.conv_shortcut = None
        if self.use_in_shortcut:
            self.conv_shortcut = torch.nn.Conv2d(self.in_channels_prod, out_channels_prod, kernel_size=1, stride=1, padding=0)

    def forward(self, input_tensor, temb):
        shape = input_tensor.shape
        n_dim = len(self.channels_multidim)
        input_tensor = input_tensor.reshape(*shape[0:-n_dim], self.in_channels_prod, 1, 1)
        input_tensor = input_tensor.view(-1, self.in_channels_prod, 1, 1)
        hidden_states = input_tensor
        hidden_states = self.norm1(hidden_states)
        hidden_states = self.nonlinearity(hidden_states)
        hidden_states = self.conv1(hidden_states)
        if temb is not None:
            temb = self.time_emb_proj(self.nonlinearity(temb))[:, :, None, None]
            hidden_states = hidden_states + temb
        hidden_states = self.norm2(hidden_states)
        hidden_states = self.nonlinearity(hidden_states)
        hidden_states = self.dropout(hidden_states)
        hidden_states = self.conv2(hidden_states)
        if self.conv_shortcut is not None:
            input_tensor = self.conv_shortcut(input_tensor)
        output_tensor = input_tensor + hidden_states
        output_tensor = output_tensor.view(*shape[0:-n_dim], -1)
        output_tensor = output_tensor.view(*shape[0:-n_dim], *self.out_channels_multidim)
        return output_tensor


class CrossAttnDownBlockFlat(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attn_num_head_channels=1, cross_attention_dim=1280, output_scale_factor=1.0, downsample_padding=1, add_downsample=True, dual_cross_attention=False, use_linear_projection=False, only_cross_attention=False, upcast_attention=False):
        super().__init__()
        resnets = []
        attentions = []
        self.has_cross_attention = True
        self.attn_num_head_channels = attn_num_head_channels
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlockFlat(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            if not dual_cross_attention:
                attentions.append(Transformer2DModel(attn_num_head_channels, out_channels // attn_num_head_channels, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups, use_linear_projection=use_linear_projection, only_cross_attention=only_cross_attention, upcast_attention=upcast_attention))
            else:
                attentions.append(DualTransformer2DModel(attn_num_head_channels, out_channels // attn_num_head_channels, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if add_downsample:
            self.downsamplers = nn.ModuleList([LinearMultiDim(out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name='op')])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.FloatTensor', temb: 'Optional[torch.FloatTensor]'=None, encoder_hidden_states: 'Optional[torch.FloatTensor]'=None, attention_mask: 'Optional[torch.FloatTensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[torch.FloatTensor]'=None):
        output_states = ()
        for resnet, attn in zip(self.resnets, self.attentions):
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, **ckpt_kwargs)
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(attn, return_dict=False), hidden_states, encoder_hidden_states, None, None, cross_attention_kwargs, attention_mask, encoder_attention_mask, **ckpt_kwargs)[0]
            else:
                hidden_states = resnet(hidden_states, temb)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, attention_mask=attention_mask, encoder_attention_mask=encoder_attention_mask, return_dict=False)[0]
            output_states = output_states + (hidden_states,)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
            output_states = output_states + (hidden_states,)
        return hidden_states, output_states


class DownBlockFlat(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor=1.0, add_downsample=True, downsample_padding=1):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            in_channels = in_channels if i == 0 else out_channels
            resnets.append(ResnetBlockFlat(in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.resnets = nn.ModuleList(resnets)
        if add_downsample:
            self.downsamplers = nn.ModuleList([LinearMultiDim(out_channels, use_conv=True, out_channels=out_channels, padding=downsample_padding, name='op')])
        else:
            self.downsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states, temb=None):
        output_states = ()
        for resnet in self.resnets:
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs)
                    return custom_forward
                if is_torch_version('>=', '1.11.0'):
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, use_reentrant=False)
                else:
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
            else:
                hidden_states = resnet(hidden_states, temb)
            output_states = output_states + (hidden_states,)
        if self.downsamplers is not None:
            for downsampler in self.downsamplers:
                hidden_states = downsampler(hidden_states)
            output_states = output_states + (hidden_states,)
        return hidden_states, output_states


def get_down_block(down_block_type, num_layers, in_channels, out_channels, temb_channels, add_downsample, resnet_eps, resnet_act_fn, attn_num_head_channels, resnet_groups=None, cross_attention_dim=None, downsample_padding=None, dual_cross_attention=False, use_linear_projection=False, only_cross_attention=False, upcast_attention=False, resnet_time_scale_shift='default', resnet_skip_time_act=False, resnet_out_scale_factor=1.0, cross_attention_norm=None):
    down_block_type = down_block_type[7:] if down_block_type.startswith('UNetRes') else down_block_type
    if down_block_type == 'DownBlockFlat':
        return DownBlockFlat(num_layers=num_layers, in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, add_downsample=add_downsample, resnet_eps=resnet_eps, resnet_act_fn=resnet_act_fn, resnet_groups=resnet_groups, downsample_padding=downsample_padding, resnet_time_scale_shift=resnet_time_scale_shift)
    elif down_block_type == 'CrossAttnDownBlockFlat':
        if cross_attention_dim is None:
            raise ValueError('cross_attention_dim must be specified for CrossAttnDownBlockFlat')
        return CrossAttnDownBlockFlat(num_layers=num_layers, in_channels=in_channels, out_channels=out_channels, temb_channels=temb_channels, add_downsample=add_downsample, resnet_eps=resnet_eps, resnet_act_fn=resnet_act_fn, resnet_groups=resnet_groups, downsample_padding=downsample_padding, cross_attention_dim=cross_attention_dim, attn_num_head_channels=attn_num_head_channels, dual_cross_attention=dual_cross_attention, use_linear_projection=use_linear_projection, only_cross_attention=only_cross_attention, resnet_time_scale_shift=resnet_time_scale_shift)
    raise ValueError(f'{down_block_type} is not supported.')


class Encoder(nn.Module):

    def __init__(self, in_channels=3, out_channels=3, down_block_types=('DownEncoderBlock2D',), block_out_channels=(64,), layers_per_block=2, norm_num_groups=32, act_fn='silu', double_z=True):
        super().__init__()
        self.layers_per_block = layers_per_block
        self.conv_in = torch.nn.Conv2d(in_channels, block_out_channels[0], kernel_size=3, stride=1, padding=1)
        self.mid_block = None
        self.down_blocks = nn.ModuleList([])
        output_channel = block_out_channels[0]
        for i, down_block_type in enumerate(down_block_types):
            input_channel = output_channel
            output_channel = block_out_channels[i]
            is_final_block = i == len(block_out_channels) - 1
            down_block = get_down_block(down_block_type, num_layers=self.layers_per_block, in_channels=input_channel, out_channels=output_channel, add_downsample=not is_final_block, resnet_eps=1e-06, downsample_padding=0, resnet_act_fn=act_fn, resnet_groups=norm_num_groups, attn_num_head_channels=None, temb_channels=None)
            self.down_blocks.append(down_block)
        self.mid_block = UNetMidBlock2D(in_channels=block_out_channels[-1], resnet_eps=1e-06, resnet_act_fn=act_fn, output_scale_factor=1, resnet_time_scale_shift='default', attn_num_head_channels=None, resnet_groups=norm_num_groups, temb_channels=None)
        self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[-1], num_groups=norm_num_groups, eps=1e-06)
        self.conv_act = nn.SiLU()
        conv_out_channels = 2 * out_channels if double_z else out_channels
        self.conv_out = nn.Conv2d(block_out_channels[-1], conv_out_channels, 3, padding=1)
        self.gradient_checkpointing = False

    def forward(self, x):
        sample = x
        sample = self.conv_in(sample)
        if self.training and self.gradient_checkpointing:

            def create_custom_forward(module):

                def custom_forward(*inputs):
                    return module(*inputs)
                return custom_forward
            if is_torch_version('>=', '1.11.0'):
                for down_block in self.down_blocks:
                    sample = torch.utils.checkpoint.checkpoint(create_custom_forward(down_block), sample, use_reentrant=False)
                sample = torch.utils.checkpoint.checkpoint(create_custom_forward(self.mid_block), sample, use_reentrant=False)
            else:
                for down_block in self.down_blocks:
                    sample = torch.utils.checkpoint.checkpoint(create_custom_forward(down_block), sample)
                sample = torch.utils.checkpoint.checkpoint(create_custom_forward(self.mid_block), sample)
        else:
            for down_block in self.down_blocks:
                sample = down_block(sample)
            sample = self.mid_block(sample)
        sample = self.conv_norm_out(sample)
        sample = self.conv_act(sample)
        sample = self.conv_out(sample)
        return sample


class CrossAttnUpBlockFlat(nn.Module):

    def __init__(self, in_channels: 'int', out_channels: 'int', prev_output_channel: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attn_num_head_channels=1, cross_attention_dim=1280, output_scale_factor=1.0, add_upsample=True, dual_cross_attention=False, use_linear_projection=False, only_cross_attention=False, upcast_attention=False):
        super().__init__()
        resnets = []
        attentions = []
        self.has_cross_attention = True
        self.attn_num_head_channels = attn_num_head_channels
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(ResnetBlockFlat(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
            if not dual_cross_attention:
                attentions.append(Transformer2DModel(attn_num_head_channels, out_channels // attn_num_head_channels, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups, use_linear_projection=use_linear_projection, only_cross_attention=only_cross_attention, upcast_attention=upcast_attention))
            else:
                attentions.append(DualTransformer2DModel(attn_num_head_channels, out_channels // attn_num_head_channels, in_channels=out_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([LinearMultiDim(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states: 'torch.FloatTensor', res_hidden_states_tuple: 'Tuple[torch.FloatTensor, ...]', temb: 'Optional[torch.FloatTensor]'=None, encoder_hidden_states: 'Optional[torch.FloatTensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, upsample_size: 'Optional[int]'=None, attention_mask: 'Optional[torch.FloatTensor]'=None, encoder_attention_mask: 'Optional[torch.FloatTensor]'=None):
        for resnet, attn in zip(self.resnets, self.attentions):
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module, return_dict=None):

                    def custom_forward(*inputs):
                        if return_dict is not None:
                            return module(*inputs, return_dict=return_dict)
                        else:
                            return module(*inputs)
                    return custom_forward
                ckpt_kwargs: 'Dict[str, Any]' = {'use_reentrant': False} if is_torch_version('>=', '1.11.0') else {}
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, **ckpt_kwargs)
                hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(attn, return_dict=False), hidden_states, encoder_hidden_states, None, None, cross_attention_kwargs, attention_mask, encoder_attention_mask, **ckpt_kwargs)[0]
            else:
                hidden_states = resnet(hidden_states, temb)
                hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, attention_mask=attention_mask, encoder_attention_mask=encoder_attention_mask, return_dict=False)[0]
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states, upsample_size)
        return hidden_states


class UpBlockFlat(nn.Module):

    def __init__(self, in_channels: 'int', prev_output_channel: 'int', out_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, output_scale_factor=1.0, add_upsample=True):
        super().__init__()
        resnets = []
        for i in range(num_layers):
            res_skip_channels = in_channels if i == num_layers - 1 else out_channels
            resnet_in_channels = prev_output_channel if i == 0 else out_channels
            resnets.append(ResnetBlockFlat(in_channels=resnet_in_channels + res_skip_channels, out_channels=out_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.resnets = nn.ModuleList(resnets)
        if add_upsample:
            self.upsamplers = nn.ModuleList([LinearMultiDim(out_channels, use_conv=True, out_channels=out_channels)])
        else:
            self.upsamplers = None
        self.gradient_checkpointing = False

    def forward(self, hidden_states, res_hidden_states_tuple, temb=None, upsample_size=None):
        for resnet in self.resnets:
            res_hidden_states = res_hidden_states_tuple[-1]
            res_hidden_states_tuple = res_hidden_states_tuple[:-1]
            hidden_states = torch.cat([hidden_states, res_hidden_states], dim=1)
            if self.training and self.gradient_checkpointing:

                def create_custom_forward(module):

                    def custom_forward(*inputs):
                        return module(*inputs)
                    return custom_forward
                if is_torch_version('>=', '1.11.0'):
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb, use_reentrant=False)
                else:
                    hidden_states = torch.utils.checkpoint.checkpoint(create_custom_forward(resnet), hidden_states, temb)
            else:
                hidden_states = resnet(hidden_states, temb)
        if self.upsamplers is not None:
            for upsampler in self.upsamplers:
                hidden_states = upsampler(hidden_states, upsample_size)
        return hidden_states


def get_up_block(up_block_type, num_layers, in_channels, out_channels, prev_output_channel, temb_channels, add_upsample, resnet_eps, resnet_act_fn, attn_num_head_channels, resnet_groups=None, cross_attention_dim=None, dual_cross_attention=False, use_linear_projection=False, only_cross_attention=False, upcast_attention=False, resnet_time_scale_shift='default', resnet_skip_time_act=False, resnet_out_scale_factor=1.0, cross_attention_norm=None):
    up_block_type = up_block_type[7:] if up_block_type.startswith('UNetRes') else up_block_type
    if up_block_type == 'UpBlockFlat':
        return UpBlockFlat(num_layers=num_layers, in_channels=in_channels, out_channels=out_channels, prev_output_channel=prev_output_channel, temb_channels=temb_channels, add_upsample=add_upsample, resnet_eps=resnet_eps, resnet_act_fn=resnet_act_fn, resnet_groups=resnet_groups, resnet_time_scale_shift=resnet_time_scale_shift)
    elif up_block_type == 'CrossAttnUpBlockFlat':
        if cross_attention_dim is None:
            raise ValueError('cross_attention_dim must be specified for CrossAttnUpBlockFlat')
        return CrossAttnUpBlockFlat(num_layers=num_layers, in_channels=in_channels, out_channels=out_channels, prev_output_channel=prev_output_channel, temb_channels=temb_channels, add_upsample=add_upsample, resnet_eps=resnet_eps, resnet_act_fn=resnet_act_fn, resnet_groups=resnet_groups, cross_attention_dim=cross_attention_dim, attn_num_head_channels=attn_num_head_channels, dual_cross_attention=dual_cross_attention, use_linear_projection=use_linear_projection, only_cross_attention=only_cross_attention, resnet_time_scale_shift=resnet_time_scale_shift)
    raise ValueError(f'{up_block_type} is not supported.')


class Decoder(nn.Module):

    def __init__(self, in_channels=3, out_channels=3, up_block_types=('UpDecoderBlock2D',), block_out_channels=(64,), layers_per_block=2, norm_num_groups=32, act_fn='silu', norm_type='group'):
        super().__init__()
        self.layers_per_block = layers_per_block
        self.conv_in = nn.Conv2d(in_channels, block_out_channels[-1], kernel_size=3, stride=1, padding=1)
        self.mid_block = None
        self.up_blocks = nn.ModuleList([])
        temb_channels = in_channels if norm_type == 'spatial' else None
        self.mid_block = UNetMidBlock2D(in_channels=block_out_channels[-1], resnet_eps=1e-06, resnet_act_fn=act_fn, output_scale_factor=1, resnet_time_scale_shift='default' if norm_type == 'group' else norm_type, attn_num_head_channels=None, resnet_groups=norm_num_groups, temb_channels=temb_channels)
        reversed_block_out_channels = list(reversed(block_out_channels))
        output_channel = reversed_block_out_channels[0]
        for i, up_block_type in enumerate(up_block_types):
            prev_output_channel = output_channel
            output_channel = reversed_block_out_channels[i]
            is_final_block = i == len(block_out_channels) - 1
            up_block = get_up_block(up_block_type, num_layers=self.layers_per_block + 1, in_channels=prev_output_channel, out_channels=output_channel, prev_output_channel=None, add_upsample=not is_final_block, resnet_eps=1e-06, resnet_act_fn=act_fn, resnet_groups=norm_num_groups, attn_num_head_channels=None, temb_channels=temb_channels, resnet_time_scale_shift=norm_type)
            self.up_blocks.append(up_block)
            prev_output_channel = output_channel
        if norm_type == 'spatial':
            self.conv_norm_out = SpatialNorm(block_out_channels[0], temb_channels)
        else:
            self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[0], num_groups=norm_num_groups, eps=1e-06)
        self.conv_act = nn.SiLU()
        self.conv_out = nn.Conv2d(block_out_channels[0], out_channels, 3, padding=1)
        self.gradient_checkpointing = False

    def forward(self, z, latent_embeds=None):
        sample = z
        sample = self.conv_in(sample)
        upscale_dtype = next(iter(self.up_blocks.parameters())).dtype
        if self.training and self.gradient_checkpointing:

            def create_custom_forward(module):

                def custom_forward(*inputs):
                    return module(*inputs)
                return custom_forward
            if is_torch_version('>=', '1.11.0'):
                sample = torch.utils.checkpoint.checkpoint(create_custom_forward(self.mid_block), sample, latent_embeds, use_reentrant=False)
                sample = sample
                for up_block in self.up_blocks:
                    sample = torch.utils.checkpoint.checkpoint(create_custom_forward(up_block), sample, latent_embeds, use_reentrant=False)
            else:
                sample = torch.utils.checkpoint.checkpoint(create_custom_forward(self.mid_block), sample, latent_embeds)
                sample = sample
                for up_block in self.up_blocks:
                    sample = torch.utils.checkpoint.checkpoint(create_custom_forward(up_block), sample, latent_embeds)
        else:
            sample = self.mid_block(sample, latent_embeds)
            sample = sample
            for up_block in self.up_blocks:
                sample = up_block(sample, latent_embeds)
        if latent_embeds is None:
            sample = self.conv_norm_out(sample)
        else:
            sample = self.conv_norm_out(sample, latent_embeds)
        sample = self.conv_act(sample)
        sample = self.conv_out(sample)
        return sample


class VectorQuantizer(nn.Module):
    """
    Improved version over VectorQuantizer, can be used as a drop-in replacement. Mostly avoids costly matrix
    multiplications and allows for post-hoc remapping of indices.
    """

    def __init__(self, n_e, vq_embed_dim, beta, remap=None, unknown_index='random', sane_index_shape=False, legacy=True):
        super().__init__()
        self.n_e = n_e
        self.vq_embed_dim = vq_embed_dim
        self.beta = beta
        self.legacy = legacy
        self.embedding = nn.Embedding(self.n_e, self.vq_embed_dim)
        self.embedding.weight.data.uniform_(-1.0 / self.n_e, 1.0 / self.n_e)
        self.remap = remap
        if self.remap is not None:
            self.register_buffer('used', torch.tensor(np.load(self.remap)))
            self.re_embed = self.used.shape[0]
            self.unknown_index = unknown_index
            if self.unknown_index == 'extra':
                self.unknown_index = self.re_embed
                self.re_embed = self.re_embed + 1
            None
        else:
            self.re_embed = n_e
        self.sane_index_shape = sane_index_shape

    def remap_to_used(self, inds):
        ishape = inds.shape
        assert len(ishape) > 1
        inds = inds.reshape(ishape[0], -1)
        used = self.used
        match = (inds[:, :, None] == used[None, None, ...]).long()
        new = match.argmax(-1)
        unknown = match.sum(2) < 1
        if self.unknown_index == 'random':
            new[unknown] = torch.randint(0, self.re_embed, size=new[unknown].shape)
        else:
            new[unknown] = self.unknown_index
        return new.reshape(ishape)

    def unmap_to_all(self, inds):
        ishape = inds.shape
        assert len(ishape) > 1
        inds = inds.reshape(ishape[0], -1)
        used = self.used
        if self.re_embed > self.used.shape[0]:
            inds[inds >= self.used.shape[0]] = 0
        back = torch.gather(used[None, :][inds.shape[0] * [0], :], 1, inds)
        return back.reshape(ishape)

    def forward(self, z):
        z = z.permute(0, 2, 3, 1).contiguous()
        z_flattened = z.view(-1, self.vq_embed_dim)
        min_encoding_indices = torch.argmin(torch.cdist(z_flattened, self.embedding.weight), dim=1)
        z_q = self.embedding(min_encoding_indices).view(z.shape)
        perplexity = None
        min_encodings = None
        if not self.legacy:
            loss = self.beta * torch.mean((z_q.detach() - z) ** 2) + torch.mean((z_q - z.detach()) ** 2)
        else:
            loss = torch.mean((z_q.detach() - z) ** 2) + self.beta * torch.mean((z_q - z.detach()) ** 2)
        z_q = z + (z_q - z).detach()
        z_q = z_q.permute(0, 3, 1, 2).contiguous()
        if self.remap is not None:
            min_encoding_indices = min_encoding_indices.reshape(z.shape[0], -1)
            min_encoding_indices = self.remap_to_used(min_encoding_indices)
            min_encoding_indices = min_encoding_indices.reshape(-1, 1)
        if self.sane_index_shape:
            min_encoding_indices = min_encoding_indices.reshape(z_q.shape[0], z_q.shape[2], z_q.shape[3])
        return z_q, loss, (perplexity, min_encodings, min_encoding_indices)

    def get_codebook_entry(self, indices, shape):
        if self.remap is not None:
            indices = indices.reshape(shape[0], -1)
            indices = self.unmap_to_all(indices)
            indices = indices.reshape(-1)
        z_q = self.embedding(indices)
        if shape is not None:
            z_q = z_q.view(shape)
            z_q = z_q.permute(0, 3, 1, 2).contiguous()
        return z_q


class FlaxDiagonalGaussianDistribution(object):

    def __init__(self, parameters, deterministic=False):
        self.mean, self.logvar = jnp.split(parameters, 2, axis=-1)
        self.logvar = jnp.clip(self.logvar, -30.0, 20.0)
        self.deterministic = deterministic
        self.std = jnp.exp(0.5 * self.logvar)
        self.var = jnp.exp(self.logvar)
        if self.deterministic:
            self.var = self.std = jnp.zeros_like(self.mean)

    def sample(self, key):
        return self.mean + self.std * jax.random.normal(key, self.mean.shape)

    def kl(self, other=None):
        if self.deterministic:
            return jnp.array([0.0])
        if other is None:
            return 0.5 * jnp.sum(self.mean ** 2 + self.var - 1.0 - self.logvar, axis=[1, 2, 3])
        return 0.5 * jnp.sum(jnp.square(self.mean - other.mean) / other.var + self.var / other.var - 1.0 - self.logvar + other.logvar, axis=[1, 2, 3])

    def nll(self, sample, axis=[1, 2, 3]):
        if self.deterministic:
            return jnp.array([0.0])
        logtwopi = jnp.log(2.0 * jnp.pi)
        return 0.5 * jnp.sum(logtwopi + self.logvar + jnp.square(sample - self.mean) / self.var, axis=axis)

    def mode(self):
        return self.mean


class VQModel(metaclass=DummyObject):
    _backends = ['torch']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch'])


class IFWatermarker(ModelMixin, ConfigMixin):

    def __init__(self):
        super().__init__()
        self.register_buffer('watermark_image', torch.zeros((62, 62, 4)))
        self.watermark_image_as_pil = None

    def apply_watermark(self, images: 'List[PIL.Image.Image]', sample_size=None):
        h = images[0].height
        w = images[0].width
        sample_size = sample_size or h
        coef = min(h / sample_size, w / sample_size)
        img_h, img_w = (int(h / coef), int(w / coef)) if coef < 1 else (h, w)
        S1, S2 = 1024 ** 2, img_w * img_h
        K = (S2 / S1) ** 0.5
        wm_size, wm_x, wm_y = int(K * 62), img_w - int(14 * K), img_h - int(14 * K)
        if self.watermark_image_as_pil is None:
            watermark_image = self.watermark_image.cpu().numpy()
            watermark_image = Image.fromarray(watermark_image, mode='RGBA')
            self.watermark_image_as_pil = watermark_image
        wm_img = self.watermark_image_as_pil.resize((wm_size, wm_size), PIL_INTERPOLATION['bicubic'], reducing_gap=None)
        for pil_img in images:
            pil_img.paste(wm_img, box=(wm_x - wm_size, wm_y - wm_size, wm_x, wm_y), mask=wm_img.split()[-1])
        return images


class LDMBertAttention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, embed_dim: 'int', num_heads: 'int', head_dim: 'int', dropout: 'float'=0.0, is_decoder: 'bool'=False, bias: 'bool'=False):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.dropout = dropout
        self.head_dim = head_dim
        self.inner_dim = head_dim * num_heads
        self.scaling = self.head_dim ** -0.5
        self.is_decoder = is_decoder
        self.k_proj = nn.Linear(embed_dim, self.inner_dim, bias=bias)
        self.v_proj = nn.Linear(embed_dim, self.inner_dim, bias=bias)
        self.q_proj = nn.Linear(embed_dim, self.inner_dim, bias=bias)
        self.out_proj = nn.Linear(self.inner_dim, embed_dim)

    def _shape(self, tensor: 'torch.Tensor', seq_len: 'int', bsz: 'int'):
        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()

    def forward(self, hidden_states: 'torch.Tensor', key_value_states: 'Optional[torch.Tensor]'=None, past_key_value: 'Optional[Tuple[torch.Tensor]]'=None, attention_mask: 'Optional[torch.Tensor]'=None, layer_head_mask: 'Optional[torch.Tensor]'=None, output_attentions: 'bool'=False) ->Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        """Input shape: Batch x Time x Channel"""
        is_cross_attention = key_value_states is not None
        bsz, tgt_len, _ = hidden_states.size()
        query_states = self.q_proj(hidden_states) * self.scaling
        if is_cross_attention and past_key_value is not None:
            key_states = past_key_value[0]
            value_states = past_key_value[1]
        elif is_cross_attention:
            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)
            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)
        elif past_key_value is not None:
            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)
            key_states = torch.cat([past_key_value[0], key_states], dim=2)
            value_states = torch.cat([past_key_value[1], value_states], dim=2)
        else:
            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)
            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)
        if self.is_decoder:
            past_key_value = key_states, value_states
        proj_shape = bsz * self.num_heads, -1, self.head_dim
        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)
        key_states = key_states.view(*proj_shape)
        value_states = value_states.view(*proj_shape)
        src_len = key_states.size(1)
        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))
        if attn_weights.size() != (bsz * self.num_heads, tgt_len, src_len):
            raise ValueError(f'Attention weights should be of size {bsz * self.num_heads, tgt_len, src_len}, but is {attn_weights.size()}')
        if attention_mask is not None:
            if attention_mask.size() != (bsz, 1, tgt_len, src_len):
                raise ValueError(f'Attention mask should be of size {bsz, 1, tgt_len, src_len}, but is {attention_mask.size()}')
            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask
            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
        attn_weights = nn.functional.softmax(attn_weights, dim=-1)
        if layer_head_mask is not None:
            if layer_head_mask.size() != (self.num_heads,):
                raise ValueError(f'Head mask for a single layer should be of size {self.num_heads,}, but is {layer_head_mask.size()}')
            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
        if output_attentions:
            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)
        else:
            attn_weights_reshaped = None
        attn_probs = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)
        attn_output = torch.bmm(attn_probs, value_states)
        if attn_output.size() != (bsz * self.num_heads, tgt_len, self.head_dim):
            raise ValueError(f'`attn_output` should be of size {bsz, self.num_heads, tgt_len, self.head_dim}, but is {attn_output.size()}')
        attn_output = attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)
        attn_output = attn_output.transpose(1, 2)
        attn_output = attn_output.reshape(bsz, tgt_len, self.inner_dim)
        attn_output = self.out_proj(attn_output)
        return attn_output, attn_weights_reshaped, past_key_value


class LDMBertEncoderLayer(nn.Module):

    def __init__(self, config: 'LDMBertConfig'):
        super().__init__()
        self.embed_dim = config.d_model
        self.self_attn = LDMBertAttention(embed_dim=self.embed_dim, num_heads=config.encoder_attention_heads, head_dim=config.head_dim, dropout=config.attention_dropout)
        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)
        self.dropout = config.dropout
        self.activation_fn = ACT2FN[config.activation_function]
        self.activation_dropout = config.activation_dropout
        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)
        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)
        self.final_layer_norm = nn.LayerNorm(self.embed_dim)

    def forward(self, hidden_states: 'torch.FloatTensor', attention_mask: 'torch.FloatTensor', layer_head_mask: 'torch.FloatTensor', output_attentions: 'Optional[bool]'=False) ->Tuple[torch.FloatTensor, Optional[torch.FloatTensor]]:
        """
        Args:
            hidden_states (`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`
            attention_mask (`torch.FloatTensor`): attention mask of size
                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.
            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size
                `(encoder_attention_heads,)`.
            output_attentions (`bool`, *optional*):
                Whether or not to return the attentions tensors of all attention layers. See `attentions` under
                returned tensors for more detail.
        """
        residual = hidden_states
        hidden_states = self.self_attn_layer_norm(hidden_states)
        hidden_states, attn_weights, _ = self.self_attn(hidden_states=hidden_states, attention_mask=attention_mask, layer_head_mask=layer_head_mask, output_attentions=output_attentions)
        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
        hidden_states = residual + hidden_states
        residual = hidden_states
        hidden_states = self.final_layer_norm(hidden_states)
        hidden_states = self.activation_fn(self.fc1(hidden_states))
        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)
        hidden_states = self.fc2(hidden_states)
        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)
        hidden_states = residual + hidden_states
        if hidden_states.dtype == torch.float16 and (torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any()):
            clamp_value = torch.finfo(hidden_states.dtype).max - 1000
            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)
        outputs = hidden_states,
        if output_attentions:
            outputs += attn_weights,
        return outputs


class PaintByExampleMapper(nn.Module):

    def __init__(self, config):
        super().__init__()
        num_layers = (config.num_hidden_layers + 1) // 5
        hid_size = config.hidden_size
        num_heads = 1
        self.blocks = nn.ModuleList([BasicTransformerBlock(hid_size, num_heads, hid_size, activation_fn='gelu', attention_bias=True) for _ in range(num_layers)])

    def forward(self, hidden_states):
        for block in self.blocks:
            hidden_states = block(hidden_states)
        return hidden_states


class GaussianSmoothing(torch.nn.Module):
    """
    Arguments:
    Apply gaussian smoothing on a 1d, 2d or 3d tensor. Filtering is performed seperately for each channel in the input
    using a depthwise convolution.
        channels (int, sequence): Number of channels of the input tensors. Output will
            have this number of channels as well.
        kernel_size (int, sequence): Size of the gaussian kernel. sigma (float, sequence): Standard deviation of the
        gaussian kernel. dim (int, optional): The number of dimensions of the data.
            Default value is 2 (spatial).
    """

    def __init__(self, channels: 'int'=1, kernel_size: 'int'=3, sigma: 'float'=0.5, dim: 'int'=2):
        super().__init__()
        if isinstance(kernel_size, int):
            kernel_size = [kernel_size] * dim
        if isinstance(sigma, float):
            sigma = [sigma] * dim
        kernel = 1
        meshgrids = torch.meshgrid([torch.arange(size, dtype=torch.float32) for size in kernel_size])
        for size, std, mgrid in zip(kernel_size, sigma, meshgrids):
            mean = (size - 1) / 2
            kernel *= 1 / (std * math.sqrt(2 * math.pi)) * torch.exp(-((mgrid - mean) / (2 * std)) ** 2)
        kernel = kernel / torch.sum(kernel)
        kernel = kernel.view(1, 1, *kernel.size())
        kernel = kernel.repeat(channels, *([1] * (kernel.dim() - 1)))
        self.register_buffer('weight', kernel)
        self.groups = channels
        if dim == 1:
            self.conv = F.conv1d
        elif dim == 2:
            self.conv = F.conv2d
        elif dim == 3:
            self.conv = F.conv3d
        else:
            raise RuntimeError('Only 1, 2 and 3 dimensions are supported. Received {}.'.format(dim))

    def forward(self, input):
        """
        Arguments:
        Apply gaussian filter to input.
            input (torch.Tensor): Input to apply gaussian filter on.
        Returns:
            filtered (torch.Tensor): Filtered output.
        """
        return self.conv(input, weight=self.weight, groups=self.groups)


def jax_cosine_distance(emb_1, emb_2, eps=1e-12):
    norm_emb_1 = jnp.divide(emb_1.T, jnp.clip(jnp.linalg.norm(emb_1, axis=1), a_min=eps)).T
    norm_emb_2 = jnp.divide(emb_2.T, jnp.clip(jnp.linalg.norm(emb_2, axis=1), a_min=eps)).T
    return jnp.matmul(norm_emb_1, norm_emb_2.T)


class StableUnCLIPImageNormalizer(ModelMixin, ConfigMixin):
    """
    This class is used to hold the mean and standard deviation of the CLIP embedder used in stable unCLIP.

    It is used to normalize the image embeddings before the noise is applied and un-normalize the noised image
    embeddings.
    """

    @register_to_config
    def __init__(self, embedding_dim: 'int'=768):
        super().__init__()
        self.mean = nn.Parameter(torch.zeros(1, embedding_dim))
        self.std = nn.Parameter(torch.ones(1, embedding_dim))

    def to(self, torch_device: 'Optional[Union[str, torch.device]]'=None, torch_dtype: 'Optional[torch.dtype]'=None):
        self.mean = nn.Parameter(self.mean.to(torch_device))
        self.std = nn.Parameter(self.std.to(torch_device))
        return self

    def scale(self, embeds):
        embeds = (embeds - self.mean) * 1.0 / self.std
        return embeds

    def unscale(self, embeds):
        embeds = embeds * self.std + self.mean
        return embeds


class UnCLIPTextProjModel(ModelMixin, ConfigMixin):
    """
    Utility class for CLIP embeddings. Used to combine the image and text embeddings into a format usable by the
    decoder.

    For more details, see the original paper: https://arxiv.org/abs/2204.06125 section 2.1
    """

    @register_to_config
    def __init__(self, *, clip_extra_context_tokens: int=4, clip_embeddings_dim: int=768, time_embed_dim: int, cross_attention_dim):
        super().__init__()
        self.learned_classifier_free_guidance_embeddings = nn.Parameter(torch.zeros(clip_embeddings_dim))
        self.embedding_proj = nn.Linear(clip_embeddings_dim, time_embed_dim)
        self.clip_image_embeddings_project_to_time_embeddings = nn.Linear(clip_embeddings_dim, time_embed_dim)
        self.clip_extra_context_tokens = clip_extra_context_tokens
        self.clip_extra_context_tokens_proj = nn.Linear(clip_embeddings_dim, self.clip_extra_context_tokens * cross_attention_dim)
        self.encoder_hidden_states_proj = nn.Linear(clip_embeddings_dim, cross_attention_dim)
        self.text_encoder_hidden_states_norm = nn.LayerNorm(cross_attention_dim)

    def forward(self, *, image_embeddings, prompt_embeds, text_encoder_hidden_states, do_classifier_free_guidance):
        if do_classifier_free_guidance:
            image_embeddings_batch_size = image_embeddings.shape[0]
            classifier_free_guidance_embeddings = self.learned_classifier_free_guidance_embeddings.unsqueeze(0)
            classifier_free_guidance_embeddings = classifier_free_guidance_embeddings.expand(image_embeddings_batch_size, -1)
            image_embeddings = torch.cat([classifier_free_guidance_embeddings, image_embeddings], dim=0)
        assert image_embeddings.shape[0] == prompt_embeds.shape[0]
        batch_size = prompt_embeds.shape[0]
        time_projected_prompt_embeds = self.embedding_proj(prompt_embeds)
        time_projected_image_embeddings = self.clip_image_embeddings_project_to_time_embeddings(image_embeddings)
        additive_clip_time_embeddings = time_projected_image_embeddings + time_projected_prompt_embeds
        clip_extra_context_tokens = self.clip_extra_context_tokens_proj(image_embeddings)
        clip_extra_context_tokens = clip_extra_context_tokens.reshape(batch_size, -1, self.clip_extra_context_tokens)
        clip_extra_context_tokens = clip_extra_context_tokens.permute(0, 2, 1)
        text_encoder_hidden_states = self.encoder_hidden_states_proj(text_encoder_hidden_states)
        text_encoder_hidden_states = self.text_encoder_hidden_states_norm(text_encoder_hidden_states)
        text_encoder_hidden_states = torch.cat([clip_extra_context_tokens, text_encoder_hidden_states], dim=1)
        return text_encoder_hidden_states, additive_clip_time_embeddings


class UniDiffuserTextDecoder(metaclass=DummyObject):
    _backends = ['torch', 'transformers']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch', 'transformers'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch', 'transformers'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch', 'transformers'])


class SkipBlock(nn.Module):

    def __init__(self, dim: 'int'):
        super().__init__()
        self.skip_linear = nn.Linear(2 * dim, dim)
        self.norm = nn.LayerNorm(dim)

    def forward(self, x, skip):
        x = self.skip_linear(torch.cat([x, skip], dim=-1))
        x = self.norm(x)
        return x


class UTransformerBlock(nn.Module):
    """
    A modification of BasicTransformerBlock which supports pre-LayerNorm and post-LayerNorm configurations.

    Parameters:
        dim (`int`): The number of channels in the input and output.
        num_attention_heads (`int`): The number of heads to use for multi-head attention.
        attention_head_dim (`int`): The number of channels in each head.
        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.
        cross_attention_dim (`int`, *optional*): The size of the encoder_hidden_states vector for cross attention.
        activation_fn (`str`, *optional*, defaults to `"geglu"`):
            Activation function to be used in feed-forward.
        num_embeds_ada_norm (:obj: `int`, *optional*):
            The number of diffusion steps used during training. See `Transformer2DModel`.
        attention_bias (:obj: `bool`, *optional*, defaults to `False`):
            Configure if the attentions should contain a bias parameter.
        only_cross_attention (`bool`, *optional*):
            Whether to use only cross-attention layers. In this case two cross attention layers are used.
        double_self_attention (`bool`, *optional*):
            Whether to use two self-attention layers. In this case no cross attention layers are used.
        upcast_attention (`bool`, *optional*):
            Whether to upcast the query and key to float32 when performing the attention calculation.
        norm_elementwise_affine (`bool`, *optional*):
            Whether to use learnable per-element affine parameters during layer normalization.
        norm_type (`str`, defaults to `"layer_norm"`):
            The layer norm implementation to use.
        pre_layer_norm (`bool`, *optional*):
            Whether to perform layer normalization before the attention and feedforward operations ("pre-LayerNorm"),
            as opposed to after ("post-LayerNorm"). Note that `BasicTransformerBlock` uses pre-LayerNorm, e.g.
            `pre_layer_norm = True`.
        final_dropout (`bool`, *optional*):
            Whether to use a final Dropout layer after the feedforward network.
    """

    def __init__(self, dim: 'int', num_attention_heads: 'int', attention_head_dim: 'int', dropout=0.0, cross_attention_dim: 'Optional[int]'=None, activation_fn: 'str'='geglu', num_embeds_ada_norm: 'Optional[int]'=None, attention_bias: 'bool'=False, only_cross_attention: 'bool'=False, double_self_attention: 'bool'=False, upcast_attention: 'bool'=False, norm_elementwise_affine: 'bool'=True, norm_type: 'str'='layer_norm', pre_layer_norm: 'bool'=True, final_dropout: 'bool'=False):
        super().__init__()
        self.only_cross_attention = only_cross_attention
        self.use_ada_layer_norm = num_embeds_ada_norm is not None and norm_type == 'ada_norm'
        self.pre_layer_norm = pre_layer_norm
        if norm_type in ('ada_norm', 'ada_norm_zero') and num_embeds_ada_norm is None:
            raise ValueError(f'`norm_type` is set to {norm_type}, but `num_embeds_ada_norm` is not defined. Please make sure to define `num_embeds_ada_norm` if setting `norm_type` to {norm_type}.')
        self.attn1 = Attention(query_dim=dim, heads=num_attention_heads, dim_head=attention_head_dim, dropout=dropout, bias=attention_bias, cross_attention_dim=cross_attention_dim if only_cross_attention else None, upcast_attention=upcast_attention)
        if cross_attention_dim is not None or double_self_attention:
            self.attn2 = Attention(query_dim=dim, cross_attention_dim=cross_attention_dim if not double_self_attention else None, heads=num_attention_heads, dim_head=attention_head_dim, dropout=dropout, bias=attention_bias, upcast_attention=upcast_attention)
        else:
            self.attn2 = None
        if self.use_ada_layer_norm:
            self.norm1 = AdaLayerNorm(dim, num_embeds_ada_norm)
        else:
            self.norm1 = nn.LayerNorm(dim, elementwise_affine=norm_elementwise_affine)
        if cross_attention_dim is not None or double_self_attention:
            self.norm2 = AdaLayerNorm(dim, num_embeds_ada_norm) if self.use_ada_layer_norm else nn.LayerNorm(dim, elementwise_affine=norm_elementwise_affine)
        else:
            self.norm2 = None
        self.norm3 = nn.LayerNorm(dim, elementwise_affine=norm_elementwise_affine)
        self.ff = FeedForward(dim, dropout=dropout, activation_fn=activation_fn, final_dropout=final_dropout)

    def forward(self, hidden_states, attention_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, timestep=None, cross_attention_kwargs=None, class_labels=None):
        if self.pre_layer_norm:
            if self.use_ada_layer_norm:
                norm_hidden_states = self.norm1(hidden_states, timestep)
            else:
                norm_hidden_states = self.norm1(hidden_states)
        else:
            norm_hidden_states = hidden_states
        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
        attn_output = self.attn1(norm_hidden_states, encoder_hidden_states=encoder_hidden_states if self.only_cross_attention else None, attention_mask=attention_mask, **cross_attention_kwargs)
        if not self.pre_layer_norm:
            if self.use_ada_layer_norm:
                attn_output = self.norm1(attn_output, timestep)
            else:
                attn_output = self.norm1(attn_output)
        hidden_states = attn_output + hidden_states
        if self.attn2 is not None:
            if self.pre_layer_norm:
                norm_hidden_states = self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)
            else:
                norm_hidden_states = hidden_states
            attn_output = self.attn2(norm_hidden_states, encoder_hidden_states=encoder_hidden_states, attention_mask=encoder_attention_mask, **cross_attention_kwargs)
            if not self.pre_layer_norm:
                attn_output = self.norm2(attn_output, timestep) if self.use_ada_layer_norm else self.norm2(attn_output)
            hidden_states = attn_output + hidden_states
        if self.pre_layer_norm:
            norm_hidden_states = self.norm3(hidden_states)
        else:
            norm_hidden_states = hidden_states
        ff_output = self.ff(norm_hidden_states)
        if not self.pre_layer_norm:
            ff_output = self.norm3(ff_output)
        hidden_states = ff_output + hidden_states
        return hidden_states


class UniDiffuserBlock(nn.Module):
    """
    A modification of BasicTransformerBlock which supports pre-LayerNorm and post-LayerNorm configurations and puts the
    LayerNorms on the residual backbone of the block. This matches the transformer block in the [original UniDiffuser
    implementation](https://github.com/thu-ml/unidiffuser/blob/main/libs/uvit_multi_post_ln_v1.py#L104).

    Parameters:
        dim (`int`): The number of channels in the input and output.
        num_attention_heads (`int`): The number of heads to use for multi-head attention.
        attention_head_dim (`int`): The number of channels in each head.
        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.
        cross_attention_dim (`int`, *optional*): The size of the encoder_hidden_states vector for cross attention.
        activation_fn (`str`, *optional*, defaults to `"geglu"`):
            Activation function to be used in feed-forward.
        num_embeds_ada_norm (:obj: `int`, *optional*):
            The number of diffusion steps used during training. See `Transformer2DModel`.
        attention_bias (:obj: `bool`, *optional*, defaults to `False`):
            Configure if the attentions should contain a bias parameter.
        only_cross_attention (`bool`, *optional*):
            Whether to use only cross-attention layers. In this case two cross attention layers are used.
        double_self_attention (`bool`, *optional*):
            Whether to use two self-attention layers. In this case no cross attention layers are used.
        upcast_attention (`bool`, *optional*):
            Whether to upcast the query and key to float() when performing the attention calculation.
        norm_elementwise_affine (`bool`, *optional*):
            Whether to use learnable per-element affine parameters during layer normalization.
        norm_type (`str`, defaults to `"layer_norm"`):
            The layer norm implementation to use.
        pre_layer_norm (`bool`, *optional*):
            Whether to perform layer normalization before the attention and feedforward operations ("pre-LayerNorm"),
            as opposed to after ("post-LayerNorm"). The original UniDiffuser implementation is post-LayerNorm
            (`pre_layer_norm = False`).
        final_dropout (`bool`, *optional*):
            Whether to use a final Dropout layer after the feedforward network.
    """

    def __init__(self, dim: 'int', num_attention_heads: 'int', attention_head_dim: 'int', dropout=0.0, cross_attention_dim: 'Optional[int]'=None, activation_fn: 'str'='geglu', num_embeds_ada_norm: 'Optional[int]'=None, attention_bias: 'bool'=False, only_cross_attention: 'bool'=False, double_self_attention: 'bool'=False, upcast_attention: 'bool'=False, norm_elementwise_affine: 'bool'=True, norm_type: 'str'='layer_norm', pre_layer_norm: 'bool'=False, final_dropout: 'bool'=True):
        super().__init__()
        self.only_cross_attention = only_cross_attention
        self.use_ada_layer_norm = num_embeds_ada_norm is not None and norm_type == 'ada_norm'
        self.pre_layer_norm = pre_layer_norm
        if norm_type in ('ada_norm', 'ada_norm_zero') and num_embeds_ada_norm is None:
            raise ValueError(f'`norm_type` is set to {norm_type}, but `num_embeds_ada_norm` is not defined. Please make sure to define `num_embeds_ada_norm` if setting `norm_type` to {norm_type}.')
        self.attn1 = Attention(query_dim=dim, heads=num_attention_heads, dim_head=attention_head_dim, dropout=dropout, bias=attention_bias, cross_attention_dim=cross_attention_dim if only_cross_attention else None, upcast_attention=upcast_attention)
        if cross_attention_dim is not None or double_self_attention:
            self.attn2 = Attention(query_dim=dim, cross_attention_dim=cross_attention_dim if not double_self_attention else None, heads=num_attention_heads, dim_head=attention_head_dim, dropout=dropout, bias=attention_bias, upcast_attention=upcast_attention)
        else:
            self.attn2 = None
        if self.use_ada_layer_norm:
            self.norm1 = AdaLayerNorm(dim, num_embeds_ada_norm)
        else:
            self.norm1 = nn.LayerNorm(dim, elementwise_affine=norm_elementwise_affine)
        if cross_attention_dim is not None or double_self_attention:
            self.norm2 = AdaLayerNorm(dim, num_embeds_ada_norm) if self.use_ada_layer_norm else nn.LayerNorm(dim, elementwise_affine=norm_elementwise_affine)
        else:
            self.norm2 = None
        self.norm3 = nn.LayerNorm(dim, elementwise_affine=norm_elementwise_affine)
        self.ff = FeedForward(dim, dropout=dropout, activation_fn=activation_fn, final_dropout=final_dropout)

    def forward(self, hidden_states, attention_mask=None, encoder_hidden_states=None, encoder_attention_mask=None, timestep=None, cross_attention_kwargs=None, class_labels=None):
        if self.pre_layer_norm:
            if self.use_ada_layer_norm:
                hidden_states = self.norm1(hidden_states, timestep)
            else:
                hidden_states = self.norm1(hidden_states)
        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
        attn_output = self.attn1(hidden_states, encoder_hidden_states=encoder_hidden_states if self.only_cross_attention else None, attention_mask=attention_mask, **cross_attention_kwargs)
        hidden_states = attn_output + hidden_states
        if not self.pre_layer_norm:
            if self.use_ada_layer_norm:
                hidden_states = self.norm1(hidden_states, timestep)
            else:
                hidden_states = self.norm1(hidden_states)
        if self.attn2 is not None:
            if self.pre_layer_norm:
                hidden_states = self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)
            attn_output = self.attn2(hidden_states, encoder_hidden_states=encoder_hidden_states, attention_mask=encoder_attention_mask, **cross_attention_kwargs)
            hidden_states = attn_output + hidden_states
            if not self.pre_layer_norm:
                hidden_states = self.norm2(hidden_states, timestep) if self.use_ada_layer_norm else self.norm2(hidden_states)
        if self.pre_layer_norm:
            hidden_states = self.norm3(hidden_states)
        ff_output = self.ff(hidden_states)
        hidden_states = ff_output + hidden_states
        if not self.pre_layer_norm:
            hidden_states = self.norm3(hidden_states)
        return hidden_states


class UTransformer2DModel(ModelMixin, ConfigMixin):
    """
    Transformer model based on the [U-ViT](https://github.com/baofff/U-ViT) architecture for image-like data. Compared
    to [`Transformer2DModel`], this model has skip connections between transformer blocks in a "U"-shaped fashion,
    similar to a U-Net. Supports only continuous (actual embeddings) inputs, which are embedded via a [`PatchEmbed`]
    layer and then reshaped to (b, t, d).

    Parameters:
        num_attention_heads (`int`, *optional*, defaults to 16): The number of heads to use for multi-head attention.
        attention_head_dim (`int`, *optional*, defaults to 88): The number of channels in each head.
        in_channels (`int`, *optional*):
            Pass if the input is continuous. The number of channels in the input.
        out_channels (`int`, *optional*):
            The number of output channels; if `None`, defaults to `in_channels`.
        num_layers (`int`, *optional*, defaults to 1): The number of layers of Transformer blocks to use.
        dropout (`float`, *optional*, defaults to 0.0): The dropout probability to use.
        norm_num_groups (`int`, *optional*, defaults to `32`):
            The number of groups to use when performing Group Normalization.
        cross_attention_dim (`int`, *optional*): The number of encoder_hidden_states dimensions to use.
        attention_bias (`bool`, *optional*):
            Configure if the TransformerBlocks' attention should contain a bias parameter.
        sample_size (`int`, *optional*): Pass if the input is discrete. The width of the latent images.
            Note that this is fixed at training time as it is used for learning a number of position embeddings. See
            `ImagePositionalEmbeddings`.
        num_vector_embeds (`int`, *optional*):
            Pass if the input is discrete. The number of classes of the vector embeddings of the latent pixels.
            Includes the class for the masked latent pixel.
        patch_size (`int`, *optional*, defaults to 2):
            The patch size to use in the patch embedding.
        activation_fn (`str`, *optional*, defaults to `"geglu"`): Activation function to be used in feed-forward.
        num_embeds_ada_norm ( `int`, *optional*): Pass if at least one of the norm_layers is `AdaLayerNorm`.
            The number of diffusion steps used during training. Note that this is fixed at training time as it is used
            to learn a number of embeddings that are added to the hidden states. During inference, you can denoise for
            up to but not more than steps than `num_embeds_ada_norm`.
        use_linear_projection (int, *optional*): TODO: Not used
        only_cross_attention (`bool`, *optional*):
            Whether to use only cross-attention layers. In this case two cross attention layers are used in each
            transformer block.
        upcast_attention (`bool`, *optional*):
            Whether to upcast the query and key to float() when performing the attention calculation.
        norm_type (`str`, *optional*, defaults to `"layer_norm"`):
            The Layer Normalization implementation to use. Defaults to `torch.nn.LayerNorm`.
        block_type (`str`, *optional*, defaults to `"unidiffuser"`):
            The transformer block implementation to use. If `"unidiffuser"`, has the LayerNorms on the residual
            backbone of each transformer block; otherwise has them in the attention/feedforward branches (the standard
            behavior in `diffusers`.)
        pre_layer_norm (`bool`, *optional*):
            Whether to perform layer normalization before the attention and feedforward operations ("pre-LayerNorm"),
            as opposed to after ("post-LayerNorm"). The original UniDiffuser implementation is post-LayerNorm
            (`pre_layer_norm = False`).
        norm_elementwise_affine (`bool`, *optional*):
            Whether to use learnable per-element affine parameters during layer normalization.
        use_patch_pos_embed (`bool`, *optional*):
            Whether to use position embeddings inside the patch embedding layer (`PatchEmbed`).
        final_dropout (`bool`, *optional*):
            Whether to use a final Dropout layer after the feedforward network.
    """

    @register_to_config
    def __init__(self, num_attention_heads: 'int'=16, attention_head_dim: 'int'=88, in_channels: 'Optional[int]'=None, out_channels: 'Optional[int]'=None, num_layers: 'int'=1, dropout: 'float'=0.0, norm_num_groups: 'int'=32, cross_attention_dim: 'Optional[int]'=None, attention_bias: 'bool'=False, sample_size: 'Optional[int]'=None, num_vector_embeds: 'Optional[int]'=None, patch_size: 'Optional[int]'=2, activation_fn: 'str'='geglu', num_embeds_ada_norm: 'Optional[int]'=None, use_linear_projection: 'bool'=False, only_cross_attention: 'bool'=False, upcast_attention: 'bool'=False, norm_type: 'str'='layer_norm', block_type: 'str'='unidiffuser', pre_layer_norm: 'bool'=False, norm_elementwise_affine: 'bool'=True, use_patch_pos_embed=False, ff_final_dropout: 'bool'=False):
        super().__init__()
        self.use_linear_projection = use_linear_projection
        self.num_attention_heads = num_attention_heads
        self.attention_head_dim = attention_head_dim
        inner_dim = num_attention_heads * attention_head_dim
        assert in_channels is not None and patch_size is not None, 'Patch input requires in_channels and patch_size.'
        assert sample_size is not None, 'UTransformer2DModel over patched input must provide sample_size'
        self.height = sample_size
        self.width = sample_size
        self.patch_size = patch_size
        self.pos_embed = PatchEmbed(height=sample_size, width=sample_size, patch_size=patch_size, in_channels=in_channels, embed_dim=inner_dim, use_pos_embed=use_patch_pos_embed)
        if block_type == 'unidiffuser':
            block_cls = UniDiffuserBlock
        else:
            block_cls = UTransformerBlock
        self.transformer_in_blocks = nn.ModuleList([block_cls(inner_dim, num_attention_heads, attention_head_dim, dropout=dropout, cross_attention_dim=cross_attention_dim, activation_fn=activation_fn, num_embeds_ada_norm=num_embeds_ada_norm, attention_bias=attention_bias, only_cross_attention=only_cross_attention, upcast_attention=upcast_attention, norm_type=norm_type, pre_layer_norm=pre_layer_norm, norm_elementwise_affine=norm_elementwise_affine, final_dropout=ff_final_dropout) for d in range(num_layers // 2)])
        self.transformer_mid_block = block_cls(inner_dim, num_attention_heads, attention_head_dim, dropout=dropout, cross_attention_dim=cross_attention_dim, activation_fn=activation_fn, num_embeds_ada_norm=num_embeds_ada_norm, attention_bias=attention_bias, only_cross_attention=only_cross_attention, upcast_attention=upcast_attention, norm_type=norm_type, pre_layer_norm=pre_layer_norm, norm_elementwise_affine=norm_elementwise_affine, final_dropout=ff_final_dropout)
        self.transformer_out_blocks = nn.ModuleList([nn.ModuleDict({'skip': SkipBlock(inner_dim), 'block': block_cls(inner_dim, num_attention_heads, attention_head_dim, dropout=dropout, cross_attention_dim=cross_attention_dim, activation_fn=activation_fn, num_embeds_ada_norm=num_embeds_ada_norm, attention_bias=attention_bias, only_cross_attention=only_cross_attention, upcast_attention=upcast_attention, norm_type=norm_type, pre_layer_norm=pre_layer_norm, norm_elementwise_affine=norm_elementwise_affine, final_dropout=ff_final_dropout)}) for d in range(num_layers // 2)])
        self.out_channels = in_channels if out_channels is None else out_channels
        self.norm_out = nn.LayerNorm(inner_dim)

    def forward(self, hidden_states, encoder_hidden_states=None, timestep=None, class_labels=None, cross_attention_kwargs=None, return_dict: 'bool'=True, hidden_states_is_embedding: 'bool'=False, unpatchify: 'bool'=True):
        """
        Args:
            hidden_states ( When discrete, `torch.LongTensor` of shape `(batch size, num latent pixels)`.
                When continuous, `torch.FloatTensor` of shape `(batch size, channel, height, width)`): Input
                hidden_states
            encoder_hidden_states ( `torch.LongTensor` of shape `(batch size, encoder_hidden_states dim)`, *optional*):
                Conditional embeddings for cross attention layer. If not given, cross-attention defaults to
                self-attention.
            timestep ( `torch.long`, *optional*):
                Optional timestep to be applied as an embedding in AdaLayerNorm's. Used to indicate denoising step.
            class_labels ( `torch.LongTensor` of shape `(batch size, num classes)`, *optional*):
                Optional class labels to be applied as an embedding in AdaLayerZeroNorm. Used to indicate class labels
                conditioning.
            cross_attention_kwargs (*optional*):
                Keyword arguments to supply to the cross attention layers, if used.
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`models.unet_2d_condition.UNet2DConditionOutput`] instead of a plain tuple.
            hidden_states_is_embedding (`bool`, *optional*, defaults to `False`):
                Whether or not hidden_states is an embedding directly usable by the transformer. In this case we will
                ignore input handling (e.g. continuous, vectorized, etc.) and directly feed hidden_states into the
                transformer blocks.
            unpatchify (`bool`, *optional*, defaults to `True`):
                Whether to unpatchify the transformer output.

        Returns:
            [`~models.transformer_2d.Transformer2DModelOutput`] or `tuple`:
            [`~models.transformer_2d.Transformer2DModelOutput`] if `return_dict` is True, otherwise a `tuple`. When
            returning a tuple, the first element is the sample tensor.
        """
        if not unpatchify and return_dict:
            raise ValueError(f'Cannot both define `unpatchify`: {unpatchify} and `return_dict`: {return_dict} since when `unpatchify` is {unpatchify} the returned output is of shape (batch_size, seq_len, hidden_dim) rather than (batch_size, num_channels, height, width).')
        if not hidden_states_is_embedding:
            hidden_states = self.pos_embed(hidden_states)
        skips = []
        for in_block in self.transformer_in_blocks:
            hidden_states = in_block(hidden_states, encoder_hidden_states=encoder_hidden_states, timestep=timestep, cross_attention_kwargs=cross_attention_kwargs, class_labels=class_labels)
            skips.append(hidden_states)
        hidden_states = self.transformer_mid_block(hidden_states)
        for out_block in self.transformer_out_blocks:
            hidden_states = out_block['skip'](hidden_states, skips.pop())
            hidden_states = out_block['block'](hidden_states, encoder_hidden_states=encoder_hidden_states, timestep=timestep, cross_attention_kwargs=cross_attention_kwargs, class_labels=class_labels)
        hidden_states = self.norm_out(hidden_states)
        if unpatchify:
            height = width = int(hidden_states.shape[1] ** 0.5)
            hidden_states = hidden_states.reshape(shape=(-1, height, width, self.patch_size, self.patch_size, self.out_channels))
            hidden_states = torch.einsum('nhwpqc->nchpwq', hidden_states)
            output = hidden_states.reshape(shape=(-1, self.out_channels, height * self.patch_size, width * self.patch_size))
        else:
            output = hidden_states
        if not return_dict:
            return output,
        return Transformer2DModelOutput(sample=output)


class UniDiffuserModel(metaclass=DummyObject):
    _backends = ['torch', 'transformers']

    def __init__(self, *args, **kwargs):
        requires_backends(self, ['torch', 'transformers'])

    @classmethod
    def from_config(cls, *args, **kwargs):
        requires_backends(cls, ['torch', 'transformers'])

    @classmethod
    def from_pretrained(cls, *args, **kwargs):
        requires_backends(cls, ['torch', 'transformers'])


class AttnProcessor:
    """
    Default processor for performing attention-related computations.
    """

    def __call__(self, attn: 'Attention', hidden_states, encoder_hidden_states=None, attention_mask=None, temb=None):
        residual = hidden_states
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        query = attn.head_to_batch_dim(query)
        key = attn.head_to_batch_dim(key)
        value = attn.head_to_batch_dim(value)
        attention_probs = attn.get_attention_scores(query, key, attention_mask)
        hidden_states = torch.bmm(attention_probs, value)
        hidden_states = attn.batch_to_head_dim(hidden_states)
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


class AttnProcessor2_0:
    """
    Processor for implementing scaled dot-product attention (enabled by default if you're using PyTorch 2.0).
    """

    def __init__(self):
        if not hasattr(F, 'scaled_dot_product_attention'):
            raise ImportError('AttnProcessor2_0 requires PyTorch 2.0, to use it, please upgrade PyTorch to 2.0.')

    def __call__(self, attn: 'Attention', hidden_states, encoder_hidden_states=None, attention_mask=None, temb=None):
        residual = hidden_states
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        inner_dim = hidden_states.shape[-1]
        if attention_mask is not None:
            attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
            attention_mask = attention_mask.view(batch_size, attn.heads, -1, attention_mask.shape[-1])
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        head_dim = inner_dim // attn.heads
        query = query.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        key = key.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        value = value.view(batch_size, -1, attn.heads, head_dim).transpose(1, 2)
        hidden_states = F.scaled_dot_product_attention(query, key, value, attn_mask=attention_mask, dropout_p=0.0, is_causal=False)
        hidden_states = hidden_states.transpose(1, 2).reshape(batch_size, -1, attn.heads * head_dim)
        hidden_states = hidden_states
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


class SlicedAttnAddedKVProcessor:
    """
    Processor for implementing sliced attention with extra learnable key and value matrices for the text encoder.

    Args:
        slice_size (`int`, *optional*):
            The number of steps to compute attention. Uses as many slices as `attention_head_dim // slice_size`, and
            `attention_head_dim` must be a multiple of the `slice_size`.
    """

    def __init__(self, slice_size):
        self.slice_size = slice_size

    def __call__(self, attn: "'Attention'", hidden_states, encoder_hidden_states=None, attention_mask=None, temb=None):
        residual = hidden_states
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        hidden_states = hidden_states.view(hidden_states.shape[0], hidden_states.shape[1], -1).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        dim = query.shape[-1]
        query = attn.head_to_batch_dim(query)
        encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)
        encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)
        encoder_hidden_states_key_proj = attn.head_to_batch_dim(encoder_hidden_states_key_proj)
        encoder_hidden_states_value_proj = attn.head_to_batch_dim(encoder_hidden_states_value_proj)
        if not attn.only_cross_attention:
            key = attn.to_k(hidden_states)
            value = attn.to_v(hidden_states)
            key = attn.head_to_batch_dim(key)
            value = attn.head_to_batch_dim(value)
            key = torch.cat([encoder_hidden_states_key_proj, key], dim=1)
            value = torch.cat([encoder_hidden_states_value_proj, value], dim=1)
        else:
            key = encoder_hidden_states_key_proj
            value = encoder_hidden_states_value_proj
        batch_size_attention, query_tokens, _ = query.shape
        hidden_states = torch.zeros((batch_size_attention, query_tokens, dim // attn.heads), device=query.device, dtype=query.dtype)
        for i in range(batch_size_attention // self.slice_size):
            start_idx = i * self.slice_size
            end_idx = (i + 1) * self.slice_size
            query_slice = query[start_idx:end_idx]
            key_slice = key[start_idx:end_idx]
            attn_mask_slice = attention_mask[start_idx:end_idx] if attention_mask is not None else None
            attn_slice = attn.get_attention_scores(query_slice, key_slice, attn_mask_slice)
            attn_slice = torch.bmm(attn_slice, value[start_idx:end_idx])
            hidden_states[start_idx:end_idx] = attn_slice
        hidden_states = attn.batch_to_head_dim(hidden_states)
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        hidden_states = hidden_states.transpose(-1, -2).reshape(residual.shape)
        hidden_states = hidden_states + residual
        return hidden_states


class SlicedAttnProcessor:
    """
    Processor for implementing sliced attention.

    Args:
        slice_size (`int`, *optional*):
            The number of steps to compute attention. Uses as many slices as `attention_head_dim // slice_size`, and
            `attention_head_dim` must be a multiple of the `slice_size`.
    """

    def __init__(self, slice_size):
        self.slice_size = slice_size

    def __call__(self, attn: 'Attention', hidden_states, encoder_hidden_states=None, attention_mask=None):
        residual = hidden_states
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        dim = query.shape[-1]
        query = attn.head_to_batch_dim(query)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)
        key = attn.head_to_batch_dim(key)
        value = attn.head_to_batch_dim(value)
        batch_size_attention, query_tokens, _ = query.shape
        hidden_states = torch.zeros((batch_size_attention, query_tokens, dim // attn.heads), device=query.device, dtype=query.dtype)
        for i in range(batch_size_attention // self.slice_size):
            start_idx = i * self.slice_size
            end_idx = (i + 1) * self.slice_size
            query_slice = query[start_idx:end_idx]
            key_slice = key[start_idx:end_idx]
            attn_mask_slice = attention_mask[start_idx:end_idx] if attention_mask is not None else None
            attn_slice = attn.get_attention_scores(query_slice, key_slice, attn_mask_slice)
            attn_slice = torch.bmm(attn_slice, value[start_idx:end_idx])
            hidden_states[start_idx:end_idx] = attn_slice
        hidden_states = attn.batch_to_head_dim(hidden_states)
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


class XFormersAttnAddedKVProcessor:
    """
    Processor for implementing memory efficient attention using xFormers.

    Args:
        attention_op (`Callable`, *optional*, defaults to `None`):
            The base
            [operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase) to
            use as the attention operator. It is recommended to set to `None`, and allow xFormers to choose the best
            operator.
    """

    def __init__(self, attention_op: 'Optional[Callable]'=None):
        self.attention_op = attention_op

    def __call__(self, attn: 'Attention', hidden_states, encoder_hidden_states=None, attention_mask=None):
        residual = hidden_states
        hidden_states = hidden_states.view(hidden_states.shape[0], hidden_states.shape[1], -1).transpose(1, 2)
        batch_size, sequence_length, _ = hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, sequence_length, batch_size)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        query = attn.head_to_batch_dim(query)
        encoder_hidden_states_key_proj = attn.add_k_proj(encoder_hidden_states)
        encoder_hidden_states_value_proj = attn.add_v_proj(encoder_hidden_states)
        encoder_hidden_states_key_proj = attn.head_to_batch_dim(encoder_hidden_states_key_proj)
        encoder_hidden_states_value_proj = attn.head_to_batch_dim(encoder_hidden_states_value_proj)
        if not attn.only_cross_attention:
            key = attn.to_k(hidden_states)
            value = attn.to_v(hidden_states)
            key = attn.head_to_batch_dim(key)
            value = attn.head_to_batch_dim(value)
            key = torch.cat([encoder_hidden_states_key_proj, key], dim=1)
            value = torch.cat([encoder_hidden_states_value_proj, value], dim=1)
        else:
            key = encoder_hidden_states_key_proj
            value = encoder_hidden_states_value_proj
        hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask, op=self.attention_op, scale=attn.scale)
        hidden_states = hidden_states
        hidden_states = attn.batch_to_head_dim(hidden_states)
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        hidden_states = hidden_states.transpose(-1, -2).reshape(residual.shape)
        hidden_states = hidden_states + residual
        return hidden_states


class XFormersAttnProcessor:
    """
    Processor for implementing memory efficient attention using xFormers.

    Args:
        attention_op (`Callable`, *optional*, defaults to `None`):
            The base
            [operator](https://facebookresearch.github.io/xformers/components/ops.html#xformers.ops.AttentionOpBase) to
            use as the attention operator. It is recommended to set to `None`, and allow xFormers to choose the best
            operator.
    """

    def __init__(self, attention_op: 'Optional[Callable]'=None):
        self.attention_op = attention_op

    def __call__(self, attn: 'Attention', hidden_states: 'torch.FloatTensor', encoder_hidden_states: 'Optional[torch.FloatTensor]'=None, attention_mask: 'Optional[torch.FloatTensor]'=None, temb: 'Optional[torch.FloatTensor]'=None):
        actual_size = hidden_states.shape[0]
        if SPLIT_SIZE != -1 and actual_size > SPLIT_SIZE:
            split_steps = math.ceil(actual_size / SPLIT_SIZE)
            split_steps = min(split_steps, actual_size)
            hidden_states_out = []
            _hidden_states = hidden_states.chunk(split_steps)
            if encoder_hidden_states is None:
                _encoder_hidden_states = [None] * split_steps
            else:
                _encoder_hidden_states = encoder_hidden_states.chunk(split_steps)
            assert attention_mask is None
            assert temb is None
            for i in range(split_steps):
                hidden_states_out.append(self._real_call(attn, _hidden_states[i], _encoder_hidden_states[i], attention_mask, temb))
            return torch.cat(hidden_states_out, dim=0)
        else:
            return self._real_call(attn, hidden_states, encoder_hidden_states, attention_mask, temb)

    def _real_call(self, attn: 'Attention', hidden_states: 'torch.FloatTensor', encoder_hidden_states: 'Optional[torch.FloatTensor]'=None, attention_mask: 'Optional[torch.FloatTensor]'=None, temb: 'Optional[torch.FloatTensor]'=None):
        residual = hidden_states
        if attn.spatial_norm is not None:
            hidden_states = attn.spatial_norm(hidden_states, temb)
        input_ndim = hidden_states.ndim
        if input_ndim == 4:
            batch_size, channel, height, width = hidden_states.shape
            hidden_states = hidden_states.view(batch_size, channel, height * width).transpose(1, 2)
        batch_size, key_tokens, _ = hidden_states.shape if encoder_hidden_states is None else encoder_hidden_states.shape
        attention_mask = attn.prepare_attention_mask(attention_mask, key_tokens, batch_size)
        if attention_mask is not None:
            _, query_tokens, _ = hidden_states.shape
            attention_mask = attention_mask.expand(-1, query_tokens, -1)
        if attn.group_norm is not None:
            hidden_states = attn.group_norm(hidden_states.transpose(1, 2)).transpose(1, 2)
        query = attn.to_q(hidden_states)
        if encoder_hidden_states is None:
            encoder_hidden_states = hidden_states
        elif attn.norm_cross:
            encoder_hidden_states = attn.norm_encoder_hidden_states(encoder_hidden_states)
        key = attn.to_k(encoder_hidden_states)
        value = attn.to_v(encoder_hidden_states)

        def _split_head(tensor):
            head_size = attn.heads
            batch_size, seq_len, dim = tensor.shape
            tensor = tensor.reshape(batch_size, seq_len, head_size, dim // head_size)
            return tensor

        def _back_head(tensor):
            batch_size, seq_len, head_size, dim = tensor.shape
            tensor = tensor.reshape(batch_size, seq_len, head_size * dim)
            return tensor
        query = _split_head(query)
        key = _split_head(key)
        value = _split_head(value)
        if attention_mask is not None:
            b, l1, l2 = attention_mask.shape
            if attention_mask.stride(-2) % 8 != 0:
                l1_align = (l1 // 8 + 1) * 8
                l2_align = (l2 // 8 + 1) * 8
                attention_mask_align = torch.zeros((b, l1_align, l2_align), dtype=attention_mask.dtype, device=attention_mask.device)
                attention_mask_align[:, :l1, :l2] = attention_mask
                attention_mask = attention_mask_align
            hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask[:, :l1, :l2], op=self.attention_op, scale=attn.scale)
        else:
            hidden_states = xformers.ops.memory_efficient_attention(query, key, value, attn_bias=attention_mask, op=self.attention_op, scale=attn.scale)
        hidden_states = hidden_states
        hidden_states = _back_head(hidden_states)
        hidden_states = attn.to_out[0](hidden_states)
        hidden_states = attn.to_out[1](hidden_states)
        if input_ndim == 4:
            hidden_states = hidden_states.transpose(-1, -2).reshape(batch_size, channel, height, width)
        if attn.residual_connection:
            hidden_states = hidden_states + residual
        hidden_states = hidden_states / attn.rescale_output_factor
        return hidden_states


AttentionProcessor = Union[AttnProcessor, AttnProcessor2_0, XFormersAttnProcessor, SlicedAttnProcessor, AttnAddedKVProcessor, SlicedAttnAddedKVProcessor, AttnAddedKVProcessor2_0, XFormersAttnAddedKVProcessor, LoRAAttnProcessor, LoRAXFormersAttnProcessor, LoRAAttnProcessor2_0, LoRAAttnAddedKVProcessor, CustomDiffusionAttnProcessor, CustomDiffusionXFormersAttnProcessor]


@dataclass
class UNet2DConditionOutput(BaseOutput):
    """
    Args:
        sample (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):
            Hidden states conditioned on `encoder_hidden_states` input. Output of last layer of model.
    """
    sample: 'torch.FloatTensor'


class UNetMidBlockFlatCrossAttn(nn.Module):

    def __init__(self, in_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attn_num_head_channels=1, output_scale_factor=1.0, cross_attention_dim=1280, dual_cross_attention=False, use_linear_projection=False, upcast_attention=False):
        super().__init__()
        self.has_cross_attention = True
        self.attn_num_head_channels = attn_num_head_channels
        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)
        resnets = [ResnetBlockFlat(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm)]
        attentions = []
        for _ in range(num_layers):
            if not dual_cross_attention:
                attentions.append(Transformer2DModel(attn_num_head_channels, in_channels // attn_num_head_channels, in_channels=in_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups, use_linear_projection=use_linear_projection, upcast_attention=upcast_attention))
            else:
                attentions.append(DualTransformer2DModel(attn_num_head_channels, in_channels // attn_num_head_channels, in_channels=in_channels, num_layers=1, cross_attention_dim=cross_attention_dim, norm_num_groups=resnet_groups))
            resnets.append(ResnetBlockFlat(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)

    def forward(self, hidden_states: 'torch.FloatTensor', temb: 'Optional[torch.FloatTensor]'=None, encoder_hidden_states: 'Optional[torch.FloatTensor]'=None, attention_mask: 'Optional[torch.FloatTensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[torch.FloatTensor]'=None) ->torch.FloatTensor:
        hidden_states = self.resnets[0](hidden_states, temb)
        for attn, resnet in zip(self.attentions, self.resnets[1:]):
            hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, attention_mask=attention_mask, encoder_attention_mask=encoder_attention_mask, return_dict=False)[0]
            hidden_states = resnet(hidden_states, temb)
        return hidden_states


class UNetMidBlockFlatSimpleCrossAttn(nn.Module):

    def __init__(self, in_channels: 'int', temb_channels: 'int', dropout: 'float'=0.0, num_layers: 'int'=1, resnet_eps: 'float'=1e-06, resnet_time_scale_shift: 'str'='default', resnet_act_fn: 'str'='swish', resnet_groups: 'int'=32, resnet_pre_norm: 'bool'=True, attn_num_head_channels=1, output_scale_factor=1.0, cross_attention_dim=1280, skip_time_act=False, only_cross_attention=False, cross_attention_norm=None):
        super().__init__()
        self.has_cross_attention = True
        self.attn_num_head_channels = attn_num_head_channels
        resnet_groups = resnet_groups if resnet_groups is not None else min(in_channels // 4, 32)
        self.num_heads = in_channels // self.attn_num_head_channels
        resnets = [ResnetBlockFlat(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act)]
        attentions = []
        for _ in range(num_layers):
            processor = AttnAddedKVProcessor2_0() if hasattr(F, 'scaled_dot_product_attention') else AttnAddedKVProcessor()
            attentions.append(Attention(query_dim=in_channels, cross_attention_dim=in_channels, heads=self.num_heads, dim_head=attn_num_head_channels, added_kv_proj_dim=cross_attention_dim, norm_num_groups=resnet_groups, bias=True, upcast_softmax=True, only_cross_attention=only_cross_attention, cross_attention_norm=cross_attention_norm, processor=processor))
            resnets.append(ResnetBlockFlat(in_channels=in_channels, out_channels=in_channels, temb_channels=temb_channels, eps=resnet_eps, groups=resnet_groups, dropout=dropout, time_embedding_norm=resnet_time_scale_shift, non_linearity=resnet_act_fn, output_scale_factor=output_scale_factor, pre_norm=resnet_pre_norm, skip_time_act=skip_time_act))
        self.attentions = nn.ModuleList(attentions)
        self.resnets = nn.ModuleList(resnets)

    def forward(self, hidden_states: 'torch.FloatTensor', temb: 'Optional[torch.FloatTensor]'=None, encoder_hidden_states: 'Optional[torch.FloatTensor]'=None, attention_mask: 'Optional[torch.FloatTensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, encoder_attention_mask: 'Optional[torch.FloatTensor]'=None):
        cross_attention_kwargs = cross_attention_kwargs if cross_attention_kwargs is not None else {}
        if attention_mask is None:
            mask = None if encoder_hidden_states is None else encoder_attention_mask
        else:
            mask = attention_mask
        hidden_states = self.resnets[0](hidden_states, temb)
        for attn, resnet in zip(self.attentions, self.resnets[1:]):
            hidden_states = attn(hidden_states, encoder_hidden_states=encoder_hidden_states, attention_mask=mask, **cross_attention_kwargs)
            hidden_states = resnet(hidden_states, temb)
        return hidden_states


class UNetFlatConditionModel(ModelMixin, ConfigMixin):
    """
    UNetFlatConditionModel is a conditional 2D UNet model that takes in a noisy sample, conditional state, and a
    timestep and returns sample shaped output.

    This model inherits from [`ModelMixin`]. Check the superclass documentation for the generic methods the library
    implements for all the models (such as downloading or saving, etc.)

    Parameters:
        sample_size (`int` or `Tuple[int, int]`, *optional*, defaults to `None`):
            Height and width of input/output sample.
        in_channels (`int`, *optional*, defaults to 4): The number of channels in the input sample.
        out_channels (`int`, *optional*, defaults to 4): The number of channels in the output.
        center_input_sample (`bool`, *optional*, defaults to `False`): Whether to center the input sample.
        flip_sin_to_cos (`bool`, *optional*, defaults to `False`):
            Whether to flip the sin to cos in the time embedding.
        freq_shift (`int`, *optional*, defaults to 0): The frequency shift to apply to the time embedding.
        down_block_types (`Tuple[str]`, *optional*, defaults to `("CrossAttnDownBlockFlat", "CrossAttnDownBlockFlat", "CrossAttnDownBlockFlat", "DownBlockFlat")`):
            The tuple of downsample blocks to use.
        mid_block_type (`str`, *optional*, defaults to `"UNetMidBlockFlatCrossAttn"`):
            The mid block type. Choose from `UNetMidBlockFlatCrossAttn` or `UNetMidBlockFlatSimpleCrossAttn`, will skip
            the mid block layer if `None`.
        up_block_types (`Tuple[str]`, *optional*, defaults to `("UpBlockFlat", "CrossAttnUpBlockFlat", "CrossAttnUpBlockFlat", "CrossAttnUpBlockFlat",)`):
            The tuple of upsample blocks to use.
        only_cross_attention(`bool` or `Tuple[bool]`, *optional*, default to `False`):
            Whether to include self-attention in the basic transformer blocks, see
            [`~models.attention.BasicTransformerBlock`].
        block_out_channels (`Tuple[int]`, *optional*, defaults to `(320, 640, 1280, 1280)`):
            The tuple of output channels for each block.
        layers_per_block (`int`, *optional*, defaults to 2): The number of layers per block.
        downsample_padding (`int`, *optional*, defaults to 1): The padding to use for the downsampling convolution.
        mid_block_scale_factor (`float`, *optional*, defaults to 1.0): The scale factor to use for the mid block.
        act_fn (`str`, *optional*, defaults to `"silu"`): The activation function to use.
        norm_num_groups (`int`, *optional*, defaults to 32): The number of groups to use for the normalization.
            If `None`, it will skip the normalization and activation layers in post-processing
        norm_eps (`float`, *optional*, defaults to 1e-5): The epsilon to use for the normalization.
        cross_attention_dim (`int` or `Tuple[int]`, *optional*, defaults to 1280):
            The dimension of the cross attention features.
        encoder_hid_dim (`int`, *optional*, defaults to None):
            If `encoder_hid_dim_type` is defined, `encoder_hidden_states` will be projected from `encoder_hid_dim`
            dimension to `cross_attention_dim`.
        encoder_hid_dim_type (`str`, *optional*, defaults to None):
            If given, the `encoder_hidden_states` and potentially other embeddings will be down-projected to text
            embeddings of dimension `cross_attention` according to `encoder_hid_dim_type`.
        attention_head_dim (`int`, *optional*, defaults to 8): The dimension of the attention heads.
        resnet_time_scale_shift (`str`, *optional*, defaults to `"default"`): Time scale shift config
            for resnet blocks, see [`~models.resnet.ResnetBlockFlat`]. Choose from `default` or `scale_shift`.
        class_embed_type (`str`, *optional*, defaults to None):
            The type of class embedding to use which is ultimately summed with the time embeddings. Choose from `None`,
            `"timestep"`, `"identity"`, `"projection"`, or `"simple_projection"`.
        addition_embed_type (`str`, *optional*, defaults to None):
            Configures an optional embedding which will be summed with the time embeddings. Choose from `None` or
            "text". "text" will use the `TextTimeEmbedding` layer.
        num_class_embeds (`int`, *optional*, defaults to None):
            Input dimension of the learnable embedding matrix to be projected to `time_embed_dim`, when performing
            class conditioning with `class_embed_type` equal to `None`.
        time_embedding_type (`str`, *optional*, default to `positional`):
            The type of position embedding to use for timesteps. Choose from `positional` or `fourier`.
        time_embedding_dim (`int`, *optional*, default to `None`):
            An optional override for the dimension of the projected time embedding.
        time_embedding_act_fn (`str`, *optional*, default to `None`):
            Optional activation function to use on the time embeddings only one time before they as passed to the rest
            of the unet. Choose from `silu`, `mish`, `gelu`, and `swish`.
        timestep_post_act (`str, *optional*, default to `None`):
            The second activation function to use in timestep embedding. Choose from `silu`, `mish` and `gelu`.
        time_cond_proj_dim (`int`, *optional*, default to `None`):
            The dimension of `cond_proj` layer in timestep embedding.
        conv_in_kernel (`int`, *optional*, default to `3`): The kernel size of `conv_in` layer.
        conv_out_kernel (`int`, *optional*, default to `3`): The kernel size of `conv_out` layer.
        projection_class_embeddings_input_dim (`int`, *optional*): The dimension of the `class_labels` input when
            using the "projection" `class_embed_type`. Required when using the "projection" `class_embed_type`.
        class_embeddings_concat (`bool`, *optional*, defaults to `False`): Whether to concatenate the time
            embeddings with the class embeddings.
        mid_block_only_cross_attention (`bool`, *optional*, defaults to `None`):
            Whether to use cross attention with the mid block when using the `UNetMidBlockFlatSimpleCrossAttn`. If
            `only_cross_attention` is given as a single boolean and `mid_block_only_cross_attention` is None, the
            `only_cross_attention` value will be used as the value for `mid_block_only_cross_attention`. Else, it will
            default to `False`.
    """
    _supports_gradient_checkpointing = True

    @register_to_config
    def __init__(self, sample_size: 'Optional[int]'=None, in_channels: 'int'=4, out_channels: 'int'=4, center_input_sample: 'bool'=False, flip_sin_to_cos: 'bool'=True, freq_shift: 'int'=0, down_block_types: 'Tuple[str]'=('CrossAttnDownBlockFlat', 'CrossAttnDownBlockFlat', 'CrossAttnDownBlockFlat', 'DownBlockFlat'), mid_block_type: 'Optional[str]'='UNetMidBlockFlatCrossAttn', up_block_types: 'Tuple[str]'=('UpBlockFlat', 'CrossAttnUpBlockFlat', 'CrossAttnUpBlockFlat', 'CrossAttnUpBlockFlat'), only_cross_attention: 'Union[bool, Tuple[bool]]'=False, block_out_channels: 'Tuple[int]'=(320, 640, 1280, 1280), layers_per_block: 'Union[int, Tuple[int]]'=2, downsample_padding: 'int'=1, mid_block_scale_factor: 'float'=1, act_fn: 'str'='silu', norm_num_groups: 'Optional[int]'=32, norm_eps: 'float'=1e-05, cross_attention_dim: 'Union[int, Tuple[int]]'=1280, encoder_hid_dim: 'Optional[int]'=None, encoder_hid_dim_type: 'Optional[str]'=None, attention_head_dim: 'Union[int, Tuple[int]]'=8, dual_cross_attention: 'bool'=False, use_linear_projection: 'bool'=False, class_embed_type: 'Optional[str]'=None, addition_embed_type: 'Optional[str]'=None, num_class_embeds: 'Optional[int]'=None, upcast_attention: 'bool'=False, resnet_time_scale_shift: 'str'='default', resnet_skip_time_act: 'bool'=False, resnet_out_scale_factor: 'int'=1.0, time_embedding_type: 'str'='positional', time_embedding_dim: 'Optional[int]'=None, time_embedding_act_fn: 'Optional[str]'=None, timestep_post_act: 'Optional[str]'=None, time_cond_proj_dim: 'Optional[int]'=None, conv_in_kernel: 'int'=3, conv_out_kernel: 'int'=3, projection_class_embeddings_input_dim: 'Optional[int]'=None, class_embeddings_concat: 'bool'=False, mid_block_only_cross_attention: 'Optional[bool]'=None, cross_attention_norm: 'Optional[str]'=None, addition_embed_type_num_heads=64):
        super().__init__()
        self.sample_size = sample_size
        if len(down_block_types) != len(up_block_types):
            raise ValueError(f'Must provide the same number of `down_block_types` as `up_block_types`. `down_block_types`: {down_block_types}. `up_block_types`: {up_block_types}.')
        if len(block_out_channels) != len(down_block_types):
            raise ValueError(f'Must provide the same number of `block_out_channels` as `down_block_types`. `block_out_channels`: {block_out_channels}. `down_block_types`: {down_block_types}.')
        if not isinstance(only_cross_attention, bool) and len(only_cross_attention) != len(down_block_types):
            raise ValueError(f'Must provide the same number of `only_cross_attention` as `down_block_types`. `only_cross_attention`: {only_cross_attention}. `down_block_types`: {down_block_types}.')
        if not isinstance(attention_head_dim, int) and len(attention_head_dim) != len(down_block_types):
            raise ValueError(f'Must provide the same number of `attention_head_dim` as `down_block_types`. `attention_head_dim`: {attention_head_dim}. `down_block_types`: {down_block_types}.')
        if isinstance(cross_attention_dim, list) and len(cross_attention_dim) != len(down_block_types):
            raise ValueError(f'Must provide the same number of `cross_attention_dim` as `down_block_types`. `cross_attention_dim`: {cross_attention_dim}. `down_block_types`: {down_block_types}.')
        if not isinstance(layers_per_block, int) and len(layers_per_block) != len(down_block_types):
            raise ValueError(f'Must provide the same number of `layers_per_block` as `down_block_types`. `layers_per_block`: {layers_per_block}. `down_block_types`: {down_block_types}.')
        conv_in_padding = (conv_in_kernel - 1) // 2
        self.conv_in = LinearMultiDim(in_channels, block_out_channels[0], kernel_size=conv_in_kernel, padding=conv_in_padding)
        if time_embedding_type == 'fourier':
            time_embed_dim = time_embedding_dim or block_out_channels[0] * 2
            if time_embed_dim % 2 != 0:
                raise ValueError(f'`time_embed_dim` should be divisible by 2, but is {time_embed_dim}.')
            self.time_proj = GaussianFourierProjection(time_embed_dim // 2, set_W_to_weight=False, log=False, flip_sin_to_cos=flip_sin_to_cos)
            timestep_input_dim = time_embed_dim
        elif time_embedding_type == 'positional':
            time_embed_dim = time_embedding_dim or block_out_channels[0] * 4
            self.time_proj = Timesteps(block_out_channels[0], flip_sin_to_cos, freq_shift)
            timestep_input_dim = block_out_channels[0]
        else:
            raise ValueError(f'{time_embedding_type} does not exist. Please make sure to use one of `fourier` or `positional`.')
        self.time_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim, act_fn=act_fn, post_act_fn=timestep_post_act, cond_proj_dim=time_cond_proj_dim)
        if encoder_hid_dim_type is None and encoder_hid_dim is not None:
            encoder_hid_dim_type = 'text_proj'
            self.register_to_config(encoder_hid_dim_type=encoder_hid_dim_type)
            logger.info("encoder_hid_dim_type defaults to 'text_proj' as `encoder_hid_dim` is defined.")
        if encoder_hid_dim is None and encoder_hid_dim_type is not None:
            raise ValueError(f'`encoder_hid_dim` has to be defined when `encoder_hid_dim_type` is set to {encoder_hid_dim_type}.')
        if encoder_hid_dim_type == 'text_proj':
            self.encoder_hid_proj = nn.Linear(encoder_hid_dim, cross_attention_dim)
        elif encoder_hid_dim_type == 'text_image_proj':
            self.encoder_hid_proj = TextImageProjection(text_embed_dim=encoder_hid_dim, image_embed_dim=cross_attention_dim, cross_attention_dim=cross_attention_dim)
        elif encoder_hid_dim_type is not None:
            raise ValueError(f"encoder_hid_dim_type: {encoder_hid_dim_type} must be None, 'text_proj' or 'text_image_proj'.")
        else:
            self.encoder_hid_proj = None
        if class_embed_type is None and num_class_embeds is not None:
            self.class_embedding = nn.Embedding(num_class_embeds, time_embed_dim)
        elif class_embed_type == 'timestep':
            self.class_embedding = TimestepEmbedding(timestep_input_dim, time_embed_dim, act_fn=act_fn)
        elif class_embed_type == 'identity':
            self.class_embedding = nn.Identity(time_embed_dim, time_embed_dim)
        elif class_embed_type == 'projection':
            if projection_class_embeddings_input_dim is None:
                raise ValueError("`class_embed_type`: 'projection' requires `projection_class_embeddings_input_dim` be set")
            self.class_embedding = TimestepEmbedding(projection_class_embeddings_input_dim, time_embed_dim)
        elif class_embed_type == 'simple_projection':
            if projection_class_embeddings_input_dim is None:
                raise ValueError("`class_embed_type`: 'simple_projection' requires `projection_class_embeddings_input_dim` be set")
            self.class_embedding = nn.Linear(projection_class_embeddings_input_dim, time_embed_dim)
        else:
            self.class_embedding = None
        if addition_embed_type == 'text':
            if encoder_hid_dim is not None:
                text_time_embedding_from_dim = encoder_hid_dim
            else:
                text_time_embedding_from_dim = cross_attention_dim
            self.add_embedding = TextTimeEmbedding(text_time_embedding_from_dim, time_embed_dim, num_heads=addition_embed_type_num_heads)
        elif addition_embed_type == 'text_image':
            self.add_embedding = TextImageTimeEmbedding(text_embed_dim=cross_attention_dim, image_embed_dim=cross_attention_dim, time_embed_dim=time_embed_dim)
        elif addition_embed_type is not None:
            raise ValueError(f"addition_embed_type: {addition_embed_type} must be None, 'text' or 'text_image'.")
        if time_embedding_act_fn is None:
            self.time_embed_act = None
        else:
            self.time_embed_act = get_activation(time_embedding_act_fn)
        self.down_blocks = nn.ModuleList([])
        self.up_blocks = nn.ModuleList([])
        if isinstance(only_cross_attention, bool):
            if mid_block_only_cross_attention is None:
                mid_block_only_cross_attention = only_cross_attention
            only_cross_attention = [only_cross_attention] * len(down_block_types)
        if mid_block_only_cross_attention is None:
            mid_block_only_cross_attention = False
        if isinstance(attention_head_dim, int):
            attention_head_dim = (attention_head_dim,) * len(down_block_types)
        if isinstance(cross_attention_dim, int):
            cross_attention_dim = (cross_attention_dim,) * len(down_block_types)
        if isinstance(layers_per_block, int):
            layers_per_block = [layers_per_block] * len(down_block_types)
        if class_embeddings_concat:
            blocks_time_embed_dim = time_embed_dim * 2
        else:
            blocks_time_embed_dim = time_embed_dim
        output_channel = block_out_channels[0]
        for i, down_block_type in enumerate(down_block_types):
            input_channel = output_channel
            output_channel = block_out_channels[i]
            is_final_block = i == len(block_out_channels) - 1
            down_block = get_down_block(down_block_type, num_layers=layers_per_block[i], in_channels=input_channel, out_channels=output_channel, temb_channels=blocks_time_embed_dim, add_downsample=not is_final_block, resnet_eps=norm_eps, resnet_act_fn=act_fn, resnet_groups=norm_num_groups, cross_attention_dim=cross_attention_dim[i], attn_num_head_channels=attention_head_dim[i], downsample_padding=downsample_padding, dual_cross_attention=dual_cross_attention, use_linear_projection=use_linear_projection, only_cross_attention=only_cross_attention[i], upcast_attention=upcast_attention, resnet_time_scale_shift=resnet_time_scale_shift, resnet_skip_time_act=resnet_skip_time_act, resnet_out_scale_factor=resnet_out_scale_factor, cross_attention_norm=cross_attention_norm)
            self.down_blocks.append(down_block)
        if mid_block_type == 'UNetMidBlockFlatCrossAttn':
            self.mid_block = UNetMidBlockFlatCrossAttn(in_channels=block_out_channels[-1], temb_channels=blocks_time_embed_dim, resnet_eps=norm_eps, resnet_act_fn=act_fn, output_scale_factor=mid_block_scale_factor, resnet_time_scale_shift=resnet_time_scale_shift, cross_attention_dim=cross_attention_dim[-1], attn_num_head_channels=attention_head_dim[-1], resnet_groups=norm_num_groups, dual_cross_attention=dual_cross_attention, use_linear_projection=use_linear_projection, upcast_attention=upcast_attention)
        elif mid_block_type == 'UNetMidBlockFlatSimpleCrossAttn':
            self.mid_block = UNetMidBlockFlatSimpleCrossAttn(in_channels=block_out_channels[-1], temb_channels=blocks_time_embed_dim, resnet_eps=norm_eps, resnet_act_fn=act_fn, output_scale_factor=mid_block_scale_factor, cross_attention_dim=cross_attention_dim[-1], attn_num_head_channels=attention_head_dim[-1], resnet_groups=norm_num_groups, resnet_time_scale_shift=resnet_time_scale_shift, skip_time_act=resnet_skip_time_act, only_cross_attention=mid_block_only_cross_attention, cross_attention_norm=cross_attention_norm)
        elif mid_block_type is None:
            self.mid_block = None
        else:
            raise ValueError(f'unknown mid_block_type : {mid_block_type}')
        self.num_upsamplers = 0
        reversed_block_out_channels = list(reversed(block_out_channels))
        reversed_attention_head_dim = list(reversed(attention_head_dim))
        reversed_layers_per_block = list(reversed(layers_per_block))
        reversed_cross_attention_dim = list(reversed(cross_attention_dim))
        only_cross_attention = list(reversed(only_cross_attention))
        output_channel = reversed_block_out_channels[0]
        for i, up_block_type in enumerate(up_block_types):
            is_final_block = i == len(block_out_channels) - 1
            prev_output_channel = output_channel
            output_channel = reversed_block_out_channels[i]
            input_channel = reversed_block_out_channels[min(i + 1, len(block_out_channels) - 1)]
            if not is_final_block:
                add_upsample = True
                self.num_upsamplers += 1
            else:
                add_upsample = False
            up_block = get_up_block(up_block_type, num_layers=reversed_layers_per_block[i] + 1, in_channels=input_channel, out_channels=output_channel, prev_output_channel=prev_output_channel, temb_channels=blocks_time_embed_dim, add_upsample=add_upsample, resnet_eps=norm_eps, resnet_act_fn=act_fn, resnet_groups=norm_num_groups, cross_attention_dim=reversed_cross_attention_dim[i], attn_num_head_channels=reversed_attention_head_dim[i], dual_cross_attention=dual_cross_attention, use_linear_projection=use_linear_projection, only_cross_attention=only_cross_attention[i], upcast_attention=upcast_attention, resnet_time_scale_shift=resnet_time_scale_shift, resnet_skip_time_act=resnet_skip_time_act, resnet_out_scale_factor=resnet_out_scale_factor, cross_attention_norm=cross_attention_norm)
            self.up_blocks.append(up_block)
            prev_output_channel = output_channel
        if norm_num_groups is not None:
            self.conv_norm_out = nn.GroupNorm(num_channels=block_out_channels[0], num_groups=norm_num_groups, eps=norm_eps)
            self.conv_act = get_activation(act_fn)
        else:
            self.conv_norm_out = None
            self.conv_act = None
        conv_out_padding = (conv_out_kernel - 1) // 2
        self.conv_out = LinearMultiDim(block_out_channels[0], out_channels, kernel_size=conv_out_kernel, padding=conv_out_padding)

    @property
    def attn_processors(self) ->Dict[str, AttentionProcessor]:
        """
        Returns:
            `dict` of attention processors: A dictionary containing all attention processors used in the model with
            indexed by its weight name.
        """
        processors = {}

        def fn_recursive_add_processors(name: 'str', module: 'torch.nn.Module', processors: 'Dict[str, AttentionProcessor]'):
            if hasattr(module, 'set_processor'):
                processors[f'{name}.processor'] = module.processor
            for sub_name, child in module.named_children():
                fn_recursive_add_processors(f'{name}.{sub_name}', child, processors)
            return processors
        for name, module in self.named_children():
            fn_recursive_add_processors(name, module, processors)
        return processors

    def set_attn_processor(self, processor: 'Union[AttentionProcessor, Dict[str, AttentionProcessor]]'):
        """
        Parameters:
            `processor (`dict` of `AttentionProcessor` or `AttentionProcessor`):
                The instantiated processor class or a dictionary of processor classes that will be set as the processor
                of **all** `Attention` layers.
            In case `processor` is a dict, the key needs to define the path to the corresponding cross attention processor. This is strongly recommended when setting trainable attention processors.:

        """
        count = len(self.attn_processors.keys())
        if isinstance(processor, dict) and len(processor) != count:
            raise ValueError(f'A dict of processors was passed, but the number of processors {len(processor)} does not match the number of attention layers: {count}. Please make sure to pass {count} processor classes.')

        def fn_recursive_attn_processor(name: 'str', module: 'torch.nn.Module', processor):
            if hasattr(module, 'set_processor'):
                if not isinstance(processor, dict):
                    module.set_processor(processor)
                else:
                    module.set_processor(processor.pop(f'{name}.processor'))
            for sub_name, child in module.named_children():
                fn_recursive_attn_processor(f'{name}.{sub_name}', child, processor)
        for name, module in self.named_children():
            fn_recursive_attn_processor(name, module, processor)

    def set_default_attn_processor(self):
        """
        Disables custom attention processors and sets the default attention implementation.
        """
        self.set_attn_processor(AttnProcessor())

    def set_attention_slice(self, slice_size):
        """
        Enable sliced attention computation.

        When this option is enabled, the attention module will split the input tensor in slices, to compute attention
        in several steps. This is useful to save some memory in exchange for a small speed decrease.

        Args:
            slice_size (`str` or `int` or `list(int)`, *optional*, defaults to `"auto"`):
                When `"auto"`, halves the input to the attention heads, so attention will be computed in two steps. If
                `"max"`, maximum amount of memory will be saved by running only one slice at a time. If a number is
                provided, uses as many slices as `attention_head_dim // slice_size`. In this case, `attention_head_dim`
                must be a multiple of `slice_size`.
        """
        sliceable_head_dims = []

        def fn_recursive_retrieve_sliceable_dims(module: 'torch.nn.Module'):
            if hasattr(module, 'set_attention_slice'):
                sliceable_head_dims.append(module.sliceable_head_dim)
            for child in module.children():
                fn_recursive_retrieve_sliceable_dims(child)
        for module in self.children():
            fn_recursive_retrieve_sliceable_dims(module)
        num_sliceable_layers = len(sliceable_head_dims)
        if slice_size == 'auto':
            slice_size = [(dim // 2) for dim in sliceable_head_dims]
        elif slice_size == 'max':
            slice_size = num_sliceable_layers * [1]
        slice_size = num_sliceable_layers * [slice_size] if not isinstance(slice_size, list) else slice_size
        if len(slice_size) != len(sliceable_head_dims):
            raise ValueError(f'You have provided {len(slice_size)}, but {self.config} has {len(sliceable_head_dims)} different attention layers. Make sure to match `len(slice_size)` to be {len(sliceable_head_dims)}.')
        for i in range(len(slice_size)):
            size = slice_size[i]
            dim = sliceable_head_dims[i]
            if size is not None and size > dim:
                raise ValueError(f'size {size} has to be smaller or equal to {dim}.')

        def fn_recursive_set_attention_slice(module: 'torch.nn.Module', slice_size: 'List[int]'):
            if hasattr(module, 'set_attention_slice'):
                module.set_attention_slice(slice_size.pop())
            for child in module.children():
                fn_recursive_set_attention_slice(child, slice_size)
        reversed_slice_size = list(reversed(slice_size))
        for module in self.children():
            fn_recursive_set_attention_slice(module, reversed_slice_size)

    def _set_gradient_checkpointing(self, module, value=False):
        if isinstance(module, (CrossAttnDownBlockFlat, DownBlockFlat, CrossAttnUpBlockFlat, UpBlockFlat)):
            module.gradient_checkpointing = value

    def forward(self, sample: 'torch.FloatTensor', timestep: 'Union[torch.Tensor, float, int]', encoder_hidden_states: 'torch.Tensor', class_labels: 'Optional[torch.Tensor]'=None, timestep_cond: 'Optional[torch.Tensor]'=None, attention_mask: 'Optional[torch.Tensor]'=None, cross_attention_kwargs: 'Optional[Dict[str, Any]]'=None, added_cond_kwargs: 'Optional[Dict[str, torch.Tensor]]'=None, down_block_additional_residuals: 'Optional[Tuple[torch.Tensor]]'=None, mid_block_additional_residual: 'Optional[torch.Tensor]'=None, encoder_attention_mask: 'Optional[torch.Tensor]'=None, return_dict: 'bool'=True) ->Union[UNet2DConditionOutput, Tuple]:
        """
        Args:
            sample (`torch.FloatTensor`): (batch, channel, height, width) noisy inputs tensor
            timestep (`torch.FloatTensor` or `float` or `int`): (batch) timesteps
            encoder_hidden_states (`torch.FloatTensor`): (batch, sequence_length, feature_dim) encoder hidden states
            encoder_attention_mask (`torch.Tensor`):
                (batch, sequence_length) cross-attention mask, applied to encoder_hidden_states. True = keep, False =
                discard. Mask will be converted into a bias, which adds large negative values to attention scores
                corresponding to "discard" tokens.
            return_dict (`bool`, *optional*, defaults to `True`):
                Whether or not to return a [`models.unet_2d_condition.UNet2DConditionOutput`] instead of a plain tuple.
            cross_attention_kwargs (`dict`, *optional*):
                A kwargs dictionary that if specified is passed along to the `AttentionProcessor` as defined under
                `self.processor` in
                [diffusers.cross_attention](https://github.com/huggingface/diffusers/blob/main/src/diffusers/models/cross_attention.py).
            added_cond_kwargs (`dict`, *optional*):
                A kwargs dictionary that if specified includes additonal conditions that can be used for additonal time
                embeddings or encoder hidden states projections. See the configurations `encoder_hid_dim_type` and
                `addition_embed_type` for more information.

        Returns:
            [`~models.unet_2d_condition.UNet2DConditionOutput`] or `tuple`:
            [`~models.unet_2d_condition.UNet2DConditionOutput`] if `return_dict` is True, otherwise a `tuple`. When
            returning a tuple, the first element is the sample tensor.
        """
        default_overall_up_factor = 2 ** self.num_upsamplers
        forward_upsample_size = False
        upsample_size = None
        if any(s % default_overall_up_factor != 0 for s in sample.shape[-2:]):
            logger.info('Forward upsample size to force interpolation output size.')
            forward_upsample_size = True
        if attention_mask is not None:
            attention_mask = (1 - attention_mask) * -10000.0
            attention_mask = attention_mask.unsqueeze(1)
        if encoder_attention_mask is not None:
            encoder_attention_mask = (1 - encoder_attention_mask) * -10000.0
            encoder_attention_mask = encoder_attention_mask.unsqueeze(1)
        if self.config.center_input_sample:
            sample = 2 * sample - 1.0
        timesteps = timestep
        if not torch.is_tensor(timesteps):
            is_mps = sample.device.type == 'mps'
            if isinstance(timestep, float):
                dtype = torch.float32 if is_mps else torch.float64
            else:
                dtype = torch.int32 if is_mps else torch.int64
            timesteps = torch.tensor([timesteps], dtype=dtype, device=sample.device)
        elif len(timesteps.shape) == 0:
            timesteps = timesteps[None]
        timesteps = timesteps.expand(sample.shape[0])
        t_emb = self.time_proj(timesteps)
        t_emb = t_emb
        emb = self.time_embedding(t_emb, timestep_cond)
        if self.class_embedding is not None:
            if class_labels is None:
                raise ValueError('class_labels should be provided when num_class_embeds > 0')
            if self.config.class_embed_type == 'timestep':
                class_labels = self.time_proj(class_labels)
                class_labels = class_labels
            class_emb = self.class_embedding(class_labels)
            if self.config.class_embeddings_concat:
                emb = torch.cat([emb, class_emb], dim=-1)
            else:
                emb = emb + class_emb
        if self.config.addition_embed_type == 'text':
            aug_emb = self.add_embedding(encoder_hidden_states)
            emb = emb + aug_emb
        elif self.config.addition_embed_type == 'text_image':
            if 'image_embeds' not in added_cond_kwargs:
                raise ValueError(f"{self.__class__} has the config param `addition_embed_type` set to 'text_image' which requires the keyword argument `image_embeds` to be passed in `added_cond_kwargs`")
            image_embs = added_cond_kwargs.get('image_embeds')
            text_embs = added_cond_kwargs.get('text_embeds', encoder_hidden_states)
            aug_emb = self.add_embedding(text_embs, image_embs)
            emb = emb + aug_emb
        if self.time_embed_act is not None:
            emb = self.time_embed_act(emb)
        if self.encoder_hid_proj is not None and self.config.encoder_hid_dim_type == 'text_proj':
            encoder_hidden_states = self.encoder_hid_proj(encoder_hidden_states)
        elif self.encoder_hid_proj is not None and self.config.encoder_hid_dim_type == 'text_image_proj':
            if 'image_embeds' not in added_cond_kwargs:
                raise ValueError(f"{self.__class__} has the config param `encoder_hid_dim_type` set to 'text_image_proj' which requires the keyword argument `image_embeds` to be passed in  `added_conditions`")
            image_embeds = added_cond_kwargs.get('image_embeds')
            encoder_hidden_states = self.encoder_hid_proj(encoder_hidden_states, image_embeds)
        sample = self.conv_in(sample)
        down_block_res_samples = sample,
        for downsample_block in self.down_blocks:
            if hasattr(downsample_block, 'has_cross_attention') and downsample_block.has_cross_attention:
                sample, res_samples = downsample_block(hidden_states=sample, temb=emb, encoder_hidden_states=encoder_hidden_states, attention_mask=attention_mask, cross_attention_kwargs=cross_attention_kwargs, encoder_attention_mask=encoder_attention_mask)
            else:
                sample, res_samples = downsample_block(hidden_states=sample, temb=emb)
            down_block_res_samples += res_samples
        if down_block_additional_residuals is not None:
            new_down_block_res_samples = ()
            for down_block_res_sample, down_block_additional_residual in zip(down_block_res_samples, down_block_additional_residuals):
                down_block_res_sample = down_block_res_sample + down_block_additional_residual
                new_down_block_res_samples = new_down_block_res_samples + (down_block_res_sample,)
            down_block_res_samples = new_down_block_res_samples
        if self.mid_block is not None:
            sample = self.mid_block(sample, emb, encoder_hidden_states=encoder_hidden_states, attention_mask=attention_mask, cross_attention_kwargs=cross_attention_kwargs, encoder_attention_mask=encoder_attention_mask)
        if mid_block_additional_residual is not None:
            sample = sample + mid_block_additional_residual
        for i, upsample_block in enumerate(self.up_blocks):
            is_final_block = i == len(self.up_blocks) - 1
            res_samples = down_block_res_samples[-len(upsample_block.resnets):]
            down_block_res_samples = down_block_res_samples[:-len(upsample_block.resnets)]
            if not is_final_block and forward_upsample_size:
                upsample_size = down_block_res_samples[-1].shape[2:]
            if hasattr(upsample_block, 'has_cross_attention') and upsample_block.has_cross_attention:
                sample = upsample_block(hidden_states=sample, temb=emb, res_hidden_states_tuple=res_samples, encoder_hidden_states=encoder_hidden_states, cross_attention_kwargs=cross_attention_kwargs, upsample_size=upsample_size, attention_mask=attention_mask, encoder_attention_mask=encoder_attention_mask)
            else:
                sample = upsample_block(hidden_states=sample, temb=emb, res_hidden_states_tuple=res_samples, upsample_size=upsample_size)
        if self.conv_norm_out:
            sample = self.conv_norm_out(sample)
            sample = self.conv_act(sample)
        sample = self.conv_out(sample)
        if not return_dict:
            return sample,
        return UNet2DConditionOutput(sample=sample)


class LearnedClassifierFreeSamplingEmbeddings(ModelMixin, ConfigMixin):
    """
    Utility class for storing learned text embeddings for classifier free sampling
    """

    @register_to_config
    def __init__(self, learnable: 'bool', hidden_size: 'Optional[int]'=None, length: 'Optional[int]'=None):
        super().__init__()
        self.learnable = learnable
        if self.learnable:
            assert hidden_size is not None, 'learnable=True requires `hidden_size` to be set'
            assert length is not None, 'learnable=True requires `length` to be set'
            embeddings = torch.zeros(length, hidden_size)
        else:
            embeddings = None
        self.embeddings = torch.nn.Parameter(embeddings)


class CustomEncoder(ModelMixin, ConfigMixin):

    def __init__(self):
        super().__init__()


test_config = [{'reversible': False, 'block_type': 'encoder', 'num_layers': 2, 'dim_model': 768, 'residual_norm_style': 'pre', 'multi_head_config': {'num_heads': 12, 'residual_dropout': 0.1, 'use_rotary_embeddings': True, 'attention': {'name': 'scaled_dot_product', 'dropout': 0.1, 'causal': False}}, 'feedforward_config': {'name': 'FusedMLP', 'dropout': 0.1, 'activation': 'gelu', 'hidden_layer_multiplier': 4}}]


class ResidualNormStyle(str, Enum):
    """Support different residual path and norm styles.
    See "On Layer Normalization in the Transformer Architecture",
    Xiong et al., https://arxiv.org/pdf/2002.04745v1.pdf
    """
    Pre = 'pre'
    Post = 'post'
    DeepNorm = 'deepnorm'


DeepNormCoefficients = namedtuple('DeepNormCoefficients', ['alpha', 'beta'])


def get_deepnorm_coefficients(encoder_layers: 'int', decoder_layers: 'int') ->Tuple[Optional[DeepNormCoefficients], Optional[DeepNormCoefficients]]:
    """
    See DeepNet_.

    Returns alpha and beta depending on the number of encoder and decoder layers,
    first tuple is for the encoder and second for the decoder

    .. _DeepNet: https://arxiv.org/pdf/2203.00555v1.pdf
    """
    N = encoder_layers
    M = decoder_layers
    if decoder_layers == 0:
        return DeepNormCoefficients(alpha=(2 * N) ** 0.25, beta=(8 * N) ** -0.25), None
    elif encoder_layers == 0:
        return None, DeepNormCoefficients(alpha=(2 * M) ** 0.25, beta=(8 * M) ** -0.25)
    else:
        encoder_coeffs = DeepNormCoefficients(alpha=0.81 * (N ** 4 * M) ** 0.0625, beta=0.87 * (N ** 4 * M) ** -0.0625)
        decoder_coeffs = DeepNormCoefficients(alpha=(3 * M) ** 0.25, beta=(12 * M) ** -0.25)
        return encoder_coeffs, decoder_coeffs


def _lecun_normal(tensor, gain=1.0):
    fan_in, _ = _calculate_fan_in_and_fan_out(tensor)
    denom = fan_in
    variance = gain / denom
    _no_grad_trunc_normal_(tensor, mean=0.0, std=math.sqrt(variance) / 0.8796256610342398, a=-2.0, b=2.0)


def _maybe_init_tensor(module: 'nn.Module', attr: 'str', distribution_: 'Callable', **kwargs):
    if hasattr(module, attr):
        maybe_tensor = getattr(module, attr)
        if maybe_tensor is not None and isinstance(maybe_tensor, torch.Tensor):
            distribution_(maybe_tensor, **kwargs)


_assert_if_not_initialized = False


def _maybe_report_no_init(module, name):
    if len(list(module.named_children())) == 0 and (hasattr(module, 'weight') or hasattr(module, 'bias')):
        if isinstance(module, torch.nn.LayerNorm):
            return
        if isinstance(module, torch.nn.Embedding):
            return
        logger.warning(f'Not initializing weights in {name}, this could be a mistake.\nModule {module}')
        if _assert_if_not_initialized:
            assert False, f'Uninitialized weight found in {module}.' + ' If you have a custom module, please provide a `init_weights()` method'


def _small_init_(tensor: 'torch.Tensor', gain: 'float'=1.0) ->torch.Tensor:
    """Fills the input `Tensor` with values according to the method
    described in `Transformer Without Tears`_, using a uniform distribution.

    This is a variation of the Xavier init. The resulting tensor will have values sampled from
    :math:`\\mathcal{U}(-a, a)` where

    .. math::
        a = \\text{gain} \\times \\sqrt{\\frac{6}{\\text{fan\\_in} + 4 * \\text{fan\\_out}}}

    Also known as Glorot initialization.

    Args:
        tensor: an n-dimensional `torch.Tensor`
        gain: an optional scaling factor

    .. _`Transformer Without Tears`: https://arxiv.org/abs/1910.05895

    """
    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)
    std = gain * math.sqrt(2.0 / float(fan_in + 4 * fan_out))
    a = math.sqrt(3.0) * std
    return _no_grad_uniform_(tensor, -a, a)


def is_ffn(n):
    return 'feedforward' in n or 'wrap_ff' in n and not n.endswith('norm')


def is_mha_input_projection(n):
    return 'q_proj' in n or 'k_proj' in n or 'v_proj' in n


def _init_weights_small(module: 'nn.Module', name: 'str'='', head_bias: 'float'=0.0, gain: 'float'=1.0, deepnorm_style: 'bool'=False, **kwargs):
    """Follow the `Transformer Without Tears`_ initialization for self-attention"""
    if is_ffn(name):
        _maybe_init_tensor(module, 'weight', torch.nn.init.xavier_uniform_, gain=gain)
        _maybe_init_tensor(module, 'bias', nn.init.normal_, std=1e-06)
    elif is_mha_input_projection(name) or isinstance(module, nn.Linear):
        if deepnorm_style and ('q_proj' in name.split('.') or 'k_proj' in name.split('.')):
            gain = 1.0
        _maybe_init_tensor(module, 'weight', _small_init_, gain=gain)
        _maybe_init_tensor(module, 'bias', nn.init.zeros_)
    elif isinstance(module, nn.Conv2d):
        _maybe_init_tensor(module, 'weight', _lecun_normal)
        _maybe_init_tensor(module, 'bias', nn.init.zeros_)
    elif hasattr(module, 'init_weights'):
        module.init_weights()
    else:
        _maybe_report_no_init(module, name)
    if not hasattr(module, 'init_weights'):
        for child_name, child_module in module.named_children():
            _init_weights_small(child_module, f'{name}.{child_name}', head_bias, gain)


def _init_weights_vit_jax(module: 'nn.Module', name: 'str'='', head_bias: 'float'=0.0, gain: 'float'=1.0, deepnorm_style: 'bool'=False, **kwargs):
    """ViT weight initialization, matching JAX (Flax) impl"""
    if is_ffn(name):
        _maybe_init_tensor(module, 'bias', nn.init.normal_, std=1e-06)
        _maybe_init_tensor(module, 'weight', torch.nn.init.xavier_uniform_, gain=gain)
    elif is_mha_input_projection(name) or isinstance(module, nn.Linear):
        if deepnorm_style and ('q_proj' in name.split('.') or 'k_proj' in name.split('.')):
            gain = 1.0
        _maybe_init_tensor(module, 'weight', torch.nn.init.xavier_uniform_, gain=gain)
        _maybe_init_tensor(module, 'bias', nn.init.zeros_)
    elif isinstance(module, nn.Conv2d):
        _maybe_init_tensor(module, 'weight', _lecun_normal, gain=gain)
        _maybe_init_tensor(module, 'bias', nn.init.zeros_)
    elif hasattr(module, 'init_weights'):
        module.init_weights()
    else:
        _maybe_report_no_init(module, name)
    if not hasattr(module, 'init_weights'):
        for child_name, child_module in module.named_children():
            _init_weights_vit_jax(child_module, f'{name}.{child_name}', head_bias, gain)


def _init_weights_vit_moco(module: 'nn.Module', name: 'str'='', gain: 'float'=1.0, **kwargs):
    """ViT weight initialization, matching moco-v3 impl minus fixed PatchEmbed"""
    assert 'deepnorm_style' not in kwargs.keys(), 'This initialization method does not support deepnorm'
    if is_ffn(name):
        _maybe_init_tensor(module, 'weight', torch.nn.init.xavier_uniform_, gain=gain)
        _maybe_init_tensor(module, 'bias', nn.init.zeros_)
    elif is_mha_input_projection(name) or isinstance(module, nn.Linear):
        if isinstance(module.weight, torch.Tensor):
            val = math.sqrt(6.0 / float(module.weight.shape[0] + module.weight.shape[1])) * gain
            _maybe_init_tensor(module, 'weight', nn.init.uniform_, a=-val, b=val)
        _maybe_init_tensor(module, 'bias', nn.init.zeros_)
    elif hasattr(module, 'init_weights'):
        module.init_weights(gain=gain)
    else:
        _maybe_report_no_init(module, name)
    if not hasattr(module, 'init_weights'):
        for child_name, child_module in module.named_children():
            _init_weights_vit_moco(child_module, child_name, gain)


def _init_weights_vit_timm(module: 'nn.Module', name: 'str'='', gain: 'float'=1.0, deepnorm_style: 'bool'=False, **kwargs):
    """
    ViT weight initialization, original timm impl (for reproducibility).

    See DeepNet_ for all the DeepNorm specific codepaths
    """
    if isinstance(module, nn.Linear):
        if deepnorm_style and ('q_proj' in name.split('.') or 'k_proj' in name.split('.')):
            gain = 1
        std = 0.02 * gain
        a = math.sqrt(3.0) * std
        _maybe_init_tensor(module, 'weight', _no_grad_trunc_normal_, mean=0.0, std=std, a=-a, b=a)
        _maybe_init_tensor(module, 'bias', nn.init.zeros_)
    elif hasattr(module, 'init_weights'):
        module.init_weights(gain=gain)
    else:
        _maybe_report_no_init(module, name)
    if not hasattr(module, 'init_weights'):
        for child_name, child_module in module.named_children():
            _init_weights_vit_timm(child_module, child_name, gain)


class xFormerWeightInit(str, Enum):
    Timm = 'timm'
    ViT = 'vit'
    Moco = 'moco'
    Small = 'small'


def get_weight_init_fn(init_choice: 'xFormerWeightInit'):
    """
    Provide the xFormers factory with weight init routines.

    Supported initializations are:
    - Small: follow the method outlined in `Transformer Without Tears`_
    - ViT: follow the initialization in the reference ViT_ codebase
    - Timm: follow the initialization in the reference Timm_ codebase
    - Moco: follow the initialization in the reference MocoV3_ codebase

    .. _ViT: https://github.com/google-research/vision_transformer
    .. _Timm: https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py
    .. _MocoV3: https://github.com/facebookresearch/moco-v3
    """
    return {xFormerWeightInit.Timm: _init_weights_vit_timm, xFormerWeightInit.ViT: _init_weights_vit_jax, xFormerWeightInit.Moco: _init_weights_vit_moco, xFormerWeightInit.Small: _init_weights_small}[init_choice]


class NormalizationType(str, Enum):
    LayerNorm = 'layernorm'
    Skip = 'skip'


_triton_layernorm_fp16_enabled = False


class _LayerNorm(torch.autograd.Function):

    @staticmethod
    @custom_fwd(cast_inputs=torch.float16 if _triton_layernorm_fp16_enabled else None)
    def forward(ctx, x, weight, bias, eps):
        if x.dtype == torch.float16:
            eps = max(eps, 1.6e-05)
        y = torch.empty_like(x)
        x_arg = x.reshape(-1, x.shape[-1])
        M, N = x_arg.shape
        mean = torch.empty((M,), dtype=torch.float32, device='cuda')
        rstd = torch.empty((M,), dtype=torch.float32, device='cuda')
        MAX_FUSED_SIZE = 65536 // x.element_size()
        BLOCK_SIZE_N = min(MAX_FUSED_SIZE, triton.next_power_of_2(N))
        if N > BLOCK_SIZE_N:
            raise RuntimeError("This layer norm doesn't support feature dim >= 64KB.")
        if not x_arg.is_contiguous() or not y.is_contiguous():
            global _triton_registered_warnings
            if not _triton_registered_warnings:
                logger.warning('Non-contiguous input tensor found. Making it contiguous,' + ' but could have perf or trainer implications')
                _triton_registered_warnings = True
            x_arg = x_arg.contiguous()
            y = y.contiguous()
        num_warps = min(max(BLOCK_SIZE_N // 256, 1), 16)
        layer_norm_fw[M,](x_arg, y, weight, bias, mean, rstd, x_arg.stride(0), N, eps, num_warps=num_warps, BLOCK_SIZE_N=BLOCK_SIZE_N, affine=weight is not None)
        ctx.save_for_backward(x, mean, rstd, weight)
        ctx.BLOCK_SIZE_N = BLOCK_SIZE_N
        ctx.num_warps = num_warps
        return y.reshape_as(x)

    @staticmethod
    @custom_bwd
    def backward(ctx, dy):
        x, mean, rstd, weight = ctx.saved_tensors
        x = x.reshape(-1, x.size(-1))
        M, N = x.size()
        GROUP_SIZE_M = 32
        if N <= 8192:
            GROUP_SIZE_M = 64
        if N <= 4096:
            GROUP_SIZE_M = 96
        if N <= 2048:
            GROUP_SIZE_M = 128
        if N <= 1024:
            GROUP_SIZE_M = 256
        if dy.dtype == torch.float32:
            GROUP_SIZE_M = GROUP_SIZE_M // 2
        locks = torch.zeros(2 * GROUP_SIZE_M, dtype=torch.int32, device='cuda')
        t_args = {'dtype': x.dtype, 'device': x.device}
        _dw = torch.empty((GROUP_SIZE_M, x.size(-1)), **t_args)
        _db = torch.empty_like(_dw)
        dw = torch.empty((x.size(-1),), **t_args)
        db = torch.empty_like(dw)
        dy = dy.contiguous()
        dx = torch.empty_like(dy)
        assert dy.numel() == x.numel(), 'Something is wrong in the backward graph, possibly because of an inplace operation after the layernorm'
        num_warps = min(max(ctx.BLOCK_SIZE_N // 256, 1), 16)
        layer_norm_bwd_dx_fused[M,](dx, dy, _dw, _db, x, weight if weight is not None else x, mean, rstd, locks, x.stride(0), N, affine=weight is not None, GROUP_SIZE_M=GROUP_SIZE_M, BLOCK_SIZE_N=ctx.BLOCK_SIZE_N, num_warps=num_warps)

        def grid(meta):
            return [triton.cdiv(N, meta['BLOCK_SIZE_N'])]
        layer_norm_bwd_dwdb[grid](_dw, _db, dw, db, GROUP_SIZE_M, N, BLOCK_SIZE_M=32, BLOCK_SIZE_N=64)
        dx = dx.reshape_as(dy)
        return dx, dw, db, None


def layer_norm(x: 'torch.Tensor', weight: 'Optional[torch.Tensor]'=None, bias: 'Optional[torch.Tensor]'=None, eps: 'float'=1e-06) ->torch.Tensor:
    global _triton_registered_warnings
    """Applies normalization over a mini batch of inputs"""
    try:
        if not _triton_registered_warnings and torch.cuda.is_available() and x.is_cuda and weight is not None and bias is not None:
            return _LayerNorm.apply(x, weight, bias, eps)
    except RuntimeError as e:
        _triton_registered_warnings = True
        logger.warning('Triton layernorm kernel register spillover or invalid image caught. Deactivating this kernel, please file an issue in the xFormers repository')
        logger.warning(e)
    return torch.nn.functional.layer_norm(x, [x.shape[-1]], weight=weight, bias=bias, eps=eps)


class FusedLayerNorm(nn.Module):
    """
    Handle a layer normalization, like torch.nn.LayerNorm_.

    This implementation should be measurably faster than the default PyTorch layernorm (as of PyTorch 1.9),
    both for training and inference worloads.

    .. NOTE: Computations under Torch AMP are kept as float32 by default, one can change this to be float16
        by setting the flag `xformers.triton.k_layer_norm._triton_layernorm_fp16_enabled = True`

    .. _torch.nn.LayerNorm: https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html

    """

    def __init__(self, normalized_shape, affine=True, eps=1e-06):
        super().__init__()
        if affine:
            self.weight = nn.Parameter(torch.ones(normalized_shape))
            self.bias = nn.Parameter(torch.zeros(normalized_shape))
        else:
            self.weight = self.bias = None
        self.epsilon = eps

    def forward(self, x):
        return layer_norm(x, self.weight, self.bias, self.epsilon)

    def init_weights(self, *args, **kwargs):
        with torch.no_grad():
            if self.weight is not None:
                self.weight.fill_(1.0)
            if self.bias is not None:
                self.bias.fill_(0.0)


class RequiresWrappedInputs:
    """Used to mark, through inheritance,
    the fact that this class will require inputs to be passed as a single list"""
    pass


def compute_once(func):
    value = None

    def func_wrapper():
        nonlocal value
        if value is None:
            value = func()
        return value
    return func_wrapper


@compute_once
def _is_triton_available():
    if not torch.cuda.is_available():
        return False
    if os.environ.get('XFORMERS_FORCE_DISABLE_TRITON', '0') == '1':
        return False
    try:
        return True
    except (ImportError, AttributeError) as e:
        logger.warning(f'A matching Triton is not available, some optimizations will not be enabled.\nError caught was: {e}')
        return False


def get_normalization_layer(normalization_type: 'NormalizationType'):


    class Skip(nn.Module):

        def __init__(self, *_, **__) ->None:
            super().__init__()

        def forward(self, x: 'torch.Tensor', **_):
            return x
    return {NormalizationType.LayerNorm: nn.LayerNorm, NormalizationType.Skip: Skip}[normalization_type]


class PostNorm(nn.Module, RequiresWrappedInputs):
    """Adds LayerNorm after computing attention"""

    def __init__(self, d_norm: 'int', sublayer: 'nn.Module', normalization: 'NormalizationType', use_triton: 'bool'=True):
        super().__init__()
        if _is_triton_available() and use_triton and normalization == NormalizationType.LayerNorm:
            self.norm: 'Union[nn.LayerNorm, FusedLayerNorm]' = FusedLayerNorm(d_norm)
        else:
            self.norm = get_normalization_layer(normalization)(d_norm)
        self.sublayer = sublayer
        self.wrap_inputs = isinstance(sublayer, RequiresWrappedInputs)

    def forward(self, inputs: 'List[torch.Tensor]', **kwargs):
        if self.wrap_inputs:
            x = self.sublayer(inputs=inputs, **kwargs)
        else:
            x = self.sublayer(*inputs, **kwargs)
        return self.norm(x)


class PreNorm(nn.Module, RequiresWrappedInputs):
    """Adds a normalization before computing attention

    ..Note: If a list of inputs is passed, all of them get normalized"""

    def __init__(self, d_norm: 'int', sublayer: 'nn.Module', normalization: 'NormalizationType', use_triton: 'bool'=True):
        super().__init__()
        if _is_triton_available() and use_triton and normalization == NormalizationType.LayerNorm:
            self.norm: 'Union[nn.LayerNorm, FusedLayerNorm]' = FusedLayerNorm(d_norm)
        else:
            self.norm = get_normalization_layer(normalization)(d_norm)
        self.sublayer = sublayer
        self.wrap_inputs = isinstance(sublayer, RequiresWrappedInputs)

    def forward(self, inputs: 'List[torch.Tensor]', **kwargs):
        assert len(inputs) > 0
        ids = [id(x) for x in inputs]
        if ids.count(ids[0]) == len(ids):
            x_norm = self.norm(inputs[0])
            inputs_normed = [x_norm for _ in inputs]
        else:
            inputs_normed = [self.norm(x_) for x_ in inputs]
        if self.wrap_inputs:
            return self.sublayer(inputs=inputs_normed, **kwargs)
        else:
            return self.sublayer(*inputs_normed, **kwargs)


class Residual(nn.Module, RequiresWrappedInputs):
    """
    Object-oriented handling of the residual path

    This supports scaling of the residual path, as proposed by DeepNet_
    .. _DeepNet: https://arxiv.org/pdf/2203.00555v1.pdf

    .. Note: the wrapped layers must accept all the inputs as a single list
    """

    def __init__(self, layer: 'nn.Module', scale: 'Optional[float]'=None):
        super().__init__()
        self.layer = layer
        self.scale = scale
        self.wrap_inputs = isinstance(layer, RequiresWrappedInputs)

    def forward(self, inputs: 'List[torch.Tensor]', **kwargs):
        if self.scale is not None:
            residue = inputs[0] * self.scale
        else:
            residue = inputs[0]
        if self.wrap_inputs:
            return residue + self.layer(inputs=inputs, **kwargs)
        else:
            return residue + self.layer(*inputs, **kwargs)


def _get_ln_factory(d_model: 'int', residual_norm_style: 'Optional[ResidualNormStyle]', use_triton: 'bool', residual: 'bool', normalization: 'NormalizationType'=NormalizationType.LayerNorm, residual_scale: 'float'=1.0):
    """
    Handle all the supported residual path configurations.

    ..Note: we return the appropriate constructor, not an actual layer
    """

    def get_layer_wrapper(d_model: 'int', sublayer: 'nn.Module', residual_norm_style: 'Optional[ResidualNormStyle]', residual: 'bool', residual_scale: 'float'):
        if residual:
            if residual_norm_style == ResidualNormStyle.Pre:
                return Residual(layer=PreNorm(d_model, sublayer, normalization, use_triton), scale=None)
            elif residual_norm_style == ResidualNormStyle.Post:
                return PostNorm(d_model, Residual(layer=sublayer, scale=None), normalization, use_triton)
            elif residual_norm_style == ResidualNormStyle.DeepNorm:
                return PostNorm(d_model, Residual(layer=sublayer, scale=residual_scale), normalization, use_triton=use_triton)
            else:
                raise ValueError
        return PreNorm(d_model, sublayer, normalization, use_triton) if residual_norm_style == ResidualNormStyle.Pre else PostNorm(d_model, sublayer, normalization, use_triton)

    def ln_factory(sublayer: 'nn.Module'):
        return get_layer_wrapper(d_model, sublayer, residual_norm_style, residual, residual_scale)
    return ln_factory


def generate_matching_config(superset: 'Dict[str, Any]', config_class: 'Any') ->Any:
    """Given a superset of the inputs and a reference config class,
    return exactly the needed config"""
    field_names = list(map(lambda x: x.name, fields(config_class)))
    subset = {k: v for k, v in superset.items() if k in field_names}
    for k in field_names:
        if k not in subset.keys():
            subset[k] = None
    return config_class(**subset)


def build_feedforward(config: 'Union[Dict[str, Any], FeedforwardConfig]'):
    """Builds a feedforward from a config.

    This assumes a 'name' key in the config which is used to determine what
    attention class to instantiate. For instance, a config `{"name": "my_feedforward",
    "foo": "bar"}` will find a class that was registered as "my_feedforward"
    (see :func:`register_feedforward`) and call .from_config on it."""
    if not isinstance(config, FeedforwardConfig):
        config_instance = generate_matching_config(config, FEEDFORWARD_REGISTRY[config['name']].config)
    else:
        config_instance = config
    return FEEDFORWARD_REGISTRY[config_instance.name].constructor.from_config(config_instance)


class InputProjection(nn.Module):
    """
    Handle all the input projections in one go, opportunistically fuse some operations.
    """

    def __init__(self, query_proj_params: 'InputProjectionConfig', key_proj_params: 'Optional[InputProjectionConfig]', value_proj_params: 'Optional[InputProjectionConfig]', use_separate_proj_weight: 'bool'=True):
        super().__init__()
        self.out_features = query_proj_params.out_features
        self.q_proj = nn.Linear(query_proj_params.in_features, query_proj_params.out_features, query_proj_params.bias)
        if key_proj_params is not None:
            self.k_proj = nn.Linear(key_proj_params.in_features, key_proj_params.out_features, key_proj_params.bias)
        else:
            logger.info('No Key projection parameters were passed, assuming that the weights' + ' are shared with the query projection')
            self.k_proj = self.q_proj
        if value_proj_params is not None:
            self.v_proj = nn.Linear(value_proj_params.in_features, value_proj_params.out_features, value_proj_params.bias)
        else:
            logger.info('No Value projection parameters were passed, assuming that the weights' + ' are shared with the query projection')
            self.v_proj = self.q_proj
        if not use_separate_proj_weight:
            with torch.no_grad():
                self.k_proj.weight = self.q_proj.weight
                self.v_proj.weight = self.q_proj.weight

    def forward(self, query: 'torch.Tensor', key: 'torch.Tensor', value: 'torch.Tensor') ->Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        q, k, v = map(lambda fn, x: fn(x), [self.q_proj, self.k_proj, self.v_proj], [query, key, value])
        return q, k, v


def rotate_half(x):
    x1, x2 = x.chunk(2, dim=-1)
    return torch.cat((-x2, x1), dim=-1)


@torch.jit.script
def apply_rotary_pos_emb(x, cos, sin):
    cos = cos[:, :, :x.shape[-2], :]
    sin = sin[:, :, :x.shape[-2], :]
    return x * cos + rotate_half(x) * sin


class RotaryEmbedding(torch.nn.Module):
    """
    The rotary position embeddings from RoFormer_ (Su et. al).
    A crucial insight from the method is that the query and keys are
    transformed by rotation matrices which depend on the relative positions.

    Other implementations are available in the Rotary Transformer repo_ and in
    GPT-NeoX_, GPT-NeoX was an inspiration

    .. _RoFormer: https://arxiv.org/abs/2104.09864
    .. _repo: https://github.com/ZhuiyiTechnology/roformer
    .. _GPT-NeoX: https://github.com/EleutherAI/gpt-neox


    .. warning: Please note that this embedding is not registered on purpose, as it is transformative
        (it does not create the embedding dimension) and will likely be picked up (imported) on a ad-hoc basis
    """

    def __init__(self, dim_model: 'int', *_, **__):
        super().__init__()
        inv_freq = 1.0 / 10000 ** (torch.arange(0, dim_model, 2).float() / dim_model)
        self.register_buffer('inv_freq', inv_freq)
        self._seq_len_cached = None
        self._cos_cached = None
        self._sin_cached = None

    def _update_cos_sin_tables(self, x, seq_dimension=1):
        seq_len = x.shape[seq_dimension]
        if seq_len != self._seq_len_cached or self._cos_cached.device != x.device or self._cos_cached.dtype != x.dtype:
            self._seq_len_cached = seq_len
            t = torch.arange(x.shape[seq_dimension], device=x.device, dtype=torch.float32)
            freqs = torch.einsum('i,j->ij', t, self.inv_freq)
            emb = torch.cat((freqs, freqs), dim=-1)
            self._cos_cached = emb.cos()[None, None, :, :]
            self._sin_cached = emb.sin()[None, None, :, :]
        return self._cos_cached, self._sin_cached

    def forward(self, q: 'torch.Tensor', k: 'torch.Tensor') ->Tuple[torch.Tensor, torch.Tensor]:
        self._cos_cached, self._sin_cached = self._update_cos_sin_tables(k, seq_dimension=-2)
        return apply_rotary_pos_emb(q, self._cos_cached, self._sin_cached), apply_rotary_pos_emb(k, self._cos_cached, self._sin_cached)


def _fold_heads(t: 'torch.Tensor', B: 'int', S: 'int', H: 'int', Hs: 'int'):
    return t.view(B, S, H, Hs).transpose(1, 2).flatten(start_dim=0, end_dim=1)


def _split_heads(t: 'torch.Tensor', B: 'int', S: 'int', H: 'int', Hs: 'int'):
    return t.view(B, S, H, Hs).transpose(1, 2)


class MultiHeadDispatch(nn.Module):
    """
    A multi-head masked self-attention dispatch mechanism, with a projection at the end,
    following the architecture proposed in `Attention is all you need`_, Vaswani et al.

    The actual attention mechanism can vary, as well as the projections.
    This can be used to wrap the proposed attention mechanisms and make them multi-head aware,
    but it is optional.

    Args:
        dim_model: The model/embedding dimension
        num_heads: The number of heads being used
        attention: The attention mechanism (needs to be registered to the xformers library)
        bias: Whether to use bias for the projections : (Q, K, V, Output)
        residual_dropout: Amount of dropout on the residual path
        use_separate_proj_weight: Use different weights for the Q, K, V projections
        dim_key: Optionally use a different dimension for the key
        dim_value:  Optionally use a different dimension for the value
        in_proj_container: Optionally provide the input projection module
        use_rotary_embeddings: Use rotary embeddings
        out_proj: Optionally provide the output projection module


    .. _`Attention is all you need`: https://arxiv.org/abs/1706.03762v5
    """

    def __init__(self, dim_model: 'int', num_heads: 'int', attention: 'Attention', bias: 'Tuple[bool, bool, bool, bool]'=(True, True, True, True), residual_dropout: 'float'=0.0, use_separate_proj_weight: 'bool'=True, dim_key: 'Optional[int]'=None, dim_value: 'Optional[int]'=None, in_proj_container: 'Optional[InputProjection]'=None, use_rotary_embeddings: 'Optional[bool]'=False, out_proj: 'Optional[nn.Module]'=None, *args, **kwargs):
        super().__init__()
        if isinstance(bias, bool):
            logger.warning('Single bias value provided for the MHA projections.' + f' Assuming the same parameter ({bias}) is to be used everywhere')
            bias = bias, bias, bias, bias
        assert dim_model % num_heads == 0
        assert num_heads > 0
        dim_key, dim_value = map(lambda x: x if x else dim_model, (dim_key, dim_value))
        self.num_heads = num_heads
        self.dim_key_head = dim_key // num_heads
        self.dim_value_head = dim_value // num_heads
        self.dim_model = dim_model
        self.attention = attention
        if attention.requires_input_projection:
            self.in_proj_container = in_proj_container if in_proj_container is not None else InputProjection(query_proj_params=InputProjectionConfig(dim_model, dim_key, bias=bias[0]), key_proj_params=InputProjectionConfig(dim_model, dim_key, bias=bias[1]), value_proj_params=InputProjectionConfig(dim_model, dim_value, bias=bias[2]), use_separate_proj_weight=use_separate_proj_weight)
        self.rotary_embeddings = RotaryEmbedding(self.dim_key_head) if use_rotary_embeddings else None
        self.resid_drop = nn.Dropout(residual_dropout, inplace=False)
        self.proj = out_proj if out_proj else nn.Linear(dim_model, dim_model, bias=bias[3])
        if isinstance(self.proj, nn.Linear) and self.proj.bias is not None:
            constant_(self.proj.bias, 0.0)

    def forward(self, query: 'torch.Tensor', key: 'Optional[torch.Tensor]'=None, value: 'Optional[torch.Tensor]'=None, att_mask: 'Optional[torch.Tensor]'=None, key_padding_mask: 'Optional[torch.Tensor]'=None) ->torch.Tensor:
        """
        Expected input dimensions are [batch size, sequence length, embed dim]
        Output dimensions are [batch size, sequence length, embed dim]
        """
        if key is None:
            key = query
        if value is None:
            value = query
        if query.shape[0] != key.shape[0] or query.shape[0] != value.shape[0]:
            max_batch = max((query.shape[0], key.shape[0], value.shape[0]))
            query, key, value = map(lambda x: x.expand(max_batch, -1, -1), [query, key, value])
        B, S_Q, _ = query.size()
        _, S_K, _ = key.size()
        if S_Q != S_K:
            assert not self.attention.requires_same_k_q_dimensions, 'This attention mechanism requires query and key to have the same sequence (context) lengths'
            if hasattr(self.attention, 'causal'):
                assert not self.attention.causal, 'Causal attention is not supported when key and query have different sequence lengths.\n' + 'In that case causality is ill-determined. Please pad your sequences accordingly'
        kw_mask_args = {}
        if att_mask is not None:
            assert self.attention.supports_attention_mask, 'This attention does not support attention masks'
            kw_mask_args['att_mask'] = att_mask
        if key_padding_mask is not None:
            assert self.attention.supports_key_padding_mask, 'This attention does not support key padding masks'
            kw_mask_args['key_padding_mask'] = key_padding_mask
        if self.attention.requires_skip_multi_head:
            return self.attention(query, key, value, **kw_mask_args)
        if self.attention.requires_input_projection:
            q, k, v = self.in_proj_container(query=query, key=key, value=value)
        else:
            k, q, v = key, query, value

        def check(t, name):
            assert t.shape[2] % self.num_heads == 0, f'the {name} embeddings need to be divisible by the number of heads'
        check(q, 'projected query')
        check(v, 'projected value')
        check(k, 'projected key')
        if self.rotary_embeddings:
            q = _split_heads(q, B, S_Q, self.num_heads, self.dim_key_head)
            k = _split_heads(k, B, S_K, self.num_heads, self.dim_key_head)
            v = _split_heads(v, B, S_K, self.num_heads, self.dim_value_head)
            q, k = self.rotary_embeddings(q=q, k=k)
            if not self.attention.requires_head_dimension:
                q, k, v = q.flatten(0, 1), k.flatten(0, 1), v.flatten(0, 1)
        else:
            reshape_fn = _split_heads if self.attention.requires_head_dimension else _fold_heads
            q = reshape_fn(q, B, S_Q, self.num_heads, self.dim_key_head)
            k = reshape_fn(k, B, S_K, self.num_heads, self.dim_key_head)
            v = reshape_fn(v, B, S_K, self.num_heads, self.dim_value_head)
        y = self.attention(q, k, v, **kw_mask_args)
        y = y.view(B, self.num_heads, S_Q, self.dim_value_head).transpose(1, 2).flatten(start_dim=2, end_dim=3)
        y = self.resid_drop(self.proj(y))
        return y

    @classmethod
    def from_config(cls, config: 'MultiHeadDispatchConfig'):
        fields = asdict(config)
        fields = {k: v for k, v in fields.items() if v is not None}
        return cls(**fields)


def build_attention(config: 'Union[Dict[str, Any], AttentionConfig]'):
    """Builds an attention from a config.

    This assumes a 'name' key in the config which is used to determine what
    attention class to instantiate. For instance, a config `{"name": "my_attention",
    "foo": "bar"}` will find a class that was registered as "my_attention"
    (see :func:`register_attention`) and call .from_config on it."""
    if not isinstance(config, AttentionConfig):
        try:
            config_instance = generate_matching_config(config, ATTENTION_REGISTRY[config['name']].config)
        except KeyError as e:
            name = config['name']
            logger.warning(f'{name} not available among {ATTENTION_REGISTRY.keys()}')
            raise e
    else:
        config_instance = config
    return ATTENTION_REGISTRY[config_instance.name].constructor.from_config(config_instance)


def build_multi_head_attention(multi_head_config: 'Union[MultiHeadDispatchConfig, Dict[str, Any]]'):
    """Builds a multihead attention from a config.

    This assumes a 'name' key in the config which is used to determine what
    attention class to instantiate. For instance, a config `{"name": "my_attention",
    "foo": "bar"}` will find a class that was registered as "my_attention"
    (see :func:`register_attention`) and call .from_config on it."""
    if not isinstance(multi_head_config, MultiHeadDispatchConfig):
        field_names = list(map(lambda x: x.name, fields(MultiHeadDispatchConfig)))
        for k in field_names:
            if k not in multi_head_config.keys():
                multi_head_config[k] = None
        if not isinstance(multi_head_config['attention'], Attention):
            if 'num_heads' not in multi_head_config['attention']:
                multi_head_config['attention']['num_heads'] = multi_head_config['num_heads']
            if 'dim_model' not in multi_head_config['attention']:
                multi_head_config['attention']['dim_model'] = multi_head_config['dim_model']
            if 'dim_features' not in multi_head_config['attention'] or multi_head_config['attention']['dim_features'] is None:
                multi_head_config['attention']['dim_features'] = multi_head_config['dim_model'] // multi_head_config['num_heads']
            multi_head_config['attention'] = build_attention(multi_head_config['attention'])
        multi_head_config = MultiHeadDispatchConfig(**multi_head_config)
    return MultiHeadDispatch.from_config(multi_head_config)


def build_positional_embedding(config: 'Union[Dict[str, Any], PositionEmbeddingConfig]'):
    """Builds a position encoding from a config.

    This assumes a 'name' key in the config which is used to determine what
    attention class to instantiate. For instance, a config `{"name": "my_position_encoding",
    "foo": "bar"}` will find a class that was registered as "my_position_encoding"
    (see :func:`register_positional_embedding`) and call .from_config on it."""
    if not isinstance(config, PositionEmbeddingConfig):
        config_instance = generate_matching_config(config, POSITION_EMBEDDING_REGISTRY[config['name']].config)
    else:
        config_instance = config
    return POSITION_EMBEDDING_REGISTRY[config_instance.name].constructor.from_config(config_instance)


class xFormerDecoderBlock(torch.nn.Module):
    """A vanilla Transformer Decoder block

    ... note: this implementation is not (yet ?) reversible"""

    def __init__(self, config: 'xFormerDecoderConfig', **kwargs):
        super().__init__()
        if config.position_encoding_config is not None and config.layer_position.is_first():
            self.pose_encoding = build_positional_embedding(config.position_encoding_config)
            pos_encoding_dim = config.position_encoding_config.dim_model
            mha_dim = config.multi_head_config_masked['dim_model']
            if pos_encoding_dim != mha_dim:
                logger.warning(f'The embedding dim and model dim do not match ({pos_encoding_dim} vs {mha_dim}), adding a projector layer.')
                self.embedding_projector = nn.Linear(pos_encoding_dim, mha_dim)
        else:
            self.pose_encoding = None
        if config.residual_norm_style == ResidualNormStyle.DeepNorm:
            _, deep_norm_coefficients = get_deepnorm_coefficients(encoder_layers=0, decoder_layers=config.num_layers)
            assert deep_norm_coefficients is not None
            residual_scale = deep_norm_coefficients.alpha
        else:
            residual_scale = 1.0
        ln_factory = _get_ln_factory(config.dim_model, config.residual_norm_style, use_triton=config.use_triton, residual=True, residual_scale=residual_scale, normalization=config.normalization)
        mha = build_multi_head_attention(config.multi_head_config_masked)
        cross_mha = build_multi_head_attention(config.multi_head_config_cross)
        feedforward = build_feedforward(config.feedforward_config)
        self.supports_attention_mask = mha.attention.supports_attention_mask
        self.requires_same_k_q_dimensions = mha.attention.requires_same_k_q_dimensions
        self.requires_squared_context_length = feedforward.requires_squared_context or mha.attention.requires_squared_context
        self.causal_attention = mha.attention.causal if hasattr(mha.attention, 'causal') else False
        self.wrap_att = ln_factory(mha)
        self.wrap_cross = ln_factory(cross_mha)
        self.wrap_ff: 'Union[Residual, PostNorm]' = ln_factory(feedforward)
        if config.residual_norm_style == ResidualNormStyle.Pre and config.layer_position.is_last():
            self.wrap_ff = PostNorm(config.dim_model, self.wrap_ff, normalization=NormalizationType.LayerNorm)

    @classmethod
    def from_config(cls, config: 'xFormerDecoderConfig'):
        return cls(config)

    def forward(self, target: 'torch.Tensor', memory: 'torch.Tensor', encoder_att_mask: 'Optional[Union[torch.Tensor, AttentionMask]]'=None, decoder_att_mask: 'Optional[Union[torch.Tensor, AttentionMask]]'=None, input_mask: 'Optional[torch.Tensor]'=None):
        if self.pose_encoding is not None:
            target = self.pose_encoding(target)
            if hasattr(self, 'embedding_projector'):
                target = self.embedding_projector(target)
        if input_mask is not None:
            target_q = target
            target_k = target * input_mask.unsqueeze(-1)
            target_v = target_k
        else:
            target_q, target_k, target_v = target, target, target
        x = self.wrap_att(inputs=[target_q, target_k, target_v], att_mask=decoder_att_mask)
        x = self.wrap_cross(inputs=[x, memory, memory], att_mask=encoder_att_mask)
        x = self.wrap_ff(inputs=[x])
        return x


class PoolType(str, Enum):
    Conv2D = 'CONV_2D'


class SimplicialEmbedding(torch.nn.Module):
    """
    An implementation of the "Simplicial Embeddings"_, as proposed by Lavoie et. al

    Arguments:
        - L: the number of embedding chunks
        - temperature: optional scaling parameter for the softmax operation.
            A small (<1.) temperature will lead to a sparse representation (up to one-hot),
            while a large (>1.) temperature will make the vector more uniform

    _"Simplicial Embeddings": https://arxiv.org/pdf/2204.00616.pdf
    """

    def __init__(self, L: 'int', temperature: 'Optional[float]'=None) ->None:
        super().__init__()
        self.L = L
        self.temperature = temperature

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        assert x.shape[-1] % self.L == 0, f'The embedding dimension {x.shape[-1]} is not divisible by the chosen L parameter {self.L}'
        B, C, E = x.shape
        V = E // self.L
        Vs = x.reshape(B, C, self.L, V)
        if self.temperature is not None:
            Vs /= self.temperature
        if _is_triton_available():
            Vs = triton_softmax(Vs, mask=None, causal=False)
        else:
            Vs = torch.nn.functional.softmax(Vs, dim=-1)
        return Vs.reshape(B, C, E)

    @classmethod
    def from_config(cls: 'Type[Self]', config: 'SimplicialEmbeddingConfig') ->Self:
        fields = asdict(config)
        return cls(**fields)


class ConditionalReshape(torch.nn.Module):

    def __init__(self) ->None:
        super().__init__()

    def forward(self, x):
        if x.ndim == 3:
            B, HW, C = x.shape
            H = int(math.sqrt(HW))
            assert H * H == HW, f'{H, HW}'
            x = x.transpose(1, 2).reshape(B, C, H, H)
        return x


class PatchToSequence(torch.nn.Module):

    def __init__(self) ->None:
        super().__init__()

    def forward(self, x):
        return x.flatten(2, 3).transpose(1, 2).contiguous()


def build_patch_embedding(config: 'PatchEmbeddingConfig'):
    if not isinstance(config, PatchEmbeddingConfig):
        config = PatchEmbeddingConfig(**config)
    if config.pool_type == PoolType.Conv2D:
        pool = torch.nn.Conv2d(config.in_channels, config.out_channels, kernel_size=config.kernel_size, stride=config.stride, padding=config.padding)
    else:
        raise NotImplementedError
    return torch.nn.Sequential(ConditionalReshape(), pool, PatchToSequence())


class xFormerEncoderBlock(torch.nn.Module):
    """A vanilla Transformer Encoder block"""

    def __init__(self, config: 'xFormerEncoderConfig', **kwargs):
        super().__init__()
        self.reversible_f = None
        self.reversible_g = None
        self.residual_norm_style = config.residual_norm_style
        self.dim_model = config.dim_model
        if config.position_encoding_config is not None and config.layer_position.is_first():
            self.pose_encoding = build_positional_embedding(asdict(config.position_encoding_config))
            pos_encoding_dim = config.position_encoding_config.dim_model
            mha_dim = config.multi_head_config['dim_model']
            if pos_encoding_dim != mha_dim:
                logger.warning(f'The embedding dim and model dim do not match ({pos_encoding_dim} vs {mha_dim}), adding a projector layer.')
                self.embedding_projector = nn.Linear(pos_encoding_dim, mha_dim)
        else:
            self.pose_encoding = None
        if config.residual_norm_style == ResidualNormStyle.DeepNorm:
            deep_norm_coefficients, _ = get_deepnorm_coefficients(encoder_layers=config.num_layers, decoder_layers=0)
            assert deep_norm_coefficients is not None
            residual_scale = deep_norm_coefficients.alpha
        else:
            residual_scale = 1.0
        ln_factory = _get_ln_factory(config.dim_model, config.residual_norm_style, use_triton=config.use_triton, residual=True, residual_scale=residual_scale, normalization=config.normalization)
        mha = build_multi_head_attention(config.multi_head_config)
        feedforward = build_feedforward(asdict(config.feedforward_config))
        self.supports_attention_mask = mha.attention.supports_attention_mask
        self.requires_same_k_q_dimensions = mha.attention.requires_same_k_q_dimensions
        self.causal = mha.attention.causal if hasattr(mha.attention, 'causal') else False
        self.wrap_att = ln_factory(mha)
        self.wrap_ff: 'Union[Residual, PostNorm]' = ln_factory(feedforward)
        if config.residual_norm_style == ResidualNormStyle.Pre and config.layer_position.is_last():
            self.wrap_ff = PostNorm(config.dim_model, self.wrap_ff, normalization=config.normalization, use_triton=config.use_triton)
        self.simplicial_embedding: 'Optional[SimplicialEmbedding]' = None
        if config.simplicial_embeddings is not None and config.layer_position.is_last():
            self.simplicial_embedding = SimplicialEmbedding(**config.simplicial_embeddings)
        self.patch_emb: 'Optional[nn.Module]' = None
        if config.patch_embedding_config is not None:
            self.patch_emb = build_patch_embedding(PatchEmbeddingConfig(**config.patch_embedding_config))

    @classmethod
    def from_config(cls, config: 'xFormerEncoderConfig'):
        return cls(config)

    @staticmethod
    def get_reversible_layer(config) ->Tuple[nn.Module, nn.Module]:
        ln_factory = _get_ln_factory(config.dim_model, config.residual_norm_style, residual=False, use_triton=config.use_triton, normalization=config.normalization)
        mha = build_multi_head_attention(config.multi_head_config)
        feedforward = build_feedforward(asdict(config.feedforward_config))
        reversible_f = ln_factory(mha)
        reversible_g = ln_factory(feedforward)
        return reversible_f, reversible_g

    def forward(self, x: 'torch.Tensor', att_mask: 'Optional[Union[torch.Tensor, AttentionMask]]'=None, input_mask: 'Optional[torch.Tensor]'=None):
        if self.patch_emb is not None:
            x = self.patch_emb(x)
        if self.pose_encoding is not None:
            x = self.pose_encoding(x)
            if hasattr(self, 'embedding_projector'):
                x = self.embedding_projector(x)
        if input_mask is not None:
            q = x
            k = x * input_mask.unsqueeze(-1)
            v = k
        else:
            q, k, v = x, x, x
        x = self.wrap_att(inputs=[q, k, v], att_mask=att_mask)
        x = self.wrap_ff(inputs=[x])
        if self.simplicial_embedding is not None:
            x = self.simplicial_embedding(x)
        return x


class BlockType(str, Enum):
    Encoder = 'encoder'
    Decoder = 'decoder'


class LayerPositionBitmask(int, Enum):
    First = 1
    Last = 2
    Default = 3


class LayerPosition:
    """Bitmask to mark this layer as first, last, nothing or both"""

    def __init__(self):
        self.bitmask = LayerPositionBitmask.Default

    def is_first(self):
        return bool(self.bitmask & LayerPositionBitmask.First)

    def is_last(self):
        return bool(self.bitmask & LayerPositionBitmask.Last)

    def mark_not_first(self):
        self.bitmask &= ~LayerPositionBitmask.First

    def mark_not_last(self):
        self.bitmask &= ~LayerPositionBitmask.Last


class xFormer(torch.nn.Module):

    def __init__(self, stack_configs: 'Union[xFormerBlockConfig, List[xFormerBlockConfig], Dict[str, xFormerBlockConfig]]', tie_embedding_weights: 'bool'=False, weight_init: 'xFormerWeightInit'=xFormerWeightInit.ViT):
        """
        Given a serialized configuration, generate the corresponding model.
        This is only a helper and can easily be bypassed
        """
        super().__init__()
        if isinstance(stack_configs, Dict):
            stack_configs = list(stack_configs.values())
        if not isinstance(stack_configs, List):
            stack_configs = [stack_configs]
        self._verify_reversible(stack_configs)
        self._verify_deepnorm(stack_configs)
        encoders: 'List[torch.nn.Module]' = []
        decoders: 'List[torch.nn.Module]' = []
        self.reversible_encoder = False
        self.rev_enc_pose_encoding = None
        for config in stack_configs:
            builder = xFormerEncoderBlock.from_config if isinstance(config, xFormerEncoderConfig) else xFormerDecoderBlock.from_config
            recipient = encoders if isinstance(config, xFormerEncoderConfig) else decoders
            for i in range(config.num_layers):
                if len(recipient) > 0:
                    config.layer_position.mark_not_first()
                if config != stack_configs[-1] or i < config.num_layers - 1:
                    config.layer_position.mark_not_last()
                block = builder(config)
                if config.reversible:
                    assert isinstance(config, xFormerEncoderConfig)
                    if block.pose_encoding is not None:
                        self.rev_enc_pose_encoding = block.pose_encoding
                    self.reversible_encoder = True
                    f, g = xFormerEncoderBlock.get_reversible_layer(config)
                    recipient.append(torch.nn.ModuleList([f, g]))
                else:
                    recipient.append(block)
        assert not tie_embedding_weights or not self.reversible_encoder, 'Reversible layers and  tied embeddings is not supported for now'
        if tie_embedding_weights and encoders and encoders[0].pose_encoding and decoders and decoders[0].pose_encoding and not config.reversible:
            logger.info('Tying encoder and decoder embeddings, as requested')
            encoders[0].pose_encoding = decoders[0].pose_encoding
        self.encoders: 'torch.nn.Module' = rv.ReversibleSequence(torch.nn.ModuleList(encoders)) if self.reversible_encoder else torch.nn.ModuleList(encoders)
        self.decoders = torch.nn.ModuleList(decoders)
        use_deepnorm = stack_configs[0].residual_norm_style == ResidualNormStyle.DeepNorm
        assert not use_deepnorm or not self.reversible_encoder, 'Reversible layers and deepnorm is not supported for now'
        self.init_weights(weight_init=weight_init, use_deep_norm=use_deepnorm)

    @classmethod
    def from_config(cls, config: 'xFormerConfig'):
        return cls(config.stack_configs, config.tie_embedding_weights, config.weight_init)

    def _verify_reversible(self, stack_configs: 'List[xFormerBlockConfig]'):
        reversible = [c.reversible for c in filter(lambda x: x.block_type == 'encoder', stack_configs)]
        assert all(reversible) or not any(reversible), 'All layers need to have the same reversibility setting. ' + f'Currently {reversible}'

    def _verify_deepnorm(self, stack_configs: 'List[xFormerBlockConfig]'):
        deepnorm = [(c.residual_norm_style == ResidualNormStyle.DeepNorm) for c in stack_configs]
        assert all(deepnorm) or not any(deepnorm), 'All layers need to have the same deepnorm setting. ' + f'Currently {deepnorm}'

    def init_weights(self, weight_init: 'xFormerWeightInit', use_deep_norm: 'bool'):
        if use_deep_norm:
            encoder_coefficients, decoder_coefficients = get_deepnorm_coefficients(encoder_layers=len(self.encoders), decoder_layers=len(self.decoders))
        else:
            encoder_coefficients, decoder_coefficients = None, None
        encoder_gain = encoder_coefficients.beta if encoder_coefficients is not None else 1.0
        decoder_gain = decoder_coefficients.beta if decoder_coefficients is not None else 1.0
        init_fn = get_weight_init_fn(weight_init)
        for name, module in self.encoders.named_children():
            init_fn(module=module, name=name, gain=encoder_gain)
        for name, module in self.decoders.named_children():
            init_fn(module=module, name=name, gain=decoder_gain)

    def forward(self, src: 'torch.Tensor', tgt: 'Optional[torch.Tensor]'=None, encoder_input_mask: 'Optional[torch.Tensor]'=None, decoder_input_mask: 'Optional[torch.Tensor]'=None) ->Optional[torch.Tensor]:
        if len(list(self.encoders.parameters())) > 0:
            encoders = self.encoders
            memory = src.clone()
            if isinstance(encoders, torch.nn.ModuleList):
                for encoder in encoders:
                    memory = encoder(memory, input_mask=encoder_input_mask)
            else:
                if self.rev_enc_pose_encoding:
                    memory = self.rev_enc_pose_encoding(src)
                x = torch.cat([memory, memory], dim=-1)
                if encoder_input_mask is not None:
                    if x.dim() - encoder_input_mask.dim() > 1:
                        encoder_input_mask.unsqueeze(0)
                    x += encoder_input_mask.unsqueeze(-1)
                x = encoders(x)
                memory = torch.stack(x.chunk(2, dim=-1)).mean(dim=0)
            if not self.decoders:
                return memory
        if len(self.decoders) > 0:
            tgt = src.clone() if tgt is None else tgt
            for decoder in self.decoders:
                tgt = decoder(target=tgt, memory=memory, input_mask=decoder_input_mask)
            return tgt
        return None


class ViT(nn.Module):

    def __init__(self, mlp):
        super().__init__()
        test_config[0]['feedforward_config']['name'] = mlp
        xformer_config = xFormerConfig(test_config)
        self.xformer = xFormer.from_config(xformer_config)


class Passthrough(torch.nn.Module):

    def __init__(self) ->None:
        super().__init__()

    def forward(self, *args):
        return args


class Pooling(Attention):

    def __init__(self, pool_size: 'int'=3, stride: 'int'=1, padding: 'Optional[int]'=None, *_, **__):
        """
        Pooling token mixing mechanism, as proposed in
        `Metaformer is actually what you need for vision`_, Yu et al (2021).

        The original notation is kept as is.

        .. _`Metaformer is actually what you need for vision` : https://arxiv.org/pdf/2111.11418v1.pdf
        """
        super().__init__()
        padding = padding if padding is not None else pool_size // 2
        self.pool = nn.AvgPool2d(pool_size, stride=stride, padding=pool_size // 2, count_include_pad=False)
        self.requires_same_k_q_dimensions = False
        self.supports_attention_mask = False
        self.requires_skip_multi_head = True
        self.requires_input_projection = False
        self.requires_same_k_q_dimensions = True
        self.requires_squared_context = True

    def forward(self, q: 'torch.Tensor', *_, **__):
        B, HW, C = q.shape
        H = int(math.sqrt(HW))
        assert H * H == HW
        q = q.transpose(-2, -1).reshape(B, C, H, H)
        x_pool = self.pool(q) - q
        return x_pool.flatten(2, 3).transpose(-2, -1)


def pooling(mode: 'Pooling'):

    def pool_cls(inp):
        return inp[:, 0, :]

    def pool_mean(inp):
        return inp.mean(dim=1)
    return {Pooling.MEAN: pool_mean, Pooling.CLS: pool_cls}[mode]


class SCHead(nn.Module):

    def __init__(self, config, dim_embedding, dim_mlp):
        super().__init__()
        self.pooling = pooling(Pooling(config['pooling_mode']))
        self.mlpblock = nn.Sequential(nn.Linear(dim_embedding, dim_mlp), nn.ReLU(), nn.Linear(dim_mlp, config['common']['num_classes']))

    def forward(self, inp: 'torch.Tensor'):
        seq_score = self.mlpblock(self.pooling(inp))
        return seq_score


class SCHeadDual(nn.Module):

    def __init__(self, config, dim_embedding, dim_mlp):
        super().__init__()
        self.pooling = pooling(Pooling(config['pooling_mode']))
        self.mlpblock = nn.Sequential(nn.Linear(dim_embedding * 4, dim_mlp), nn.ReLU(), nn.Linear(dim_mlp, config['common']['num_classes']))

    def forward(self, inp_0: 'torch.Tensor', inp_1: 'torch.Tensor'):
        X_0 = self.pooling(inp_0)
        X_1 = self.pooling(inp_1)
        seq_score = self.mlpblock(torch.cat([X_0, X_1, X_0 * X_1, X_0 - X_1], dim=-1))
        return seq_score


class TimmMemEffAttention(nn.Module):

    def __init__(self, attn: 'TimmAttention', op=None):
        super().__init__()
        self.op = None
        self.num_heads = attn.num_heads
        self.scale = attn.scale
        self.qkv = attn.qkv
        self.attn_drop = attn.attn_drop
        self.proj = attn.proj
        self.proj_drop = attn.proj_drop

    def forward(self, x):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads)
        q, k, v = xops.unbind(qkv, dim=2)
        x = xops.memory_efficient_attention(q, k, v, op=self.op).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class TimmSwiGLU(nn.Module):

    def __init__(self, mlp: 'TimmMlp', op=None) ->None:
        super().__init__()
        self.fc1 = mlp.fc1
        self.swiglu = xops.SwiGLU(in_features=mlp.fc1.in_features, hidden_features=mlp.fc1.out_features, bias=True)
        self.op = op

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        return self.swiglu(x)


class SquaredReLU(nn.Module):

    def __init__(self) ->None:
        super().__init__()

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        x_ = torch.nn.functional.relu(x)
        return x_ * x_


class StarReLU(nn.Module):

    def __init__(self) ->None:
        super().__init__()

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        x_ = torch.nn.functional.relu(x)
        return 0.8944 * x_ * x_ - 0.4472


class SmeLU(nn.Module):

    def __init__(self, beta: 'float'=2.0) ->None:
        super().__init__()
        self.beta = beta

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        relu = torch.where(x >= self.beta, x, torch.tensor([0.0], device=x.device, dtype=x.dtype))
        return torch.where(torch.abs(x) <= self.beta, ((x + self.beta) ** 2).type_as(x) / (4.0 * self.beta), relu)


class AttentionBias:
    """Base class for a custom bias that can be applied         as the attn_bias argument in
        :attr:`xformers.ops.memory_efficient_attention`.

    That function has the ability to add a tensor, the
    attention bias, to the QK^T matrix before it is used
    in the softmax part of the attention calculation.
    The attention bias tensor with shape
    (B or 1, n_queries, number of keys)
    can be given as the attn_bias input.
    The most common use case is for an attention bias is
    to contain only zeros and negative infinities, which forms
    a mask so that some queries only attend to some keys.

    Children of this class define alternative things which can
    be used as the attn_bias input to define an attention bias which
    forms such a mask, for some common cases.

    When using an :attr:`xformers.ops.AttentionBias`
    instead of a :attr:`torch.Tensor`, the mask matrix does
    not need to be materialized, and can be
    hardcoded into some kernels for better performance.

    See:

    - :attr:`xformers.ops.fmha.attn_bias.LowerTriangularMask`
    - :attr:`xformers.ops.fmha.attn_bias.LowerTriangularMaskWithTensorBias`
    - :attr:`xformers.ops.fmha.attn_bias.BlockDiagonalMask`
    - :attr:`xformers.ops.fmha.attn_bias.BlockDiagonalCausalMask`

    """

    def materialize(self, shape: 'Tuple[int, ...]', dtype: 'torch.dtype'=torch.float32, device: 'Union[str, torch.device]'='cpu') ->torch.Tensor:
        """
        Materializes the bias as a `torch.Tensor`. This is very slow
        and we don't attempt to make it fast. Only use for debugging/testing.

        Shape should be like `[*, q_seqlen, k_seqlen]`
        """
        raise NotImplementedError()


AttentionMask = AttentionBias


def _either_or(a: 'Optional[int]', b: 'int') ->int:
    return a if a is not None else b


def get_depth(args):
    return triton.next_power_of_2(args['K'])


class CompositionalAttention(Attention):
    """Compositional Attention, as proposed in
    "Compositional Attention: Disentangling search and retrieval"_, S. Mittal et al.

    A key insight from this proposal is that the attention mechanism can be conceived as two steps:
    a search and a retrieval operation. When queried, the model can search for the most relevant information
    (Softmax(QKt)), then retrieve information given the Value.

    Contrary to the original attention proposal, which does not consider interactions in between heads,
    the compositional attention will consider all possible interactions and softmax over that dimension,
    so that the information retrieved covers the most relevant dimensions. The number of heads and rules to
    use is thus typically smaller than for a comparable traditional Transformer, and asking for the same number of heads
    may not fit in memory.

    Args:
        dim_model: dimension of the incoming latent space
        num_heads: number of heads *for the search operation*
        dim_attn: dimension (embedding) of the attention
        num_rules: number of rules to consider *for the retrieval operation*
        dim_selection: dimension of the scoring/selection space for the retrievals
        dim_key, dim_value: dimensions of K and V, if different from Q
        dropout: attention dropout probability
        qk_rule: QK product will drive the retrieval process
        nonlinear: use a non linear method to score the retrievals
        bias: use bias in the initial projection step
        causal: causal computations (attend to the past only)

    _"Compositional Attention: Disentangling search and retrieval": https://arxiv.org/pdf/2110.09419v1.pdf
    """

    def __init__(self, dim_model: 'int', num_heads: 'int', dim_attn: 'Optional[int]'=None, num_rules: 'Optional[int]'=None, dim_selection: 'Optional[int]'=None, dim_key: 'Optional[int]'=None, dim_value: 'Optional[int]'=None, dropout=0.0, qk_rule=False, nonlinear=False, q_compose=False, in_proj_container: 'Optional[InputProjection]'=None, use_separate_proj_weight: 'Optional[bool]'=False, bias=True, causal=False, *_, **__):
        super().__init__()
        self.requires_skip_multi_head = True
        self.dim_model = dim_model
        num_rules = _either_or(num_rules, num_heads)
        dim_selection = _either_or(dim_selection, dim_model // num_heads)
        dim_attn = _either_or(dim_attn, dim_model)
        dim_key = _either_or(dim_key, dim_model)
        dim_value = _either_or(dim_value, dim_model)
        self.in_proj_container = in_proj_container if in_proj_container is not None else InputProjection(query_proj_params=InputProjectionConfig(dim_model, dim_key, bias=bias), key_proj_params=InputProjectionConfig(dim_model, dim_key, bias=bias) if use_separate_proj_weight else None, value_proj_params=InputProjectionConfig(dim_model, dim_value, bias=bias) if use_separate_proj_weight else None)
        self.num_heads = num_heads
        self.num_rules = num_rules
        self.qk_rule = qk_rule
        self.dim_selection = dim_selection
        self.nonlinear = nonlinear
        self.q_compose = q_compose
        self.dropout_module = nn.Dropout(dropout)
        self.dim_head = dim_model // num_heads
        self.value_dim = dim_attn // num_rules
        assert self.value_dim * num_rules == dim_attn, 'value_dim must be divisible by num_rules'
        self.scaling = self.dim_head ** -0.5
        self.scaling_values = self.dim_selection ** -0.5
        self.out_proj = nn.Linear(self.num_heads * self.value_dim, dim_model, bias=bias)
        if self.qk_rule:
            self.value_k = nn.Linear(self.value_dim, self.dim_selection, bias=bias)
            if self.q_compose:
                self.value_q = nn.Linear(self.dim_head, self.dim_selection, bias=bias)
            else:
                self.value_q = nn.Linear(dim_model, self.dim_selection * self.num_heads, bias=bias)
        else:
            if self.q_compose:
                self.value_q = nn.Linear(self.dim_head, self.dim_selection, bias=bias)
            else:
                self.value_q = nn.Linear(dim_model, self.dim_selection * self.num_heads, bias=bias)
            if self.nonlinear:
                self.score_network: 'nn.Module' = nn.Sequential(nn.Linear(self.dim_selection + self.value_dim, self.dim_selection, bias=bias), nn.ReLU(), nn.Linear(self.dim_selection, 1, bias=bias))
            else:
                self.score_network = nn.Linear(self.dim_selection + self.value_dim, 1, bias=bias)
        self.causal = causal
        self.supports_attention_mask = True
        self.supports_key_padding_mask = False
        self._reset_parameters()

    def _reset_parameters(self):
        if self.qk_rule:
            nn.init.xavier_uniform_(self.value_k.weight, gain=1 / math.sqrt(2))
            nn.init.xavier_uniform_(self.value_q.weight, gain=1 / math.sqrt(2))
        else:
            nn.init.xavier_uniform_(self.value_q.weight)
            if self.nonlinear:
                nn.init.xavier_uniform_(self.score_network[0].weight)
                nn.init.xavier_uniform_(self.score_network[2].weight)
            else:
                nn.init.xavier_uniform_(self.score_network.weight)
        nn.init.xavier_uniform_(self.out_proj.weight)
        if self.out_proj.bias is not None:
            nn.init.constant_(self.out_proj.bias, 0.0)

    def forward(self, q: 'Tensor', k: 'Tensor', v: 'Tensor', att_mask: 'Optional[Tensor]'=None, *args, **kwargs) ->Tensor:
        """
        Input shape: Time x Batch x Channel

        Args:
            att_mask (ByteTensor, optional): typically used to
                implement causal attention, where the mask prevents the
                attention from looking forward in time (default: None).
        """
        B, Sq, E = q.shape
        _, Sk, _ = k.shape
        assert E == self.dim_model
        q_unprojected = q
        q, k, v = self.in_proj_container(query=q, key=k, value=v)
        q *= self.scaling
        if self.causal and (self._causal_mask is None or self._causal_mask.shape[0] != Sk):
            self._causal_mask = AttentionMask.make_causal(Sq, Sq, device=q.device)
        if isinstance(att_mask, torch.Tensor):
            att_mask_additive: 'Optional[AttentionMask]' = AttentionMask.from_bool(att_mask) if att_mask.dtype == torch.bool else AttentionMask(att_mask, is_causal=False)
        else:
            att_mask_additive = None
        if self._causal_mask is not None:
            if att_mask_additive is not None:
                att_mask_additive += self._causal_mask
            else:
                att_mask_additive = self._causal_mask
        q = q.view(B, Sq, self.num_heads, self.dim_head).movedim(2, 1).flatten(0, 1)
        k = k.view(B, Sk, self.num_heads, self.dim_head).movedim(2, 1).flatten(0, 1)
        v = v.view(B, -1, self.num_rules, self.value_dim).movedim(2, 1).flatten(0, 1)
        attn_weights = torch.bmm(q, k.transpose(1, 2))
        if att_mask_additive is not None:
            attn_weights += att_mask_additive.values
        attn_weights = _softmax(attn_weights, causal=self.causal)
        attn_weights = attn_weights.view(B, self.num_heads, Sq, Sk)
        attn_probs = self.dropout_module(attn_weights)
        v = v.view(B, 1, self.num_rules, Sk, self.value_dim)
        attn_probs = attn_probs.unsqueeze(2)
        attn = torch.matmul(attn_probs, v).view(B, self.num_heads, self.num_rules, Sq, self.value_dim)
        attn = attn.movedim(3, 1)
        if self.q_compose:
            v_q = self.value_q(q.transpose(0, 1)).view(B, Sq, self.num_heads, 1, self.dim_selection)
        else:
            v_q = self.value_q(q_unprojected).view(B, Sq, self.num_heads, 1, self.dim_selection)
        if self.qk_rule:
            v_q *= self.scaling_values
            v_k = self.value_k(attn).view(B, Sq, self.num_heads, self.num_rules, self.dim_selection).transpose(4, 3).contiguous()
            v_score = torch.matmul(v_q, v_k).view(B, Sq, self.num_heads, self.num_rules, 1)
        else:
            v_q = v_q.expand(-1, -1, -1, self.num_rules, -1)
            v_in = torch.cat([attn, v_q], dim=-1)
            v_score = self.score_network(v_in).view(B, Sq, self.num_heads, self.num_rules, 1)
        v_score = F.softmax(v_score, dim=3)
        attn = (attn * v_score).sum(dim=3).view(B, Sq, self.num_heads * self.value_dim)
        attn = self.out_proj(attn)
        return attn


class FeatureMapType(str, Enum):
    SMOrf = 'sm_orf'
    SMHyp = 'sm_hyp'
    SMReg = 'sm_reg'


class NormDistribution(Enum):
    Xi = auto()
    Uniform = auto()


class FeatureMap(torch.nn.Module):

    def __init__(self, dim_features: 'int', iter_before_redraw: 'Optional[int]'=None, normalize_inputs: 'bool'=False, epsilon: 'float'=1e-06):
        super().__init__()
        self.dim_features = dim_features
        self.dim_feature_map = dim_features
        self.iter_before_redraw = iter_before_redraw
        self.features: 'Optional[torch.Tensor]' = None
        self.epsilon = epsilon
        self.normalize_inputs = normalize_inputs
        self._iter_counter = 0

    @abstractmethod
    def _get_feature_map(self, dim_input: 'int', dim_features: 'int', device: 'torch.device'):
        raise NotImplementedError()

    @classmethod
    def from_config(cls: 'Type[Self]', config: 'FeatureMapConfig') ->Self:
        fields = asdict(config)
        fields = {k: v for k, v in fields.items() if v is not None}
        return cls(**fields)


class SoftMaxPositiveEstimators(FeatureMap):

    def __init__(self, dim_features: 'int', iter_before_redraw: 'Optional[int]', normalize_inputs: 'bool'=False, epsilon: 'float'=1e-06, softmax_temp: 'float'=-1):
        super().__init__(dim_features, iter_before_redraw, normalize_inputs, epsilon)
        self.softmax_temp = softmax_temp
        self.h_scale = math.log(math.sqrt(self.dim_features))

    def pre_scale(self, x: 'torch.Tensor') ->torch.Tensor:
        with record_function('feature_map::pre_scale'):
            if self.iter_before_redraw is not None and self._iter_counter > self.iter_before_redraw or self.features is None or self.features.device != x.device:
                self._iter_counter = 1
                self.features = self._get_feature_map(x.shape[-1], self.dim_feature_map, x.device)
            features = self.features
            assert features is not None
            if features.dtype != x.dtype:
                self.features = features
            self._iter_counter += 1
            if self.softmax_temp < 0:
                self.softmax_temp = x.shape[-1] ** -0.25
            x_scaled = x * self.softmax_temp
            norm_x_2 = torch.einsum('...d,...d->...', x_scaled, x_scaled).unsqueeze(-1)
            self.offset = -0.5 * norm_x_2 - self.h_scale + self.epsilon
            if self.normalize_inputs:
                self.offset -= norm_x_2.max(1, keepdim=True)[0]
        return x_scaled

    @staticmethod
    @torch.no_grad()
    def _get_random_ortho_matrix(blocks: 'int', dim: 'int', device: 'torch.device', norm_distribution: 'NormDistribution'=NormDistribution.Uniform) ->torch.Tensor:
        """
        Generate a random matrix whose rows are exactly orthonormal

        "How to generate random matrices from the classical compact groups", Mezzadri, 2007
        https://arxiv.org/pdf/math-ph/0609050v2.pdf

        .. note: the typical qr decomposition does not give uniform results, qr decomposition is not
        unique and the qr decomposition routines are biased towards numerical stability. See the above
        paper for more information.

        .. note: this does not follow the original implementation from the Performers authors.
        see docs/assets/kde plots to visualize the impact of using the R signs to correct Q
        """
        H = torch.randn((blocks, dim, dim), device=device, requires_grad=False)
        if norm_distribution == NormDistribution.Xi:
            norms = torch.sqrt(torch.einsum('...d,...d->...', H, H))
        Q, R = torch.linalg.qr(H)
        Q = torch.diag_embed(torch.sign(torch.diagonal(R, dim1=1, dim2=2))) @ Q
        if norm_distribution == NormDistribution.Xi:
            return torch.diag_embed(norms) @ Q
        return Q


class SMHyperbolic(SoftMaxPositiveEstimators):
    """
    "Positive random features hyperbolic" estimator, SMHyp+,
    as proposed in the Performers_ paper, Lemma 1.

    _Performers: "Rethinking attention with performers." K. Choromanski et al. (2020).
    https://arxiv.org/pdf/2009.14794v1.pdf
    """

    def __init__(self, dim_features: 'int', iter_before_redraw: 'Optional[int]', normalize_inputs: 'bool'=False, epsilon: 'float'=1e-06, softmax_temp: 'float'=-1):
        super().__init__(dim_features, iter_before_redraw, normalize_inputs, epsilon, softmax_temp)
        assert dim_features % 2 == 0, 'The feature dimension needs to be even with this kernel'
        self.dim_feature_map = self.dim_features // 2

    @torch.no_grad()
    def _get_feature_map(self, dim_input: 'int', dim_features: 'int', device: 'torch.device'):
        """
        Generate the projection matrix onto the random features

        .. note: The heads dimension needs to be taken into account, hence the per-block random matrix
        and not uniformally random.
        """
        features = self._get_random_ortho_matrix(math.ceil(dim_input / dim_features), dim_features, norm_distribution=NormDistribution.Xi, device=device)
        return features.flatten(0, 1)[:dim_input]

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        x_scaled = super().pre_scale(x)
        x_scaled = x_scaled @ self.features
        return torch.cat([torch.exp(x_scaled + self.offset), torch.exp(-x_scaled + self.offset)], dim=-1)


class SMOrf(SoftMaxPositiveEstimators):
    """
    "Positive random orthogonal features" softmax estimator,
    SM_ort^m+, as proposed in the Performers_ paper, Lemma 1.

    _Performers: "Rethinking attention with performers." K. Choromanski et al. (2020).
    https://arxiv.org/pdf/2009.14794v1.pdf
    """

    @torch.no_grad()
    def _get_feature_map(self, dim_input: 'int', dim_features: 'int', device: 'torch.device'):
        """
        Generate the projection matrix onto the random features

        .. note: The heads dimension needs to be taken into account, hence the per-block random matrix
        and not uniformally random.
        """
        features = self._get_random_ortho_matrix(math.ceil(dim_input / dim_features), dim_features, norm_distribution=NormDistribution.Xi, device=device)
        return features.flatten(0, 1)[:dim_input]

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        x_scaled = super().pre_scale(x)
        assert self.features is not None
        x_scaled = x_scaled @ self.features
        return torch.exp(x_scaled + self.offset)


class SMReg(SoftMaxPositiveEstimators):
    """
    "Regularized softmax kernel" estimator, SMREG+, as proposed in the Performers_ paper.

    _Performers: "Rethinking attention with performers." K. Choromanski et al. (2020).
    https://arxiv.org/pdf/2009.14794v1.pdf
    """

    def __init__(self, dim_features: 'int', iter_before_redraw: 'Optional[int]', normalize_inputs: 'bool'=False, epsilon: 'float'=1e-06, softmax_temp: 'float'=-1):
        super().__init__(dim_features, iter_before_redraw, normalize_inputs, epsilon, softmax_temp)
        assert dim_features % 2 == 0, 'The feature dimension needs to be even with this kernel'
        self.dim_feature_map = self.dim_features // 2

    @torch.no_grad()
    def _get_feature_map(self, dim_input: 'int', dim_features: 'int', device: 'torch.device'):
        """
        Generate the projection matrix onto the random features

        .. note: The heads dimension needs to be taken into account, hence the per-block random matrix
        and not uniformally random.
        """
        features = self._get_random_ortho_matrix(math.ceil(dim_input / dim_features), dim_features, norm_distribution=NormDistribution.Uniform, device=device).flatten(0, 1)
        norms = math.sqrt(dim_input) * torch.ones(features.shape[0], device=device)
        return (torch.diag(norms) @ features)[:dim_input]

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        x_scaled = super().pre_scale(x)
        x_scaled = x_scaled @ self.features
        return torch.cat([torch.exp(x_scaled + self.offset), torch.exp(-x_scaled + self.offset)], dim=-1)


class FavorAttention(Attention):

    def __init__(self, causal: 'bool'=False, dropout: 'float'=0.0, dim_features: 'Optional[int]'=None, dim_head: 'Optional[int]'=None, iter_before_redraw: 'Optional[int]'=None, feature_map_type: 'FeatureMapType'=FeatureMapType.SMReg, normalize_inputs: 'bool'=False, *_, **__):
        """
        Kernelized attention, as proposed in Performers_
        ("Rethinking attention with performers." K. Choromanski et al. (2020).).

        FAVOR stands for "Fast Attention Via positive Orthogonal Random features"

        Args:
            dropout (float): the probability of an output to be randomly dropped at training time
            dim_features (int): the dimension of the random features space
            iter_before_redraw (int): the number of steps (forward calls) before a redraw of the features
            feature_map_type (FeatureMapType): the type of feature map being used,
            for instance orthogonal random features.

        .. _Performers: https://arxiv.org/pdf/2009.14794v1.pdf
        """
        super().__init__()
        self.causal = causal
        self.iter_before_redraw = 2 * iter_before_redraw if iter_before_redraw is not None else iter_before_redraw
        self.normalize_inputs = normalize_inputs
        self.feature_map_type = feature_map_type
        self.attn_drop = nn.Dropout(dropout, inplace=True)
        if dim_features is None:
            assert dim_head is not None, 'dim_features or dim_head needs to be passed'
            self.dim_features = math.ceil(dim_head * (1 + math.log2(dim_head)))
            self.dim_features = 2 * (self.dim_features // 2)
            logger.info(f'FAVOR: Automatically setting the random mapping dimension to {self.dim_features} from {dim_head}')
        else:
            self.dim_features = dim_features
        feature_map_constructor = {FeatureMapType.SMHyp: SMHyperbolic, FeatureMapType.SMReg: SMReg, FeatureMapType.SMOrf: SMOrf}[self.feature_map_type]
        feature_settings = {'dim_features': self.dim_features, 'iter_before_redraw': self.iter_before_redraw, 'normalize_inputs': self.normalize_inputs}
        self.feature_map: 'FeatureMap' = feature_map_constructor(**feature_settings)
        self.supports_attention_mask = False
        self.supports_key_padding_mask = False

    @staticmethod
    def _maybe_promote(x: 'torch.Tensor') ->torch.Tensor:
        return x.float() if x.dtype == torch.float16 else x

    @staticmethod
    def _causal_attention(k_prime: 'torch.Tensor', q_prime: 'torch.Tensor', v: 'torch.Tensor') ->Tuple[torch.Tensor, torch.Tensor]:
        ref_v = torch.ones_like(v.unsqueeze(2))
        Gps = k_prime.unsqueeze(3) * v.unsqueeze(2)
        Grenorm = k_prime.unsqueeze(3) * ref_v
        att_raw = torch.einsum('bcfe,bcf->bce', Gps, q_prime)
        att_norm = torch.einsum('bcfe,bcf->bce', Grenorm, q_prime)
        att_raw = att_raw.cumsum(2)
        att_norm = att_norm.cumsum(2)
        return att_raw, att_norm

    def forward(self, q: 'torch.Tensor', k: 'torch.Tensor', v: 'torch.Tensor', *_, **__):
        k_prime = self.feature_map(k)
        q_prime = self.feature_map(q)
        with autocast(enabled=False):
            k_prime = self._maybe_promote(k_prime)
            q_prime = self._maybe_promote(q_prime)
            v = self._maybe_promote(v)
            if not self.causal:
                att_normalization = q_prime @ (k_prime.transpose(-2, -1) @ torch.ones_like(v))
                att_raw = q_prime @ (k_prime.transpose(-2, -1) @ v)
            else:
                att_raw, att_normalization = self._causal_attention(k_prime, q_prime, v)
            att = att_raw / att_normalization
        if self.attn_drop is not None:
            att = self.attn_drop(att)
        return att


class FourierMix(Attention):

    def __init__(self, dropout: 'float', *_, **__):
        """
        FFT-based pseudo-attention mechanism, from
        "
        "FNet: Mixing Tokens with Fourier Transforms"
        Lee-Thorp et al., 2021, https://arxiv.org/pdf/2105.03824.pdf
        """
        super().__init__()
        self.attn_drop = torch.nn.Dropout(dropout, inplace=False)
        self.supports_attention_mask = False
        self.requires_input_projection = False

    def forward(self, q: 'torch.Tensor', *_, **__):
        with autocast(enabled=False):
            att = torch.fft.fft2(q).real
        att = self.attn_drop(att)
        return att


def causal_1d_pattern(attn_size: 'int') ->torch.Tensor:
    mask = torch.tril(torch.ones(attn_size, attn_size, dtype=torch.bool))
    return mask


def global_token_pattern(attention_query_mask: 'torch.Tensor') ->torch.Tensor:
    assert attention_query_mask.ndim == 1
    assert attention_query_mask.dtype == torch.bool
    attention_query_mask = attention_query_mask[None, :]
    mask = attention_query_mask | attention_query_mask.transpose(1, 0)
    return mask


_DENSITY_THRESHOLD = 0.3


def _csr_to_coo(m, n, row_offsets, column_indices):
    indices = torch.arange(m, dtype=row_offsets.dtype, device=row_offsets.device)
    row_sizes = torch.diff(row_offsets)
    row_coo = torch.repeat_interleave(indices, row_sizes.long())
    return row_coo, column_indices


def _diffsort(a):
    return torch.argsort(torch.diff(a), dim=0, descending=True)


def _nonzero_mask_to_sparse_csr_indices(mask, device):
    """Converts dense 2d matrix to a csr sparse matrix."""
    assert len(mask.shape) == 2
    index_dtype = torch.int32
    row_offsets = mask.sum(dim=-1, dtype=index_dtype).cumsum(dim=-1, dtype=index_dtype)
    row_offsets = torch.nn.functional.pad(row_offsets, (1, 0))
    row_indices = _diffsort(row_offsets)
    column_indices = torch.where(mask)[1].contiguous()
    row_indices = row_indices
    row_offsets = row_offsets
    column_indices = column_indices
    return row_indices, row_offsets, column_indices


def _round_nnz(mask, divisible_by=4):
    nonzero = torch.where(mask)
    nnz = nonzero[0].shape[0]
    nonzero = tuple(n[:nnz - nnz % divisible_by] for n in nonzero)
    nm = torch.zeros_like(mask)
    nm[nonzero] = True
    return nm


def _dense3d_to_sparse(matrix, device):
    assert len(matrix.shape) == 3
    mask = matrix != 0
    if not torch.all(mask == mask[0]):
        raise ValueError('Expected the same sparsity pattern over the batch dimension')
    mask = _round_nnz(mask[0], divisible_by=4)
    mask = mask[None].expand(matrix.shape)
    values = matrix[mask].reshape(matrix.shape[0], -1)
    row_indices, row_offsets, column_indices = _nonzero_mask_to_sparse_csr_indices(mask[0], device)
    return values, row_indices, row_offsets, column_indices


def _coo_to_csr(m, n, row_indices, column_indices):
    row_offsets = row_indices.bincount(minlength=n).cumsum(0, dtype=row_indices.dtype)
    row_offsets = torch.nn.functional.pad(row_offsets, (1, 0))
    return row_offsets, column_indices


def _get_transpose_info(m, n, row_indices, row_offsets, column_indices):
    row_coo, _ = _csr_to_coo(m, n, row_offsets, column_indices)
    row_offsets_t, perm = column_indices.sort(dim=0, stable=True)
    column_indices_t = row_coo[perm]
    row_offsets_t, _ = _coo_to_csr(m, n, row_offsets_t, column_indices)
    row_indices_t = _diffsort(row_offsets_t).int()
    return row_indices_t, row_offsets_t, column_indices_t, perm


def _transpose_with_info(values, _transpose_info):
    row_indices_t, row_offsets_t, column_indices_t, perm = _transpose_info
    values_t = values[:, perm]
    return row_indices_t, values_t, row_offsets_t, column_indices_t


def masked_matmul(a, b, mask=None):
    if torch.overrides.has_torch_function((a, b, mask)):
        return torch.overrides.handle_torch_function(masked_matmul, (a, b, mask), a, b, mask)
    att = a @ b
    if mask is None:
        return att
    if mask.dtype == torch.bool:
        if mask.ndim == 2:
            mask = mask.unsqueeze(0).expand(att.shape[0], -1, -1)
        att[~mask] = float('-inf')
    else:
        att += mask
    return att


class SparseCSRTensor(torch.Tensor):

    @staticmethod
    def __new__(cls, row_offsets, column_indices, values, shape):
        kwargs = {}
        kwargs['device'] = values.device
        kwargs['dtype'] = values.dtype
        kwargs['layout'] = values.layout
        kwargs['requires_grad'] = values.requires_grad
        assert len(shape) == 3
        assert torch.__version__ > (1, 10), 'SparseCSRTensor requires PyTorch 1.11+'
        return torch.Tensor._make_wrapper_subclass(cls, shape, **kwargs)

    def __init__(self, row_offsets, column_indices, values, shape):
        assert row_offsets.ndim == 1
        assert column_indices.ndim == 1
        assert values.ndim == 2
        self.__row_offsets = row_offsets.contiguous()
        self.__row_indices = _diffsort(row_offsets)
        self.__column_indices = column_indices.contiguous()
        self.__values = values.contiguous()
        self.__transp_info = _get_transpose_info(self.shape[1], self.shape[2], self.__row_indices, self.__row_offsets, self.__column_indices)

    def __repr__(self):
        return f'sparse_csr_tensor(shape={self.shape}, values={self.__values})'

    @classmethod
    def from_dense(cls, matrix):
        values, row_indices, row_offsets, column_indices = _dense3d_to_sparse(matrix, matrix.device)
        return cls(row_offsets, column_indices, values, matrix.shape)

    @classmethod
    def from_sparse_coo(cls, arg0):
        """
        assert arg0.is_sparse
        x = arg0.coalesce()
        rows, cols = x.indices().unbind(0)
        vals = x.values()
        _coo_to_csr()
        """
        pass

    @classmethod
    def _wrap(cls, shape, values, row_indices, row_offsets, column_indices, _transp_info):
        matrix = cls.__new__(cls, row_offsets, column_indices, values, shape)
        matrix.__values = values
        matrix.__row_indices = row_indices
        matrix.__row_offsets = row_offsets
        matrix.__column_indices = column_indices
        matrix.__transp_info = _transp_info
        return matrix

    def values(self):
        return self.__values

    @property
    def _csr_row_indices(self):
        return self.__row_indices

    @property
    def _csr_row_offsets(self):
        return self.__row_offsets

    @property
    def _csr_column_indices(self):
        return self.__column_indices

    @property
    def _csr_transp_info(self):
        return self.__transp_info

    @classmethod
    def _bmm(cls, arg0, arg1):
        if not (isinstance(arg0, cls) and type(arg1) == torch.Tensor):
            return NotImplemented
        assert arg0.ndim == 3
        assert arg1.ndim == 3
        self = arg0
        b = arg1
        _, m, n = self.shape
        row_indices = self.__row_indices
        values = self.__values
        row_offsets = self.__row_offsets
        column_indices = self.__column_indices
        out = _csr_ops._spmm.apply(b, row_indices, values, row_offsets, column_indices, m, self.__transp_info)
        return out

    @classmethod
    def _softmax(cls, arg0, dim):
        if not (dim == -1 or dim == 2):
            return NotImplemented
        self = arg0
        _, m, n = self.shape
        row_indices = self.__row_indices
        values = self.__values
        row_offsets = self.__row_offsets
        column_indices = self.__column_indices
        out = _csr_ops._SparseSoftmax.apply(m, n, row_indices, values, row_offsets, column_indices)
        return cls._wrap(self.shape, out, row_indices, row_offsets, column_indices, self.__transp_info)

    @classmethod
    def _transpose(cls, arg0, dim0, dim1):
        if not (dim0 == 1 or dim0 == -2):
            return NotImplemented
        if not (dim1 == 2 or dim1 == -1):
            return NotImplemented
        B, m, n = arg0.shape
        values = arg0.__values
        output_row_indices, output_values, output_row_offsets, output_column_indices = _transpose_with_info(values, arg0.__transp_info)
        new_transp_info = _get_transpose_info(n, m, output_row_indices, output_row_offsets, output_column_indices)
        return cls._wrap((B, n, m), output_values, output_row_indices, output_row_offsets, output_column_indices, new_transp_info)

    @classmethod
    def _masked_matmul(cls, a, b, mask):
        if not (type(a) == torch.Tensor and type(b) == torch.Tensor):
            return NotImplemented
        assert mask.shape[1] == a.shape[1]
        assert mask.shape[2] == b.shape[2]
        row_indices = mask.__row_indices
        row_offsets = mask.__row_offsets
        column_indices = mask.__column_indices
        a = a.contiguous()
        out = _csr_ops._sddmm.apply(a, b.transpose(-2, -1).contiguous(), row_indices, row_offsets, column_indices, mask.__transp_info)
        return cls._wrap(mask.shape, out, row_indices, row_offsets, column_indices, mask.__transp_info)

    @classmethod
    def _to(cls, arg0, device):
        if isinstance(device, str):
            device = torch.device(device)
        assert isinstance(device, torch.device)
        return cls._wrap(arg0.shape, arg0.__values, arg0.__row_indices, arg0.__row_offsets, arg0.__column_indices, tuple(t for t in arg0.__transp_info))

    @classmethod
    def _copy(cls, arg0, arg1):
        if not (isinstance(arg0, cls) and isinstance(arg1, cls)):
            return NotImplemented
        assert arg0.shape == arg1.shape
        av0, av1 = arg0.__values, arg1.__values
        av0.resize_as_(av1).copy_(av1)
        av0, av1 = arg0.__row_indices, arg1.__row_indices
        av0.resize_as_(av1).copy_(av1)
        av0, av1 = arg0.__row_offsets, arg1.__row_offsets
        av0.resize_as_(av1).copy_(av1)
        av0, av1 = arg0.__column_indices, arg1.__column_indices
        av0.resize_as_(av1).copy_(av1)
        for v0, v1 in zip(arg0.__transp_info, arg1.__transp_info):
            v0.resize_as_(v1).copy_(v1)
        return arg0

    @classmethod
    def _equal(cls, arg0, arg1):
        if not (isinstance(arg0, cls) and isinstance(arg1, cls)):
            return NotImplemented
        if arg0.shape != arg1.shape:
            return False
        if not torch.equal(arg0.__values, arg1.__values):
            return False
        if not torch.equal(arg0.__row_offsets, arg1.__row_offsets):
            return False
        if not torch.equal(arg0.__column_indices, arg1.__column_indices):
            return False
        return True

    @classmethod
    def _to_dense(cls, arg0):
        _, m, n = arg0.shape
        shape = arg0.shape
        matrix = torch.zeros(shape, dtype=arg0.dtype, device=arg0.device)
        row_offsets = arg0.__row_offsets.long()
        column_indices = arg0.__column_indices.long()
        row_coo, _ = _csr_to_coo(m, n, row_offsets, column_indices)
        b_idxs = torch.arange(len(arg0.__values), device=arg0.device)[:, None]
        matrix[b_idxs, row_coo, column_indices] = arg0.__values
        return matrix

    @classmethod
    def _binary_op(cls, func, arg0, arg1):
        if not (isinstance(arg0, (cls, int, float)) and isinstance(arg1, (cls, int, float))):
            return NotImplemented
        v0, v1 = arg0, arg1
        if isinstance(arg0, cls):
            v0 = arg0.__values
        if isinstance(arg1, cls):
            v1 = arg1.__values
        if isinstance(arg0, cls) and isinstance(arg1, cls):
            msg = f'arg0 and arg1 need to have the same sparsity pattern in {func} (for now)'
            if not arg0.__row_offsets.shape == arg1.__row_offsets.shape:
                raise NotImplementedError(msg)
            if not arg0.__column_indices.shape == arg1.__column_indices.shape:
                raise NotImplementedError(msg)
            if not arg0.__values.shape == arg1.__values.shape:
                raise NotImplementedError(msg)
            if arg0.__row_offsets is not arg1.__row_offsets:
                raise NotImplementedError(msg)
            if arg0.__column_indices is not arg1.__column_indices:
                raise NotImplementedError(msg)
        out = func(v0, v1)
        return cls._wrap(arg0.shape, out, arg0.__row_indices, arg0.__row_offsets, arg0.__column_indices, arg0.__transp_info)

    @classmethod
    def _binary_op_slow(cls, func, arg0, arg1):
        v0, v1 = arg0, arg1
        if isinstance(arg0, cls):
            v0 = arg0.to_dense()
        if isinstance(arg1, cls):
            v1 = arg1.to_dense()
        out = func(v0, v1)
        return cls.from_dense(out)

    @classmethod
    def __torch_function__(cls, func, types, args=(), kwargs=None):
        if kwargs is None:
            kwargs = {}
        if func in [torch.Tensor.bmm, torch.bmm, torch.Tensor.__matmul__, torch.matmul, torch.Tensor.matmul]:
            assert len(args) == 2
            return cls._bmm(args[0], args[1])
        if func in [torch.Tensor.softmax, torch.nn.functional.softmax, torch.softmax]:
            return cls._softmax(args[0], kwargs['dim'])
        if func in [torch.Tensor.transpose, torch.transpose]:
            assert len(kwargs) == 0
            return cls._transpose(args[0], args[1], args[2])
        if func == masked_matmul:
            assert len(args) == 3
            return cls._masked_matmul(args[0], args[1], args[2])
        if func in [torch.Tensor.add, torch.add, torch.Tensor.__add__]:
            assert len(args) == 2
            if not (isinstance(args[0], cls) and isinstance(args[1], cls)):
                raise NotImplementedError(f'{func} with {type(args[0])} and {type(args[1])} not implemented')
            return cls._binary_op(func, args[0], args[1])
        if func in [torch.Tensor.mul, torch.mul, torch.Tensor.__mul__]:
            assert len(args) == 2
            return cls._binary_op(func, args[0], args[1])
        if func in [torch.Tensor.logical_and, torch.logical_and, torch.Tensor.__and__]:
            assert len(args) == 2
            return cls._binary_op_slow(func, args[0], args[1])
        if func in [torch.nn.functional.dropout, torch.dropout, torch.dropout_]:
            x = args[0]
            values = x.__values.clone()
            values = func(values, *args[1:], **kwargs)
            return cls._wrap(x.shape, values, x.__row_indices, x.__row_offsets, x.__column_indices, x.__transp_info)
        if func == torch.Tensor.to:
            assert len(args) >= 2
            return cls._to(args[0], args[1])
        if func in [torch.Tensor.copy_]:
            assert len(args) == 2
            return cls._copy(args[0], args[1])
        if func in [torch.Tensor.equal, torch.equal]:
            assert len(args) == 2
            return cls._equal(args[0], args[1])
        if func == torch.Tensor.to_dense:
            assert len(args) == 1
            return cls._to_dense(args[0])
        if func == torch.Tensor.detach:
            x = args[0]
            return cls._wrap(x.shape, x.__values.detach(), x.__row_indices, x.__row_offsets, x.__column_indices, x.__transp_info)
        if func == torch.Tensor.__deepcopy__:
            x = args[0]
            memo = args[1]
            return cls._wrap(x.shape, x.__values.__deepcopy__(memo), x.__row_indices.__deepcopy__(memo), x.__row_offsets.__deepcopy__(memo), x.__column_indices.__deepcopy__(memo), tuple(v.__deepcopy__(memo) for v in x.__transp_info))
        if func in [torch.Tensor.grad.__get__, torch.Tensor._grad.__get__]:
            assert len(args) == 1
            assert len(kwargs) == 0
            x = args[0]
            return cls._wrap(x.shape, x.__values.grad, x.__row_indices, x.__row_offsets, x.__column_indices, x.__transp_info)
        if func == torch.Tensor.requires_grad_:
            func(args[0].__values)
        with torch._C.DisableTorchFunction():
            ret = func(*args, **kwargs)
            if func in torch.overrides.get_default_nowrap_functions():
                return ret
            return torch._tensor._convert(ret, cls)
        return NotImplemented

    @classmethod
    def __torch_dispatch__(cls, func, types, args, kwargs):
        return NotImplemented


class SparseCS:

    def __init__(self, matrix, device=None):
        if device is None:
            device = torch.device('cpu')
        if matrix.ndim == 2:
            matrix = matrix[None]
        assert matrix.ndim == 3
        self._mat = SparseCSRTensor.from_dense(matrix)

    @property
    def device(self):
        return self._mat.device

    @property
    def ndim(self):
        return self._mat.ndim

    @property
    def dtype(self):
        return self._mat.dtype

    @property
    def is_sparse(self):
        return True

    @property
    def shape(self):
        return self._mat.shape[1:]

    @property
    def values(self):
        return self._mat.values()

    @property
    def row_indices(self):
        return self._mat._csr_row_indices

    @property
    def column_indices(self):
        return self._mat._csr_column_indices

    @property
    def row_offsets(self):
        return self._mat._csr_row_offsets

    @property
    def _transp_info(self):
        return self._mat._csr_transp_info

    @classmethod
    def wrap(cls, shape, values, row_indices, row_offsets, column_indices, _transp_info):
        matrix = cls.__new__(cls)
        _shape = (values.shape[0],) + shape
        csr_matrix = SparseCSRTensor._wrap(_shape, values, row_indices, row_offsets, column_indices, _transp_info)
        matrix._mat = csr_matrix
        return matrix

    @classmethod
    def _wrap(cls, csr_matrix):
        assert isinstance(csr_matrix, SparseCSRTensor)
        matrix = cls.__new__(cls)
        matrix._mat = csr_matrix
        return matrix

    def __mul__(self, other):
        assert isinstance(other, (int, float))
        return type(self)._wrap(self._mat * other)

    def __add__(self, other):
        assert isinstance(other, type(self))
        return type(self)._wrap(self._mat + other._mat)

    def matmul_with_mask(self, a, b):
        return type(self)._wrap(masked_matmul(a, b, self._mat))

    def softmax(self):
        out = torch.nn.functional.softmax(self._mat, -1)
        return type(self)._wrap(out)

    def spmm(self, b):
        out = torch.bmm(self._mat, b)
        return out

    def transpose(self):
        out = torch.transpose(self._mat, -2, -1)
        return type(self)._wrap(out)

    def to(self, device):
        assert isinstance(device, torch.device)
        out = self._mat
        return type(self)._wrap(out)

    def to_dense(self):
        return self._mat.to_dense()

    def logical_and(self, other: 'torch.Tensor'):
        assert not isinstance(other, SparseCS)
        out = torch.logical_and(self._mat, other)
        return type(self)._wrap(out)

    def __and__(self, other):
        return self.logical_and(other)


_USE_SPUTNIK = True


def sparsify(matrix):
    if _USE_SPUTNIK:
        return SparseCS(matrix)
    return matrix.to_sparse()


def maybe_sparsify(matrix) ->Any:
    if torch.count_nonzero(matrix).item() / matrix.numel() > _DENSITY_THRESHOLD:
        return AttentionMask.from_bool(matrix)
    return sparsify(matrix)


def _apply_dropout(att, dropout):
    if dropout is None:
        return att
    if _has_cpp_library:
        if isinstance(att, SparseCS):
            values = att.values.clone()
            values = dropout(values)
            att = SparseCS.wrap(att.shape, values, att.row_indices, att.row_offsets, att.column_indices, att._transp_info)
        elif att.is_sparse:
            att = att.coalesce()
            values = att.values().clone()
            values = dropout(values)
            att = torch.sparse_coo_tensor(att.indices(), values, att.shape)
        else:
            att = dropout(att)
        return att
    att = dropout(att)
    return att


def gpu_capabilities_older_than_70() ->bool:
    """Return True if the GPU's compute capability is older than SM70."""
    global _gpu_is_old
    if _gpu_is_old is None:
        for i in range(torch.cuda.device_count()):
            major, _ = torch.cuda.get_device_capability(f'cuda:{i}')
            if major < 7:
                _gpu_is_old = True
        if _gpu_is_old is None:
            _gpu_is_old = False
    return _gpu_is_old


def blocksparse_attention(q: 'torch.Tensor', k: 'torch.Tensor', v: 'torch.Tensor', dropout: 'Optional[torch.nn.Module]'=None, block_size: 'int'=128) ->torch.Tensor:
    orig_dim = q.dim()
    seq_len = q.shape[-2]
    layout_heads = 1
    assert seq_len % block_size == 0, 'Sequence length must be divisible by block size'
    if orig_dim == 3:
        if layout_heads == 1:
            q = q.unsqueeze(1)
            k = k.unsqueeze(1)
            v = v.unsqueeze(1)
        else:
            q = q.unsqueeze(0)
            k = k.unsqueeze(0)
            v = v.unsqueeze(0)
    blocksparse_attention = _retrieve_blocksparse(layout_heads, seq_len, block_size)
    if isinstance(dropout, torch.nn.Dropout):
        blocksparse_attention.attn_drop = dropout
    else:
        blocksparse_attention.attn_drop = torch.nn.Dropout(0.0)
    att = blocksparse_attention(q, k, v)
    if orig_dim == 3:
        return att.flatten(0, 1)
    return att


def bmm(a: 'torch.Tensor', b: 'torch.Tensor') ->torch.Tensor:
    if _has_cpp_library:
        if isinstance(a, SparseCS):
            return a.spmm(b)
        if a.is_sparse:
            return _sparse_bmm(a, b)
    return a @ b


def _broadcast_batch(mask, batch_size):
    if mask.ndim == 3:
        return mask
    assert mask.ndim == 2
    mask = mask.coalesce()
    values = mask.values()
    indices = mask.indices()
    nnz = len(values)
    indices = indices.repeat(1, batch_size)
    batch_indices = torch.arange(batch_size, device=indices.device)
    batch_indices = batch_indices[:, None].expand(batch_size, nnz).flatten()
    indices = torch.cat([batch_indices[None, :], indices], dim=0)
    values = values.repeat(batch_size)
    size = (batch_size,) + mask.shape
    return torch.sparse_coo_tensor(indices, values, size)


def _matmul_with_mask(a: 'torch.Tensor', b: 'torch.Tensor', mask: "Optional[Union[torch.Tensor, 'SparseCS']]") ->torch.Tensor:
    if mask is None:
        return a @ b
    if _has_cpp_library and mask.dtype == torch.bool:
        if isinstance(mask, SparseCS):
            return mask.matmul_with_mask(a, b)
        if mask.is_sparse:
            mask = _broadcast_batch(mask, a.shape[0])
            mask = mask
        return torch.ops.xformers.matmul_with_mask(a, b, mask)
    if _has_cpp_library:
        assert not isinstance(mask, SparseCS)
    att = a @ b
    if mask.dtype == torch.bool:
        assert not isinstance(mask, SparseCS)
        if mask.ndim == 2:
            mask = mask.unsqueeze(0).expand(att.shape[0], -1, -1)
        att[~mask] = float('-inf')
    else:
        if not isinstance(mask, SparseCS) and mask.ndim == 3 and mask.shape[0] != att.shape[0] and att.shape[0] % mask.shape[0] == 0:
            repeat_factor = att.shape[0] // mask.shape[0]
            mask = mask.repeat([repeat_factor, 1, 1])
            logger.info('Mismatched batch dimensions for mask, repeating mask.')
        att += mask
    return att


def scaled_query_key_softmax(q: 'torch.Tensor', k: 'torch.Tensor', att_mask: "Optional[Union[AttentionMask, 'SparseCS', torch.Tensor]]") ->torch.Tensor:
    q = q / math.sqrt(k.size(-1))
    if att_mask is not None and isinstance(att_mask, AttentionMask):
        mask: 'Optional[Union[SparseCS, torch.Tensor]]' = att_mask.values
    else:
        mask = att_mask
    att = _matmul_with_mask(q, k.transpose(-2, -1), mask)
    is_causal = isinstance(att_mask, AttentionMask) and att_mask.is_causal
    att = _softmax(att, causal=is_causal)
    return att


def scaled_dot_product_attention(q: 'torch.Tensor', k: 'torch.Tensor', v: 'torch.Tensor', att_mask: "Optional[Union[AttentionMask, 'SparseCS', torch.Tensor]]", dropout: 'Optional[torch.nn.Module]'=None, block_size: 'int'=128) ->torch.Tensor:
    autocast_disabled = _has_cpp_library and isinstance(att_mask, SparseCS) or att_mask is not None and att_mask.is_sparse
    seq_len = q.shape[-2]
    switch_to_blocksparse = _is_blocksparse_available and (att_mask is not None and not att_mask.is_sparse) and (isinstance(att_mask, AttentionMask) and att_mask.is_causal) and (q.dtype == torch.float16 or torch.is_autocast_enabled()) and not seq_len % block_size and q.shape[-2] == k.shape[-2]
    if switch_to_blocksparse:
        logger.info('Switching causal attention to Triton blocksparse...')
        return blocksparse_attention(q, k, v, dropout, block_size)
    with (torch.amp.autocast(enabled=False) if autocast_disabled else nullcontext()):
        if autocast_disabled:
            q, k, v = q.float(), k.float(), v.float()
        att = scaled_query_key_softmax(q, k, att_mask=att_mask)
        att = _apply_dropout(att, dropout)
        y = bmm(att, v)
    return y


class GlobalAttention(Attention):

    def __init__(self, dropout: 'float', attention_query_mask: 'torch.Tensor', causal: 'bool'=False, force_sparsity: 'bool'=False, *_, **__):
        """
        Global attention, as proposed for instance in BigBird_ or Longformer_.

        Global means in that case that the queries positively labelled in the ```attention_query_mask``` can attend
        to all the other queries. The queries negatively labelled in the ```attention_query_mask``` cannot attend to
        any other query.

        This implementation is sparse-aware, meaning that the empty attention parts will not be represented in memory.

        Args:
            dropout (float): probability of an element to be zeroed
            attention_query_mask (torch.Tensor): if true, this query can attend to all the others

        """
        super().__init__()
        assert attention_query_mask.dtype == torch.bool, 'A boolean mask is expected'
        assert attention_query_mask.shape[1] == 1 and attention_query_mask.shape[0] > attention_query_mask.shape[1], 'A N x 1 query mask is expected'
        self.attn_drop = nn.Dropout(dropout, inplace=False)
        self.attention_mask = global_token_pattern(attention_query_mask[:, 0])
        self.force_sparsity = force_sparsity
        if causal:
            self.attention_mask &= causal_1d_pattern(attention_query_mask.shape[1])
        self.attention_mask = sparsify(self.attention_mask) if self.force_sparsity else maybe_sparsify(self.attention_mask)
        self.requires_same_k_q_dimensions = True
        self.supports_attention_mask = False
        self.supports_key_padding_mask = False

    def forward(self, q: 'torch.Tensor', k: 'torch.Tensor', v: 'torch.Tensor', att_mask: 'Optional[Union[torch.Tensor, AttentionMask]]'=None, *_, **__):
        if self.attention_mask.device != q.device:
            self.attention_mask = self.attention_mask
        if att_mask is not None:
            if att_mask.dtype == torch.bool and isinstance(self.attention_mask, AttentionMask):
                if not isinstance(att_mask, AttentionMask):
                    att_mask = AttentionMask.from_bool(att_mask)
                mask = self.attention_mask + att_mask
            else:
                mask = self.attention_mask & att_mask
        else:
            mask = self.attention_mask
        seq_len = q.shape[-2]
        q_, k_, v_ = map(lambda x: self._maybe_pad_sequence(x, mask), (q, k, v))
        att = scaled_dot_product_attention(q=q_, k=k_, v=v_, att_mask=mask, dropout=self.attn_drop)
        return att[:, :seq_len, :]


def calc_rel_pos(n: 'int'):
    rel_pos = torch.arange(n)[None, :] - torch.arange(n)[:, None]
    rel_pos += n - 1
    return rel_pos


class LambdaLayer(Attention):

    def __init__(self, dropout: 'float', seq_len: 'int', dim_head: 'int', *_, **__):
        """
        Attention approximation using Lambda layers, from
        "Lambda networks: modeling long-range interactions without attention.", Bello, I. (2021).
        """
        super().__init__()
        self.rel_pos_emb = torch.nn.Parameter(torch.randn(2 * seq_len - 1, int(dim_head)))
        self.rel_pos = calc_rel_pos(seq_len)
        self.attn_drop = torch.nn.Dropout(dropout, inplace=True)
        self.requires_same_k_q_dimensions = True
        self.supports_attention_mask = False
        self.supports_key_padding_mask = False

    def forward(self, q: 'torch.Tensor', k: 'torch.Tensor', v: 'torch.Tensor', *args, **kwargs):
        """..NOTE: We're reusing the einsum notation suggested by the paper, changed in that
        heads are folded in the batch dimension"""
        content_lambda = torch.einsum('bnk,bnv->bkv', torch.softmax(k, dim=-1), v)
        content_output = torch.einsum('bnk,bkv->bnv', q, content_lambda)
        rel_pos_emb = self.rel_pos_emb[self.rel_pos]
        seq_len = q.shape[1]
        rel_pos_emb = rel_pos_emb[:seq_len, :seq_len, :]
        position_lambdas = torch.einsum('mnk,bnv->bnkv', rel_pos_emb, v)
        position_output = (q.unsqueeze(2) @ position_lambdas).squeeze()
        att = content_output + position_output
        att = self.attn_drop(att)
        return att


class LinformerAttention(Attention):

    def __init__(self, dropout: 'float', seq_len: 'int', k: 'Optional[int]'=None, *args, **kwargs):
        """
        Linformer attention mechanism,
        from `Linformer: Self-Attention with Linear Complexity`_, Wang et al (2020).
        The original notation is kept as is.

        .. _`Linformer: Self-Attention with Linear Complexity` : https://arxiv.org/abs/2006.04768v2
        """
        super().__init__()
        if k is None:
            k = seq_len // 4
        self.k = k
        self.E = nn.Linear(seq_len, k, bias=False)
        self.F = nn.Linear(seq_len, k, bias=False)
        self.attn_drop = nn.Dropout(dropout, inplace=False)
        self.seq_len = seq_len
        self.requires_same_k_q_dimensions = True
        self.supports_attention_mask = False

    def forward(self, q: 'torch.Tensor', k: 'torch.Tensor', v: 'torch.Tensor', *args, **kwargs):
        padding = 0
        if q.shape[1] < self.seq_len:
            padding = self.seq_len - q.shape[1]
            pad_dims = 0, 0, 0, padding
            q = torch.nn.functional.pad(q, pad_dims)
            k = torch.nn.functional.pad(k, pad_dims)
            v = torch.nn.functional.pad(v, pad_dims)
        k_projected = self.E(k.transpose(-2, -1)).transpose(-2, -1)
        v_projected = self.F(v.transpose(-2, -1)).transpose(-2, -1)
        y = scaled_dot_product_attention(q=q, k=k_projected, v=v_projected, att_mask=None, dropout=self.attn_drop)
        y = self.attn_drop(y)
        return y[:, :-padding, :] if padding > 0 else y


def _generate_nd_grid(*sizes):
    coords = [torch.arange(s) for s in sizes]
    return torch.meshgrid(*coords)


def local_nd_distance(*sizes, p=2.0, weights=None):
    if weights is None:
        weights = (1,) * len(sizes)
    assert len(sizes) == len(weights)
    grid = _generate_nd_grid(*sizes)
    grid = [(i.flatten() * w) for i, w in zip(grid, weights)]
    grid = torch.stack(grid, dim=1).float()
    d = torch.cdist(grid, grid, p=p)
    return d


def local_nd_pattern(*sizes, distance, p=2.0):
    d = local_nd_distance(*sizes, p=p)
    return d < distance


def local_1d_pattern(attn_size: 'int', window_size: 'int') ->torch.Tensor:
    assert window_size % 2 == 1, 'The window size is assumed to be odd (counts self-attention + 2 wings)'
    h_win_size = window_size // 2 + 1
    return local_nd_pattern(attn_size, distance=h_win_size, p=1.0)


class LocalAttention(Attention):

    def __init__(self, dropout: 'float'=0.0, causal: 'bool'=False, window_size: 'int'=5, force_sparsity: 'bool'=False, *args, **kwargs):
        """
        An implementation of a sliding window attention, as proposed in RoutingTransformer_, LongFormer_ or BigBird_


        Args:
            dropout (float): the probability of an output to be randomly dropped at training time
            causal (bool): apply a causal mask, in that the attention cannot be applied to the future
            window_size (int): the overall window size for local attention.
                Odd number is expected if the mask is not causal, as the window size will be evenly
                distributed on both sides of each query


        .. _RoutingTransformer: https://arxiv.org/pdf/2003.05997.pdf

        .. _BigBird: https://arxiv.org/pdf/2007.14062.pdf

        .. _Longformer: https://arxiv.org/pdf/2004.05150.pdf

        """
        super().__init__()
        self.attn_drop = nn.Dropout(dropout, inplace=False)
        self.causal = causal
        self.force_sparsity = force_sparsity
        if not self.causal:
            assert window_size % 2 == 1, 'The window size is assumed to be odd (counts self-attention + 2 wings)'
        self.window_size = window_size
        self.attention_mask: 'Optional[torch.Tensor]' = None
        self.requires_same_k_q_dimensions = True
        self.supports_attention_mask = True
        self.supports_key_padding_mask = False

    def _get_local_mask(self, shape: 'torch.Size') ->torch.Tensor:
        window_size = self.window_size * 2 + 1 if self.causal else self.window_size
        mask = local_1d_pattern(shape[1], window_size)
        if self.causal:
            mask &= causal_1d_pattern(shape[1])
        mask = sparsify(mask) if self.force_sparsity else maybe_sparsify(mask)
        return mask

    def forward(self, q: 'torch.Tensor', k: 'torch.Tensor', v: 'torch.Tensor', att_mask: 'Optional[Union[torch.Tensor, AttentionMask]]'=None, *args, **kwargs):
        if self.attention_mask is None or self.attention_mask.shape[1] != q.shape[1]:
            self.attention_mask = self._get_local_mask(q.shape)
        if att_mask is None:
            mask = self.attention_mask
        else:
            if isinstance(att_mask, AttentionMask):
                att_mask = att_mask.to_bool()
            mask = self.attention_mask & att_mask
        return scaled_dot_product_attention(q=q, k=k, v=v, att_mask=mask, dropout=self.attn_drop)


class AvgPool(nn.Module):

    def __init__(self, n: 'int'):
        super().__init__()
        self.n = n

    def forward(self, x: 'torch.Tensor'):
        seq_len = x.shape[1]
        head_dim = x.shape[2]
        segments = seq_len // self.n
        assert segments > 0, 'num_landmarks should be smaller than the sequence length'
        if seq_len % self.n == 0:
            return x.reshape(-1, self.n, segments, head_dim).mean(dim=-2)
        n_round = self.n - seq_len % self.n
        x_avg_round = x[:, :n_round * segments, :].reshape(-1, n_round, segments, head_dim).mean(dim=-2)
        x_avg_off = x[:, n_round * segments:, :].reshape(-1, self.n - n_round, segments + 1, head_dim).mean(dim=-2)
        return torch.cat((x_avg_round, x_avg_off), dim=-2)


def bool_mask_to_additive(mask: 'torch.Tensor', dtype: 'Optional[torch.dtype]'=torch.float32) ->torch.Tensor:
    assert mask.dtype == torch.bool, 'This util is meant to convert in between bool masks and additive ones'
    mask_ = torch.zeros_like(mask, dtype=dtype)
    mask_[~mask] = float('-inf')
    return mask_


def iterative_pinv(softmax_mat: 'torch.Tensor', n_iter=6, pinverse_original_init=False):
    """
    Computing the Moore-Penrose inverse.
    Use an iterative method from (Razavi et al. 2014) to approximate the Moore-Penrose inverse via efficient
    matrix-matrix multiplications.
    """
    i = torch.eye(softmax_mat.size(-1), device=softmax_mat.device, dtype=softmax_mat.dtype)
    k = softmax_mat
    if pinverse_original_init:
        v = 1 / torch.max(torch.sum(k, dim=-2)) * k.transpose(-1, -2)
    else:
        v = 1 / torch.max(torch.sum(k, dim=-2), dim=-1).values[:, None, None] * k.transpose(-1, -2)
    for _ in range(n_iter):
        kv = torch.matmul(k, v)
        v = torch.matmul(0.25 * v, 13 * i - torch.matmul(kv, 15 * i - torch.matmul(kv, 7 * i - kv)))
    return v


def _reshape_key_padding_mask(key_padding_mask: 'torch.Tensor', batch_size: 'int', src_len: 'int', num_heads: 'int') ->torch.Tensor:
    assert key_padding_mask.shape == (batch_size, src_len)
    key_padding_mask = key_padding_mask.view(batch_size, 1, 1, src_len).expand(-1, num_heads, -1, -1).reshape(batch_size * num_heads, 1, src_len)
    return key_padding_mask


def reshape_key_padding_mask(key_padding_mask: 'torch.Tensor', batched_dim: 'int') ->torch.Tensor:
    assert key_padding_mask.ndim == 2
    batch_size, src_len = key_padding_mask.size()
    num_heads = batched_dim // batch_size
    return _reshape_key_padding_mask(key_padding_mask, batch_size, src_len, num_heads)


class NystromAttention(Attention):

    def __init__(self, dropout: 'float', num_heads: 'int', num_landmarks: 'int'=64, landmark_pooling: 'Optional[nn.Module]'=None, causal: 'bool'=False, use_razavi_pinverse: 'bool'=True, pinverse_original_init: 'bool'=False, inv_iterations: 'int'=6, v_skip_connection: 'Optional[nn.Module]'=None, conv_kernel_size: 'Optional[int]'=None, *args, **kwargs):
        """
        Nystrom attention mechanism, from Nystromformer_.
        ::

            "A Nystrom-based Algorithm for Approximating Self-Attention."
            Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., Singh, V. (2021)

            Reference codebase: https://github.com/mlpen/Nystromformer

        .. _Nystromformer: https://arxiv.org/pdf/2102.03902.pdf

        """
        super().__init__()
        self.requires_separate_masks = True
        self.num_landmarks = num_landmarks
        self.num_heads = num_heads
        self.use_razavi_pinverse = use_razavi_pinverse
        self.pinverse_original_init = pinverse_original_init
        self.inv_iterations = inv_iterations
        self.attn_drop = nn.Dropout(dropout)
        self.skip_connection = v_skip_connection
        self.causal = causal
        if self.skip_connection is None and conv_kernel_size is not None:
            self.skip_connection = nn.Conv2d(in_channels=self.num_heads, out_channels=self.num_heads, kernel_size=(conv_kernel_size, 1), padding=(conv_kernel_size // 2, 0), bias=False, groups=self.num_heads)
        if landmark_pooling is not None:
            self.landmark_pooling = landmark_pooling
        else:
            self.landmark_pooling = AvgPool(n=self.num_landmarks)
        self.causal_mask_1: 'Optional[torch.Tensor]' = None
        self.causal_mask_2: 'Optional[torch.Tensor]' = None
        self.causal_mask_3: 'Optional[torch.Tensor]' = None
        self.supports_attention_mask = False
        self.supports_key_padding_mask = True

    def forward(self, q: 'torch.Tensor', k: 'torch.Tensor', v: 'torch.Tensor', key_padding_mask: 'Optional[torch.Tensor]'=None, *args, **kwargs):
        """
        key_padding_mask    Only a key padding mask is accepted here. The size must be (batch size, sequence length) or
                            (batch size * num_heads, 1, sequence length). If dimensions are not correct, the mask will
                            be ignored. An additive mask is expected, meaning float values using "-inf" to mask values
        """
        batched_dim = k.size(0)
        seq_len = k.size(-2)
        tt = {'dtype': q.dtype, 'device': q.device}
        if key_padding_mask is not None:
            if key_padding_mask.dtype == torch.bool:
                logger.warning('Bool mask found, but an additive mask is expected. Converting but this is slow')
                key_padding_mask = bool_mask_to_additive(key_padding_mask)
            if key_padding_mask.ndim == 2:
                key_padding_mask = reshape_key_padding_mask(key_padding_mask, batched_dim)
            zeros = torch.zeros_like(key_padding_mask)
            ones = torch.ones_like(key_padding_mask)
            is_masked = torch.isinf(-key_padding_mask)
            _mask = torch.where(is_masked, zeros, ones)
            _mask = _mask.transpose(2, 1)
            assert _mask.shape == (batched_dim, q.shape[1], 1)
            q = q * _mask
            k = k * _mask
            assert key_padding_mask.size() == (batched_dim, 1, seq_len), f'key_padding_mask has invalid dimensions {key_padding_mask.size()}. Must have dimensions {batched_dim, 1, seq_len} or (batch_size, {seq_len}).'
        if self.num_landmarks >= seq_len:
            mask: 'Optional[torch.Tensor]' = None
            if self.causal:
                mask = self._triu_mask(batched_dim, seq_len, seq_len, **tt)
            if key_padding_mask is not None:
                mask = key_padding_mask if mask is None else mask + key_padding_mask
            x = scaled_dot_product_attention(q=q, k=k, v=v, att_mask=mask)
        else:
            q_landmarks = self.landmark_pooling(q)
            k_landmarks = self.landmark_pooling(k)
            if self.causal and (self.causal_mask_1 is None or (batched_dim, seq_len, self.num_landmarks) != self.causal_mask_1.size()):
                self.causal_mask_1 = self._triu_mask(batched_dim, seq_len, self.num_landmarks, **tt)
                self.causal_mask_2 = self._triu_mask(batched_dim, self.num_landmarks, self.num_landmarks, **tt)
                self.causal_mask_3 = self._triu_mask(batched_dim, self.num_landmarks, seq_len, **tt)
            mask_3: 'Optional[torch.Tensor]' = self.causal_mask_3
            if key_padding_mask is not None:
                mask_3 = key_padding_mask if mask_3 is None else mask_3 + key_padding_mask
            kernel_1 = scaled_query_key_softmax(q=q, k=k_landmarks, att_mask=None)
            kernel_2 = scaled_query_key_softmax(q=q_landmarks, k=k_landmarks, att_mask=None)
            kernel_3 = scaled_dot_product_attention(q=q_landmarks, k=k, v=v, att_mask=mask_3)
            kernel_2_inv = iterative_pinv(kernel_2, self.inv_iterations, self.pinverse_original_init) if self.use_razavi_pinverse else torch.linalg.pinv(kernel_2)
            x = torch.matmul(torch.matmul(kernel_1, kernel_2_inv), kernel_3)
        if self.skip_connection:
            v_conv = self.skip_connection(v.reshape(-1, self.num_heads, v.size(-2), v.size(-1)))
            x += v_conv.reshape(-1, v_conv.size(-2), v_conv.size(-1))
        x = self.attn_drop(x)
        return x

    def _triu_mask(self, dim_1: 'int', dim_2: 'int', dim_3: 'int', **kwargs) ->torch.Tensor:
        device = kwargs['device']
        dtype = kwargs['dtype']
        return torch.triu(torch.ones(dim_2, dim_3, dtype=dtype, device=device) * float('-inf'), diagonal=1).expand(dim_1, -1, -1)


class LandmarkSelection(str, Enum):
    Orthogonal = 'orthogonal'
    KMeans = 'kmeans'
    KMeans_Spherical = 'kmeans_spherical'
    Random = 'random'


class OrthoFormerAttention(Attention):

    def __init__(self, dropout: 'float', num_landmarks: 'int'=32, subsample_fraction: 'float'=1.0, landmark_selection: 'LandmarkSelection'=LandmarkSelection.Orthogonal, *args, **kwargs):
        """
        Orthoformer_ attention mechanism.
        ::

            "Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers"
            Patrick, M., Campbell, D., Asano, Y., Misra, I., Metze, F., Feichtenhofer,
            C., Vedaldi, A., Henriques, J. (2021)

            Reference codebase: https://github.com/facebookresearch/Motionformer

        .. _Orthoformer: https://arxiv.org/abs/2106.05392

        """
        super().__init__()
        self.num_landmarks = num_landmarks
        self.attn_drop = nn.Dropout(dropout)
        self.subsample_fraction = subsample_fraction
        self.landmark_selection = landmark_selection
        self.supports_attention_mask = True
        self.supports_key_padding_mask = False

    def forward(self, q: 'torch.Tensor', k: 'torch.Tensor', v: 'torch.Tensor', att_mask: 'Optional[Union[AttentionMask, torch.Tensor]]'=None, *args, **kwargs):
        N = k.shape[1]
        if self.num_landmarks == N:
            x = scaled_dot_product_attention(q, k, v, att_mask)
        else:
            with torch.no_grad(), profiler.record_function('select landmarks'):
                if self.landmark_selection == LandmarkSelection.Orthogonal:
                    landmarks = self._compute_orthogonal_landmarks(q)
                elif self.landmark_selection == LandmarkSelection.Random:
                    half_L = self.num_landmarks // 2
                    landmarks_q = q[:, torch.randint(q.size(1), (half_L,)), :]
                    landmarks_k = k[:, torch.randint(k.size(1), (half_L,)), :]
                    landmarks = torch.cat((landmarks_q, landmarks_k), dim=-2)
                elif self.landmark_selection == LandmarkSelection.KMeans:
                    landmarks = self._cluster_landmarks(q)
                elif self.landmark_selection == LandmarkSelection.KMeans_Spherical:
                    landmarks = self._cluster_landmarks(q, spherical=True)
            if att_mask is not None:
                logger.warning('Orthoformer: attention mask passed alongside with using landmarks to reduce dimensions.                     The two are typically not compatible')
                att_mask = None
            kernel_1 = scaled_query_key_softmax(q, landmarks, att_mask)
            kernel_2 = scaled_query_key_softmax(landmarks, k, att_mask)
            x = torch.matmul(kernel_1, torch.matmul(kernel_2, v))
        x = self.attn_drop(x)
        return x

    def _cluster_landmarks(self, q: 'torch.Tensor', spherical: 'bool'=False, num_iters: 'int'=6) ->torch.Tensor:
        """
        Construct set of landmarks by recursively selecting new landmarks
        that are maximally orthogonal to the existing set.
        Returns near orthogonal landmarks with shape (B, M, D).
        """
        num_landmarks = min(self.num_landmarks, q.shape[1])
        if self.subsample_fraction < 1.0:
            num_samples = max(int(self.subsample_fraction * q.size(-2)), num_landmarks)
            q_samples = q[:, torch.randint(q.size(-2), (num_samples,)), :]
        else:
            q_samples = q
        if spherical:
            q_samples_normalized = Fn.normalize(q_samples, p=2, dim=-1)
            landmarks = self._kmeans_spherical(q_samples_normalized, num_landmarks, num_iters)
        else:
            landmarks = self._kmeans(q_samples, num_landmarks, num_iters)
        return landmarks

    def _kmeans(self, x: 'torch.Tensor', K: 'int', num_iters: 'int'=10):
        """
        Arguments:
            x: (B, N, D)
            K: number of clusters
            num_iters: the number of kmeans updates
        """
        B, N, D = x.size()
        assert K <= N, f'{K} > {N}'
        c = x[:, torch.randperm(N, device=x.device)[:K], :].clone()
        with profiler.record_function('kmeans'):
            x_i = x.view(B, N, 1, D)
            c_j = c.view(B, 1, K, D)
            counts = c.new_zeros(B, K)
            ones = x.new_ones((B, N))
            for _ in range(num_iters):
                D_ij = ((x_i - c_j) ** 2).sum(-1)
                cl = D_ij.argmin(dim=-1, keepdim=True).long()
                c.zero_()
                c.scatter_add_(-2, cl.repeat(1, 1, D), x)
                counts.fill_(1e-06)
                counts.scatter_add_(-1, cl.squeeze(-1), ones)
                c.divide_(counts.unsqueeze(-1))
        return c

    def _kmeans_spherical(self, x: 'torch.Tensor', K: 'int', num_iters=10):
        """
        Arguments:
            x: (B, N, D)
        """
        B, N, D = x.size()
        assert K <= N, f'{K} > {N}'
        c = x[:, torch.randperm(N, device=x.device)[:K], :].clone()
        with profiler.record_function('kmeans_spherical'):
            counts = c.new_zeros(B, K)
            ones = x.new_ones((B, N))
            for _ in range(num_iters):
                D_ij = torch.matmul(x, c.transpose(-2, -1))
                cl = D_ij.argmax(dim=-1, keepdim=True).long()
                c.zero_()
                c.scatter_add_(-2, cl.repeat(1, 1, D), x)
                counts.fill_(1e-06)
                counts.scatter_add_(-1, cl.squeeze(-1), ones)
                c.divide_(counts.unsqueeze(-1))
                c = Fn.normalize(c, p=2, dim=-1)
        return c

    def _compute_orthogonal_landmarks(self, q: 'torch.Tensor') ->torch.Tensor:
        """
        Construct set of landmarks by recursively selecting new landmarks
        that are maximally orthogonal to the existing set.
        Returns near orthogonal landmarks with shape (B, M, D).
        """
        if self.subsample_fraction < 1.0:
            num_samples = max(int(self.subsample_fraction * q.size(-2)), self.num_landmarks)
            q_samples = q[:, torch.randint(q.size(-2), (num_samples,), device=q.device), :]
        else:
            q_samples = q
        q_samples_normalized = Fn.normalize(q_samples, p=2, dim=-1)
        B, N, D = q_samples_normalized.shape
        selected_mask = torch.zeros((B, N, 1), device=q_samples_normalized.device)
        landmark_mask = torch.ones((B, 1, 1), dtype=selected_mask.dtype, device=q_samples_normalized.device)
        random_idx = torch.randint(q_samples_normalized.size(-2), (B, 1, 1), device=q_samples_normalized.device)
        selected_mask.scatter_(-2, random_idx, landmark_mask)
        selected_landmarks = torch.empty((B, self.num_landmarks, D), device=q_samples_normalized.device, dtype=q_samples_normalized.dtype)
        selected_landmarks[:, 0, :] = q_samples_normalized[torch.arange(q_samples_normalized.size(0)), random_idx.view(-1), :].view(B, D)
        cos_sims = torch.empty((B, N, self.num_landmarks), device=q_samples_normalized.device, dtype=q_samples_normalized.dtype)
        for M in range(1, self.num_landmarks):
            with profiler.record_function('find new landmark'):
                cos_sims[:, :, M - 1] = torch.einsum('b n d, b d -> b n', q_samples_normalized, selected_landmarks[:, M - 1, :]).abs()
                cos_sim_set = cos_sims[:, :, :M]
                cos_sim_set.view(-1, M)[selected_mask.flatten().bool(), :] = 10
                selected_landmark_idx = cos_sim_set.amax(-1).argmin(-1)
                selected_landmarks[:, M, :] = q_samples_normalized[torch.arange(q_samples_normalized.size(0)), selected_landmark_idx, :].view(B, D)
                selected_mask.scatter_(-2, selected_landmark_idx.unsqueeze(-1).unsqueeze(-1), landmark_mask)
        landmarks = torch.masked_select(q_samples, selected_mask.bool()).reshape(B, -1, D)
        return landmarks


def random_pattern(attn_size: 'int', sparsity: 'float') ->torch.Tensor:
    assert 0 < sparsity < 1
    mask = torch.rand(attn_size, attn_size) > sparsity
    return mask


class RandomAttention(Attention):

    def __init__(self, dropout: 'float', causal: 'bool'=False, r: 'float'=0.01, constant_masking: 'bool'=True, force_sparsity: 'bool'=False, *args, **kwargs):
        """
        "Random" attention, as proposed for instance in BigBird_.
        Random means in that case that each query can attend to a random set of keys.
        This implementation is sparse-aware, meaning that the empty attention parts will not be represented in memory.

        Args:
            r (float): the ratio in [0,1] of keys that the query can attend to
            constant_masking (bool): if true, keep the same random set for all queries.

        .. _BigBird: https://arxiv.org/pdf/2007.14062.pdf

        """
        super().__init__()
        self.attn_drop = nn.Dropout(dropout, inplace=False)
        self.causal = causal
        self.r = r
        self.rand_attention_mask: 'Optional[torch.Tensor]' = None
        self.constant_masking = constant_masking
        self.force_sparsity = force_sparsity
        self.supports_attention_mask = True
        self.supports_key_padding_mask = False
        self.requires_same_k_q_dimensions = True

    def _get_rand_mask(self, shape: 'torch.Size') ->torch.Tensor:
        sparsity = 1 - self.r
        mask = random_pattern(shape[1], sparsity=sparsity)
        if self.causal:
            mask &= causal_1d_pattern(shape[1])
        mask = sparsify(mask) if self.force_sparsity else maybe_sparsify(mask)
        return mask

    def forward(self, q: 'torch.Tensor', k: 'torch.Tensor', v: 'torch.Tensor', att_mask: 'Optional[Union[torch.Tensor, AttentionMask]]'=None, *args, **kwargs):
        if not self.constant_masking or self.rand_attention_mask is None:
            self.rand_attention_mask = self._get_rand_mask(q.shape)
        if att_mask is not None:
            if att_mask.dtype == torch.bool and isinstance(self.rand_attention_mask, AttentionMask):
                mask = self.rand_attention_mask + AttentionMask.from_bool(att_mask)
            else:
                if isinstance(att_mask, AttentionMask):
                    att_mask = att_mask.to_bool()
                mask = self.rand_attention_mask & att_mask
        else:
            mask = self.rand_attention_mask
        seq_len = q.shape[-2]
        q_, k_, v_ = map(lambda x: self._maybe_pad_sequence(x, mask), (q, k, v))
        att = scaled_dot_product_attention(q=q_, k=k_, v=v_, att_mask=mask, dropout=self.attn_drop)
        return att[:, :seq_len, :]


class ScaledDotProduct(Attention):
    """
    Implementing the Scaled Dot-Product attention proposed in
    `Attention is all you need`_, Vaswani et al.

    .. _`Attention is all you need`: https://arxiv.org/abs/1706.03762v5
    """
    mask: 'Optional[AttentionMask]'

    def __init__(self, dropout: 'float'=0.0, causal: 'bool'=False, seq_len: 'Optional[int]'=None, to_seq_len: 'Optional[int]'=None, *args, **kwargs):
        super().__init__()
        self.attn_drop = nn.Dropout(dropout, inplace=False)
        self.causal = causal
        self.seq_len = seq_len
        if causal and seq_len is not None:
            self.mask = AttentionMask.make_causal(seq_len, to_seq_len)
        else:
            self.mask = None
        self.supports_attention_mask = True
        self.supports_key_padding_mask = False

    def forward(self, q: 'torch.Tensor', k: 'torch.Tensor', v: 'torch.Tensor', att_mask: 'Optional[Union[AttentionMask, torch.Tensor]]'=None, *args, **kwargs) ->torch.Tensor:
        """
        att_mask    A 2D or 3D mask which ignores attention at certain positions.

                    - If the mask is boolean, a value of True will keep the value,
                        while a value of False will mask the value.

                        Key padding masks (dimension: batch x sequence length) and attention masks
                        (dimension: sequence length x sequence length OR batch x sequence length x sequence length)
                        can be combined and passed in here. Method maybe_merge_masks provided in the utils can be
                        used for that merging.

                    - If the mask has the float type, then an additive mask is expected (masked values are -inf)

        """
        if att_mask is not None and isinstance(att_mask, torch.Tensor):
            att_mask = AttentionMask.from_bool(att_mask) if att_mask.dtype == torch.bool else AttentionMask(att_mask, is_causal=False)
        mask = self.mask
        if self.causal and self.mask is None:
            mask = AttentionMask.make_causal(seq_len=q.shape[-2], to_seq_len=q.shape[-2], device=q.device, dtype=q.dtype)
        if mask is not None:
            mask = mask
            att_mask = att_mask + mask if att_mask is not None else mask
        if att_mask is not None and q.shape[-2] == k.shape[-2] and q.shape[-2] < att_mask.shape[1]:
            if isinstance(att_mask, AttentionMask):
                att_mask = att_mask.make_crop(seq_len=q.shape[-2])
            else:
                logger.error('Mismatching sparse attention mask and sequence length.' + ' Please pad the inputs or adjust the attention mask')
                raise NotImplementedError
        y = scaled_dot_product_attention(q=q, k=k, v=v, att_mask=att_mask, dropout=self.attn_drop)
        return y


class LKA(nn.Module):

    def __init__(self, dim: 'int'):
        super().__init__()
        self.conv0 = nn.Conv2d(dim, dim, 5, padding=2, groups=dim)
        self.conv_spatial = nn.Conv2d(dim, dim, 7, stride=1, padding=9, groups=dim, dilation=3)
        self.conv1 = nn.Conv2d(dim, dim, 1)

    def forward(self, x: 'torch.Tensor'):
        u = x.clone()
        attn = self.conv0(x)
        attn = self.conv_spatial(attn)
        attn = self.conv1(attn)
        return u * attn


class Visual(Attention):

    def __init__(self, dim_model: 'int', *_, **__):
        """
        Large kernel attention mechanism, as proposed in `Visual Attention Network`_, Guo et al (2022).
        The original notation is tentatively kept as is. See https://github.com/Visual-Attention-Network
        for the reference implementation

        .. Note: compared to the paper, this block contains the LKA (Large Kernel Attention)
            and the prior and posterior transformations (Conv2d and activation)

        .. _`Visual Attention Network` : https://arxiv.org/pdf/2202.09741.pdf
        """
        super().__init__()
        self.block = nn.Sequential(nn.Conv2d(dim_model, dim_model, 1), nn.GELU(), LKA(dim_model), nn.Conv2d(dim_model, dim_model, 1))
        self.requires_same_k_q_dimensions = True
        self.supports_attention_mask = False
        self.requires_skip_multi_head = True
        self.requires_squared_context = True
        self.requires_input_projection = False

    def forward(self, q: 'torch.Tensor', *_, **__):
        B, HW, C = q.shape
        H = int(math.sqrt(HW))
        assert H * H == HW
        x = q.transpose(-2, -1).reshape(B, C, H, H)
        residual = x.clone()
        x = self.block(x)
        x = x + residual
        return x.flatten(2, 3).transpose(-2, -1)


class Feedforward(nn.Module, metaclass=ABCMeta):

    @abstractmethod
    def __init__(self, dim_model: 'Optional[int]'=None, dropout: 'Optional[float]'=None, activation: 'Optional[Activation]'=None, *args, **kwargs):
        super().__init__()
        self.requires_cuda = False
        self.requires_squared_context = False

    @classmethod
    def from_config(cls: 'Type[Self]', config: 'FeedforwardConfig') ->Self:
        fields = asdict(config)
        fields = {k: v for k, v in fields.items() if v is not None}
        return cls(**fields)


class Activation(str, Enum):
    SquaredReLU = 'squared_relu'
    GeLU = 'gelu'
    LeakyReLU = 'leaky_relu'
    ReLU = 'relu'
    SmeLU = 'smelu'
    StarReLU = 'star_relu'


def build_activation(activation: 'Optional[Activation]'):
    if not activation:
        return nn.Identity()
    return {Activation.ReLU: nn.ReLU, Activation.GeLU: nn.GELU, Activation.LeakyReLU: nn.LeakyReLU, Activation.SquaredReLU: SquaredReLU, Activation.StarReLU: StarReLU, Activation.SmeLU: SmeLU}[activation]()


class Conv2DFeedforward(Feedforward):
    """
    A Convolutional feed-forward network, as proposed in VAN_ (Vision Attention Network, Guo et al.)

    .. _VAN: https://arxiv.org/pdf/2202.09741.pdf
    """

    def __init__(self, dim_model: 'int', hidden_layer_multiplier: 'int'=1, dim_model_out: 'Optional[int]'=None, activation: 'Activation'=Activation.GeLU, dropout=0.0, *args, **kwargs):
        super().__init__()
        out_features = dim_model_out or dim_model
        hidden_features = hidden_layer_multiplier * dim_model
        self.conv_mlp = nn.Sequential(nn.Conv2d(dim_model, hidden_features, 1), nn.Conv2d(hidden_features, hidden_features, 3, 1, 1, bias=True, groups=hidden_features), build_activation(activation), nn.Conv2d(hidden_features, out_features, 1), nn.Dropout(dropout))
        self.requires_squared_context = True

    def init_weights(self, **kwargs):

        def init_module(m: 'nn.Module'):
            if isinstance(m, nn.Conv2d):
                fan_out = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                fan_out //= m.groups
                m.weight.data.normal_(0, math.sqrt(2.0 / fan_out))
                if m.bias is not None:
                    m.bias.data.zero_()
        self.apply(init_module)

    def forward(self, x):
        B, L, C = x.shape
        HW = int(math.sqrt(x.shape[-2]))
        assert HW ** 2 == L, 'Conv2DFeedforward requires squared context lengths'
        x = x.reshape((B, HW, HW, C)).swapdims(1, -1)
        x = self.conv_mlp(x)
        x = x.transpose(1, -1)
        return x.flatten(1, 2)


def _fn(x: 'torch.Tensor', bias: 'Optional[torch.nn.parameter.Parameter]', residual: 'torch.Tensor', prob: 'float', layer_norm_style: 'Optional[ResidualNormStyle]', norm: 'nn.Module') ->torch.Tensor:
    a = torch.add(x, bias) if bias is not None else x
    b = torch.nn.functional.dropout(a, prob) if prob > 0.0 else a
    if layer_norm_style == ResidualNormStyle.Pre:
        c = norm(b)
        return torch.add(c, residual)
    elif layer_norm_style == ResidualNormStyle.Post:
        c = torch.add(b, residual)
        return norm(c)
    else:
        raise ValueError


class NVFusedBiasActivationDropout(torch.nn.Module):
    """
    A layer which fuses the computation of Dropout(Activation(x + Bias))
    with AOTAutograd and nvFuser
    """

    def __init__(self, p: 'float', activation: 'Optional[Activation]'=None, bias_shape: 'Optional[int]'=None) ->None:
        super().__init__()
        self.p = float(p)
        self.requires_residual = False
        self.activation = activation
        self.pytorch_activation = build_activation(self.activation)
        self.bias = nn.Parameter(torch.zeros(bias_shape)) if bias_shape is not None else None
        self._fn_train = functools.partial(_fn, activation=self.pytorch_activation, prob=self.p)
        self._fn_eval = functools.partial(_fn, activation=self.pytorch_activation, prob=0.0)
        assert self.p < 1.0, f"We don't want to drop all the values, most probably p={self.p} is not properly set"

    def init_weights(self, *args, **kwargs):
        with torch.no_grad():
            if self.bias is not None:
                self.bias.fill_(0.0)

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        fn = self._fn_train if self.training else self._fn_eval
        if not x.is_cuda:
            return fn(x, self.bias)
        aot_fn = memory_efficient_fusion(fn)
        return aot_fn(x, self.bias)


class MLP(Feedforward):

    def __init__(self, dim_model: 'int', dropout: 'float', activation: 'Activation', hidden_layer_multiplier: 'int', bias: 'bool'=True, *args, **kwargs):
        super().__init__()
        dim_mlp = hidden_layer_multiplier * dim_model
        if xformers._is_functorch_available:
            self.requires_cuda = True
            self.mlp = nn.Sequential(nn.Linear(in_features=dim_model, out_features=dim_mlp, bias=False), NVFusedBiasActivationDropout(p=dropout, bias_shape=dim_mlp if bias else None, activation=activation), nn.Linear(in_features=dim_mlp, out_features=dim_model, bias=False), NVFusedBiasActivationDropout(p=dropout, bias_shape=dim_model if bias else None, activation=None))
        else:
            self.mlp = nn.Sequential(nn.Linear(in_features=dim_model, out_features=dim_mlp, bias=bias), build_activation(activation), nn.Dropout(dropout), nn.Linear(in_features=dim_mlp, out_features=dim_model, bias=bias), nn.Dropout(dropout))

    def forward(self, inputs: 'torch.Tensor') ->torch.Tensor:
        return self.mlp(inputs)


class NVFusedBiasDropoutRes(torch.nn.Module):
    """
    A layer which fuses the computation of Dropout(x + Bias) + Residual
    with AOTAutograd and nvFuser
    """

    def __init__(self, p: 'float', bias_shape: 'Optional[int]'=None) ->None:
        super().__init__()
        self.p = float(p)
        self.requires_residual = True
        self.bias = nn.Parameter(torch.zeros(bias_shape)) if bias_shape is not None else None
        self._fn_train = functools.partial(_fn, prob=self.p)
        self._fn_eval = functools.partial(_fn, prob=0.0)
        assert self.p < 1.0, f"We don't want to drop all the values, most probably p={self.p} is not properly set"

    def init_weights(self, *args, **kwargs):
        with torch.no_grad():
            if self.bias is not None:
                self.bias.fill_(0.0)

    def forward(self, x: 'torch.Tensor', residual: 'torch.Tensor') ->torch.Tensor:
        fn = self._fn_train if self.training else self._fn_eval
        if not x.is_cuda:
            return fn(x, self.bias, residual)
        aot_fn = memory_efficient_fusion(fn)
        return aot_fn(x, self.bias, residual)


class NVFusedBiasDropoutResLayerNorm(torch.nn.Module):
    """
    A layer which fuses the computation of LayerNorm, Residual, and Dropout(x + Bias)
    operations with AOTAutograd and nvFuser based on specified layer norm style
    """

    def __init__(self, p: 'float', d_model: 'int', bias_shape: 'Optional[int]'=None, layer_norm_style: 'ResidualNormStyle'=ResidualNormStyle.Post) ->None:
        super().__init__()
        self.p = float(p)
        self.requires_residual = True
        self.layer_norm_style = layer_norm_style
        self.bias = nn.Parameter(torch.zeros(bias_shape)) if bias_shape is not None else None
        self.norm = nn.LayerNorm(d_model)
        self._fn_train = functools.partial(_fn, prob=p, layer_norm_style=self.layer_norm_style, norm=self.norm)
        self._fn_eval = functools.partial(_fn, prob=0.0, layer_norm_style=self.layer_norm_style, norm=self.norm)
        assert self.p < 1.0, f"We don't want to drop all the values, most probably p={self.p} is not properly set"

    def init_weights(self, *args, **kwargs):
        with torch.no_grad():
            if self.bias is not None:
                self.bias.fill_(0.0)

    def forward(self, x: 'torch.Tensor', residual: 'torch.Tensor') ->torch.Tensor:
        fn = self._fn_train if self.training else self._fn_eval
        if not x.is_cuda:
            return fn(x, self.bias, residual)
        aot_fn = memory_efficient_fusion(fn=fn)
        return aot_fn(x, self.bias, residual)


class PositionEmbedding(nn.Module, metaclass=ABCMeta):

    @abstractmethod
    def __init__(self, *args, **kwargs) ->None:
        super().__init__()

    @classmethod
    def from_config(cls: 'Type[Self]', config: 'PositionEmbeddingConfig') ->Self:
        fields = asdict(config)
        fields = {k: v for k, v in fields.items() if v is not None}
        return cls(**fields)


class LearnablePositionalEmbedding(PositionEmbedding):

    def __init__(self, seq_len: 'int', dim_model: 'int', add_class_token: 'bool'=False, *_, **__):
        super().__init__()
        self.pos_emb = torch.nn.Parameter(torch.randn(1, seq_len + int(add_class_token), dim_model) * 0.02)
        self.class_token = torch.nn.Parameter(torch.zeros(dim_model)) if add_class_token else None

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        if self.class_token is not None:
            clf_token = torch.ones(x.shape[0], 1, self.pos_emb.shape[-1], device=x.device) * self.class_token
            x = torch.cat([clf_token, x], dim=1)
        if x.ndim == 2:
            x = x.unsqueeze(-1)
        return x + self.pos_emb


class SinePositionalEmbedding(PositionEmbedding):

    def __init__(self, dim_model: 'int', *args, **kwargs):
        super().__init__()
        self.dim_model = dim_model

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        seq_len = x.shape[1]
        pos = torch.arange(0, seq_len, device=x.device, dtype=torch.float32).unsqueeze(1).repeat(1, self.dim_model)
        dim = torch.arange(0, self.dim_model, device=x.device, dtype=torch.float32).unsqueeze(0).repeat(seq_len, 1)
        div = torch.exp(-math.log(10000) * (2 * (dim // 2) / self.dim_model))
        pos *= div
        pos[:, 0::2] = torch.sin(pos[:, 0::2])
        pos[:, 1::2] = torch.cos(pos[:, 1::2])
        output = x.unsqueeze(-1) if x.ndim == 2 else x
        return output + pos.unsqueeze(0)


class VocabEmbedding(PositionEmbedding):

    def __init__(self, dim_model: 'int', seq_len: 'int', vocab_size: 'int', dropout: 'float'=0.0, *args, **kwargs):
        super().__init__()
        self.vocab_size = vocab_size
        self.dim_model = dim_model
        self.dropout = torch.nn.Dropout(p=dropout)
        self.position_embeddings = nn.Embedding(seq_len, self.dim_model)
        self.word_embeddings = nn.Embedding(self.vocab_size, self.dim_model)
        self.position_ids: 'Optional[torch.Tensor]' = None
        self.init_weights()

    def init_weights(self, gain: 'float'=1.0):
        torch.nn.init.normal_(self.position_embeddings.weight, std=0.02 * gain)
        torch.nn.init.normal_(self.word_embeddings.weight, std=0.02 * gain)

    def forward(self, x: 'torch.Tensor'):
        position_ids = torch.arange(x.shape[1], dtype=torch.long, device=x.device)[None, :].repeat(x.shape[0], 1)
        X_token = self.word_embeddings(x)
        X_pos = self.position_embeddings(position_ids)
        X = X_token + X_pos
        X = self.dropout(X)
        return X


class Deterministic(nn.Module):

    def __init__(self, net: 'nn.Module'):
        super().__init__()
        self.net = net
        self.cpu_state: 'torch.Tensor' = torch.get_rng_state()
        self.cuda_in_fwd: 'bool' = False
        self.gpu_devices: 'List[int]' = []
        self.gpu_states: 'List[torch.Tensor]' = []
        self.wrap_inputs = isinstance(net, RequiresWrappedInputs)

    def record_rng(self, *args):
        self.cpu_state = torch.get_rng_state()
        if torch.cuda._initialized:
            self.cuda_in_fwd = True
            self.gpu_devices, self.gpu_states = get_device_states(*args)

    def forward(self, *args, record_rng: bool=False, set_rng: bool=False, **kwargs):
        if record_rng:
            self.record_rng(*args)
        if not set_rng:
            if self.wrap_inputs:
                return self.net(inputs=args, **kwargs)
            else:
                return self.net(*args, **kwargs)
        else:
            rng_devices: 'List[int]' = []
            if self.cuda_in_fwd:
                rng_devices = self.gpu_devices
            with torch.random.fork_rng(devices=rng_devices, enabled=True):
                torch.set_rng_state(self.cpu_state)
                if self.cuda_in_fwd:
                    set_device_states(self.gpu_devices, self.gpu_states)
                if self.wrap_inputs:
                    return self.net(inputs=args, **kwargs)
                else:
                    return self.net(*args, **kwargs)


class ReversibleBlock(nn.Module):

    def __init__(self, f: 'nn.Module', g: 'nn.Module', split_dim: 'int'=-1):
        super().__init__()
        self.f = Deterministic(f)
        self.g = Deterministic(g)
        self.split_dim = split_dim

    def forward(self, x: 'torch.Tensor', f_args={}, g_args={}):
        x1, x2 = torch.chunk(x, 2, dim=-1)
        y1, y2 = None, None
        with torch.no_grad():
            y1 = x1 + self.f(x2, record_rng=self.training, **f_args)
            y2 = x2 + self.g(y1, record_rng=self.training, **g_args)
        return torch.cat([y1, y2], dim=self.split_dim)

    def backward_pass(self, y: 'torch.Tensor', dy: 'torch.Tensor', f_args={}, g_args={}):
        y1, y2 = torch.chunk(y, 2, dim=self.split_dim)
        del y
        dy1, dy2 = torch.chunk(dy, 2, dim=self.split_dim)
        del dy
        with torch.enable_grad():
            y1.requires_grad = True
            gy1 = self.g(y1, set_rng=True, **g_args)
            torch.autograd.backward(gy1, dy2)
        with torch.no_grad():
            x2 = y2 - gy1
            del y2, gy1
            dx1 = dy1 + y1.grad
            del dy1
            y1.grad = None
        with torch.enable_grad():
            x2.requires_grad = True
            fx2 = self.f(x2, set_rng=True, **f_args)
            torch.autograd.backward(fx2, dx1)
        with torch.no_grad():
            x1 = y1 - fx2
            del y1, fx2
            dx2 = dy2 + x2.grad
            del dy2
            x2.grad = None
            x = torch.cat([x1, x2.detach()], dim=self.split_dim)
            dx = torch.cat([dx1, dx2], dim=self.split_dim)
        return x, dx


class _ReversibleFunction(Function):

    @staticmethod
    def forward(ctx, x, blocks, kwargs):
        ctx.kwargs = kwargs
        for block in blocks:
            x = block(x, **kwargs)
        ctx.y = x.detach()
        ctx.blocks = blocks
        return x

    @staticmethod
    def backward(ctx, dy):
        y = ctx.y
        kwargs = ctx.kwargs
        for block in ctx.blocks[::-1]:
            y, dy = block.backward_pass(y, dy, **kwargs)
        return dy, None, None


class ReversibleSequence(nn.Module):

    def __init__(self, blocks: 'nn.ModuleList'):
        super().__init__()
        self.blocks = nn.ModuleList([ReversibleBlock(f, g) for f, g in blocks])

    def forward(self, x, arg_route=(True, False), **kwargs):
        f_args, g_args = map(lambda route: kwargs if route else {}, arg_route)
        block_kwargs = {'f_args': f_args, 'g_args': g_args}
        return _ReversibleFunction.apply(x, self.blocks, block_kwargs)


class TimmSparseAttention(torch.nn.Module):
    """
    Almost drop-in replacement for timm attention
    but using the sparsity-aware scaled_dot_product_attention from xformers
    """

    def __init__(self, dim, num_heads=8, qkv_bias=False, attn_drop=0.0, proj_drop=0.0, attn_mask=None):
        super().__init__()
        self.num_heads = num_heads
        self.qkv = torch.nn.Linear(dim, dim * 3, bias=qkv_bias)
        self.attn_drop = torch.nn.Dropout(attn_drop)
        self.proj = torch.nn.Linear(dim, dim)
        self.proj_drop = torch.nn.Dropout(proj_drop)
        self.attn_mask = attn_mask

    def forward(self, x):
        B, N, C = x.shape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        qkv = qkv.flatten(1, 2)
        q, k, v = qkv.unbind()
        x = scaled_dot_product_attention(q, k, v, self.attn_mask, dropout=self.attn_drop)
        x = x.reshape(B, self.num_heads, N, C // self.num_heads)
        x = x.transpose(1, 2).reshape(B, N, C)
        x = self.proj(x)
        x = self.proj_drop(x)
        return x


class SwiGLUOp:
    """Base class for any swiglu operator in :attr:`xformers.ops.swiglu`"""

    def __init__(self, op, packed_weights: 'bool', name: 'str', constraints):
        self.NAME = name
        self.PACKED_WEIGHTS = packed_weights
        self.op = op
        self.constraints = constraints

    def supports(self, op: "'SwiGLUOpDispatch'") ->bool:
        if self.PACKED_WEIGHTS and not op.packed_weights:
            return False
        return all(c(op) for c in self.constraints)

    def __call__(self, *args: Optional[torch.Tensor]) ->torch.Tensor:
        pass

    def __str__(self) ->str:
        return f'SwiGLUOp:{self.NAME}'


class _ForwardToFunc(SwiGLUOp):

    def __call__(self, *args, **kwargs):
        return self.op(*args, **kwargs)

    def info(self):
        if self.op.__name__ == 'no_such_operator':
            return 'not built'
        return 'available'


def _eager_functional_swiglu(x: 'torch.Tensor', w1: 'torch.Tensor', b1: 'torch.Tensor', w2: 'torch.Tensor', b2: 'torch.Tensor', w3: 'torch.Tensor', b3: 'torch.Tensor') ->torch.Tensor:
    x1 = F.linear(x, w1, b1)
    x2 = F.linear(x, w2, b2)
    hidden = F.silu(x1) * x2
    return F.linear(hidden, w3, b3)


SwiGLUEagerOp = _ForwardToFunc(_eager_functional_swiglu, False, 'eager', constraints=[])


class _ForwardToPythonAutogradFunc(SwiGLUOp):

    def supports(self, op: "'SwiGLUOpDispatch'") ->bool:
        if op.dtype_autocast_gpu == torch.bfloat16:
            return False
        return super().supports(op)

    def __call__(self, *args, **kwargs):
        return self.op.apply(*args, **kwargs)


class BaseOperator:
    OPERATOR: 'Any'
    NAME: 'str'
    OPERATOR_CATEGORY: 'str'

    @classmethod
    def is_available(cls) ->bool:
        if cls.OPERATOR is None or cls.OPERATOR.__name__ == 'no_such_operator':
            return False
        return True

    @classmethod
    def operator_flop(cls, *inputs) ->int:
        """Calculate number of FLOP given inputs to `OPERATOR`"""
        return -1


def get_operator(library: 'str', name: 'str'):

    def no_such_operator(*args, **kwargs):
        raise RuntimeError(f'No such operator {library}::{name} - did you forget to build xformers with `python setup.py develop`?')
    try:
        return getattr(getattr(torch.ops, library), name)
    except (RuntimeError, AttributeError):
        return no_such_operator


def get_xformers_operator(name: 'str'):
    return get_operator('xformers', name)


class DualGemmSiluOp(BaseOperator):
    OPERATOR = get_xformers_operator('dual_gemm_silu_identity_mul')
    OPERATOR_CATEGORY = 'swiglu'
    NAME = 'dual_gemm_silu'

    @classmethod
    def operator_flop(cls, x: 'torch.Tensor', w1: 'torch.Tensor', b1, w2: 'torch.Tensor', b2) ->int:
        """NOTE: we neglect the impact of biases / pointwises"""
        M, N, K = x.shape[0], w1.shape[0], w1.shape[1]
        return M * N * K * 2 * 2


class GemmFusedSumOp(BaseOperator):
    OPERATOR = get_xformers_operator('gemm_fused_operand_sum')
    OPERATOR_CATEGORY = 'swiglu'
    NAME = 'gemm_fused_operand_sum'

    @classmethod
    def operator_flop(cls, a: 'torch.Tensor', b: 'torch.Tensor', out1, out2) ->int:
        M, N, K = a.shape[0], b.shape[1], a.shape[1]
        return M * N * K * 2


def get_stack_strides(tensors: 'Sequence[torch.Tensor]', dim: 'int') ->Optional[Tuple[int, ...]]:
    """
    If the tensors are already stacked on dimension :code:`dim`,         returns the strides of the stacked tensors.         Otherwise returns :code:`None`.
    """
    if len(tensors) <= 1 or dim > tensors[0].ndim:
        return None
    final_stride = []
    for i in range(tensors[0].ndim + 1):
        if i == dim:
            final_stride.append(tensors[1].storage_offset() - tensors[0].storage_offset())
            continue
        if i > dim:
            i -= 1
        final_stride.append(tensors[0].stride(i))
    storage_data_ptr: 'Optional[int]' = None
    for i, x in enumerate(tensors[1:]):
        if x.shape != tensors[0].shape:
            return None
        if x.stride() != tensors[0].stride():
            return None
        if x.storage_offset() != tensors[0].storage_offset() + (i + 1) * final_stride[dim]:
            return None
        if storage_data_ptr is None:
            storage_data_ptr = tensors[0].storage().data_ptr()
        if x.storage().data_ptr() != storage_data_ptr:
            return None
    return tuple(final_stride)


def _stack_or_none_fw(tensors: 'Union[Tuple[torch.Tensor, ...], List[torch.Tensor]]', dim: 'int') ->Optional[torch.Tensor]:
    strides = get_stack_strides(tensors, dim)
    if strides is not None:
        input_shape = list(tensors[0].shape)
        input_shape.insert(dim, len(tensors))
        return tensors[0].as_strided(input_shape, strides)
    return None


class _StackOrNone(torch.autograd.Function):
    """
    See function `stack_or_none`
    """

    @staticmethod
    def forward(ctx, dim: 'int', *tensors: torch.Tensor):
        ctx.dim = dim
        return _stack_or_none_fw(tensors, dim=dim)

    @classmethod
    def backward(cls, ctx, grad: 'torch.Tensor'):
        return None, *grad.unbind(dim=ctx.dim)


def stack_or_none(tensors: 'Sequence[torch.Tensor]', dim: 'int') ->torch.Tensor:
    """
    Does exactly the same as :attr:`torch.stack` if the tensors can be concatenated
    without any memory operation. Otherwise returns None.
    """
    return _StackOrNone.apply(dim, *tensors)


def _only_half_or_autocast(op: 'SwiGLUOpDispatch') ->bool:
    HALF_DTYPES = [torch.half, torch.bfloat16]
    return op.dtype in HALF_DTYPES or op.dtype_autocast_gpu is not None and op.dtype_autocast_gpu in HALF_DTYPES


def _only_sm80(op: 'SwiGLUOpDispatch') ->bool:
    device_type = op.device if isinstance(op.device, str) else op.device.type
    return device_type == 'cuda' and torch.cuda.get_device_capability(op.device)[0] >= 8


SwiGLUPackedFusedOp = _ForwardToFunc(get_xformers_operator('swiglu_packedw'), True, 'fused.p.cpp', constraints=[_only_sm80, _only_half_or_autocast])


def swiglu(x: 'torch.Tensor', w1: 'torch.Tensor', b1: 'Optional[torch.Tensor]', w2: 'torch.Tensor', b2: 'Optional[torch.Tensor]', w3: 'torch.Tensor', b3: 'Optional[torch.Tensor]', *, op: SwiGLUOp=None) ->torch.Tensor:
    """
    Computes a SwiGLU block given the weights/bias of the 3
    linear layers.

    - It is recommended to keep ``op=None`` so the best implementation     available for the inputs will be used.


    :Equivalent pytorch code:

    .. code-block:: python

        x1 = F.linear(x, w1, b1)
        x2 = F.linear(x, w2, b2)
        hidden = F.silu(x1) * x2
        return F.linear(hidden, w3, b3)

    :Packing weights:

    To allow faster implementations, it's recommended to have w1/w2 come from the same storage, as in:
        .. code-block:: python

            w1, w2 = xformers.ops.unbind(w12, 0)

    :Supported hardware:

    This operator is only optimized on A100+ on ``torch.half`` or ``torch.bfloat16``         (autocast is supported), and will fallback to a functional pytorch         implementation otherwise.
    """
    batch_shape = x.shape[:-1]
    x = x.reshape([-1, x.shape[-1]])
    if w1.ndim != 2 or w1.shape != w2.shape:
        raise ValueError(f'Invalid shapes for w1: {w1.shape} / w2: {w2.shape}')
    if b1 is not None:
        if b1.ndim != 1 or b1.shape[0] != w1.shape[0]:
            raise ValueError(f'Invalid shapes for b1: {b1.shape}')
    if b2 is not None:
        if b2.ndim != 1 or b2.shape[0] != w2.shape[0]:
            raise ValueError(f'Invalid shapes for b2: {b2.shape}')
    if w3.ndim != 2 or w3.shape[1] != w2.shape[0]:
        raise ValueError(f'Invalid shape for w3: {w3.shape}')
    if b3 is not None:
        if b3.ndim != 1 or b3.shape[0] != w3.shape[0]:
            raise ValueError(f'Invalid shapes for w3: {w3.shape} / b3: {b3.shape}')
    if op is None:
        op = SwiGLUOpDispatch.from_arguments(x, w1, b1, w2, b2, w3, b3).op
    if not op.PACKED_WEIGHTS:
        return op(x, w1, b1, w2, b2, w3, b3).reshape([*batch_shape, -1])
    w1w2 = stack_or_none((w1, w2), dim=0)
    if b1 is not None and b2 is not None:
        b1b2: 'Optional[torch.Tensor]' = stack_or_none((b1, b2), dim=0)
        if b1b2 is None:
            raise NotImplementedError('b1/b2 needs to be properly packed')
    else:
        b1b2 = None
        assert b1 is None and b2 is None
    if w1w2 is None:
        raise NotImplementedError('w1/w2 needs to be properly packed')
    return op(x, w1w2, b1b2, w3, b3).reshape([*batch_shape, -1])


def _stack_fw(tensors: 'Union[Tuple[torch.Tensor, ...], List[torch.Tensor]]', dim: 'int') ->torch.Tensor:
    out = _stack_or_none_fw(tensors, dim)
    if out is None:
        out = torch.stack(tensors, dim=dim)
    return out


class _Unbind(torch.autograd.Function):
    """
    See function `unbind`
    """

    @staticmethod
    def forward(ctx, x: 'torch.Tensor', dim: 'int'):
        ctx.dim = dim
        return x.unbind(dim)

    @classmethod
    def backward(cls, ctx, *tensors: torch.Tensor):
        return _stack_fw(tensors, ctx.dim), None


def unbind(x: 'torch.Tensor', dim: 'int') ->Tuple[torch.Tensor, ...]:
    """
    Does exactly the same as :attr:`torch.unbind` for the forward.
    In backward, avoids a :attr:`torch.cat` if the gradients
    are already multiple views of the same storage
    """
    return _Unbind.apply(x, dim)


class SwiGLU(nn.Module):
    """
    A Module that encapsulates the call to :attr:`xformers.ops.swiglu`,
    and holds the weights for the 3 linear layers
    """

    def __init__(self, in_features: 'int', hidden_features: 'int', out_features: 'Optional[int]'=None, bias: 'bool'=True, *, _pack_weights: bool=True) ->None:
        """Create a SwiGLU module

        Args:
            in_features (int): Number of features of the input
            hidden_features (int): Number of hidden features
            out_features (Optional[int], optional): Number of features of the input. Defaults to None.
            bias (bool, optional): Whether linear layers also include a bias. Defaults to True.
        """
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.w12: 'Optional[nn.Linear]'
        if _pack_weights:
            self.w12 = nn.Linear(in_features, 2 * hidden_features, bias=bias)
        else:
            self.w12 = None
            self.w1 = nn.Linear(in_features, hidden_features, bias=bias)
            self.w2 = nn.Linear(in_features, hidden_features, bias=bias)
        self.w3 = nn.Linear(hidden_features, out_features, bias=bias)
        self.hidden_features = hidden_features
        self.out_features = out_features
        self.in_features = in_features
        self.op = None

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        """Computes :attr:`swiglu` with the module's weights

        Args:
            x (torch.Tensor): A Tensor of shape ``[..., in_features]``

        Returns:
            torch.Tensor: A Tensor of shape ``[..., out_features]``
        """
        return swiglu(x, *self._ordered_params(), op=self.op)

    def _ordered_params(self):
        """Used for testing - returns ordered arguments for operators"""
        b1: 'Optional[torch.Tensor]'
        b2: 'Optional[torch.Tensor]'
        if self.w12 is not None:
            w1w2 = self.w12.weight
            b1b2 = self.w12.bias
            w1, w2 = unbind(w1w2.view([2, w1w2.shape[0] // 2, w1w2.shape[1]]), dim=0)
            if b1b2 is not None:
                b1, b2 = unbind(b1b2.view([2, b1b2.shape[0] // 2]), dim=0)
            else:
                b1, b2 = None, None
        else:
            w1, w2 = self.w1.weight, self.w2.weight
            b1, b2 = self.w1.bias, self.w2.bias
        return [w1, b1, w2, b2, self.w3.weight, self.w3.bias]


BLOCK_M = 32


BLOCK_N = 64


_kAlpha = math.sqrt(2.0 / math.pi)


class _dropout(torch.autograd.Function):

    @staticmethod
    @custom_fwd(cast_inputs=torch.float16)
    def forward(ctx, x, p, bias, activation, trainable_bias):
        x_ = x.reshape(-1, x.shape[-1]).contiguous()
        y = torch.empty_like(x_)
        M, N = x_.shape
        assert bias is None or bias.dtype == x.dtype and bias.shape[0] == N
        assert p > 0.0

        def grid(meta):
            return triton.cdiv(M, meta['BLOCK_M']), triton.cdiv(N, meta['BLOCK_N'])
        N_BLOCK_N = triton.cdiv(N, BLOCK_N)
        seeds = torch.randint(65536, (N_BLOCK_N,), device=x.device, dtype=torch.int32)
        bias_ptr = bias if bias is not None else x_
        k_dropout_fw[grid](y, x_, bias_ptr, seeds, y.stride(0), M, N, p, x.dtype == torch.float16, USE_BIAS=bias is not None, ACTIVATION=activation, BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)
        if activation is not None:
            ctx.save_for_backward(seeds, bias, x)
        else:
            ctx.save_for_backward(seeds, bias, None)
        ctx.trainable_bias = bias is not None and trainable_bias
        ctx.activation = activation
        ctx.p = p
        return y.reshape_as(x)

    @staticmethod
    @custom_bwd
    def backward(ctx, grad_out):
        seeds, bias, inputs = ctx.saved_tensors
        grad_out_ = grad_out.reshape(-1, grad_out.shape[-1]).contiguous()
        grad_in = torch.empty_like(grad_out_)
        M, N = grad_out_.shape
        assert inputs is not None or ctx.activation is None
        if inputs is None:
            inputs = grad_out_
        elif inputs.ndim > 2:
            inputs = inputs.reshape(-1, N)
        N_BLOCKS_M = triton.cdiv(M, BLOCK_M)
        if ctx.trainable_bias:
            grad_bias = torch.empty((N_BLOCKS_M, N), device=grad_in.device, dtype=grad_in.dtype)
        else:
            grad_bias = grad_in

        def grid(meta):
            return N_BLOCKS_M, triton.cdiv(N, meta['BLOCK_N'])
        k_dropout_bw[grid](grad_in, grad_bias, grad_out_, inputs, bias if bias is not None else inputs, seeds, grad_out_.stride(0), inputs.stride(0), M, N, ctx.p, grad_in.dtype == torch.float16, USE_BIAS=bias is not None, ACTIVATION=ctx.activation, TRAINABLE_BIAS=ctx.trainable_bias, BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N)
        return grad_in.reshape_as(grad_out), None, torch.sum(grad_bias, dim=0) if ctx.trainable_bias else None, None, None, None


def get_triton_activation_index(activation: 'Optional[Activation]') ->int:
    return {Activation.ReLU: 1, Activation.LeakyReLU: 2, Activation.GeLU: 3, Activation.SquaredReLU: 4, Activation.SmeLU: 5, Activation.StarReLU: 6}[activation] if activation is not None else 0


class FusedDropoutBias(torch.nn.Module):
    """
    A layer which fuses the computation of Dropout(Activation(x))
    in a single GPU kernel
    """

    def __init__(self, p: 'float', bias_shape: 'Optional[int]', activation: 'Optional[Activation]'=None) ->None:
        super().__init__()
        self.p = float(p)
        assert self.p < 1.0, f"We don't want to drop all the values, most probably p={p} is not properly set"
        self.activation_type = activation
        self.bias = torch.zeros(bias_shape, requires_grad=True) if bias_shape is not None else None
        self.activation = get_triton_activation_index(self.activation_type)
        self.activation_pytorch = build_activation(self.activation_type)

    def init_weights(self, *args, **kwargs):
        with torch.no_grad():
            if self.bias is not None:
                self.bias.fill_(0.0)

    def forward(self, x: 'torch.Tensor') ->torch.Tensor:
        if self.bias is not None:
            self.bias = self.bias
        p = self.p if self.training else 0.0
        perf_check = x.shape[-1] > 512
        if not x.is_cuda or not perf_check or p == 0.0:
            x = x + self.bias if self.bias is not None else x
            x = self.activation_pytorch(x)
            return torch.nn.functional.dropout(x, p) if p > 0.0 else x
        return _dropout.apply(x, p, self.bias, self.activation, True)


def get_configs(block_k):
    return [triton.Config({'BLOCK_M': 64, 'BLOCK_N': 32, 'BLOCK_K': block_k}, num_stages=4, num_warps=2), triton.Config({'BLOCK_M': 32, 'BLOCK_N': 64, 'BLOCK_K': block_k}, num_stages=4, num_warps=2), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 64, 'BLOCK_K': block_k}, num_stages=3, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 128, 'BLOCK_K': block_k}, num_stages=3, num_warps=4), triton.Config({'BLOCK_M': 128, 'BLOCK_N': 128, 'BLOCK_K': block_k}, num_stages=3, num_warps=4), triton.Config({'BLOCK_M': 64, 'BLOCK_N': 256, 'BLOCK_K': block_k}, num_stages=3, num_warps=4), triton.Config({'BLOCK_M': 256, 'BLOCK_N': 64, 'BLOCK_K': block_k}, num_stages=3, num_warps=4)]


def fused_matmul(x: 'torch.Tensor', weight: 'torch.Tensor', bias: 'Optional[torch.Tensor]', activation=0, save_act_inputs: 'bool'=False):
    """
    Compute e = activation(x @ weight + bias).
    This wrapper kicks the `kernel_fma` Triton kernel
    """
    if not x.is_contiguous():
        x = x.contiguous()
    x_ = x if x.ndim == 2 else x.flatten(0, 1)
    assert x_.shape[1] == weight.shape[1], f'Incompatible dimensions in between inputs and weight, {x_.shape} - {weight.shape}'
    assert bias is None or bias.is_contiguous()
    assert bias is None or bias.shape[0] == weight.shape[0], 'Incompatible dimensions in between weight and bias'
    assert weight.is_contiguous()
    M, K = x_.shape
    N, K = weight.shape
    outputs = torch.empty((M, N), device=x.device, dtype=x.dtype)
    act_inputs = torch.empty_like(outputs) if save_act_inputs else x
    grid = lambda META: (triton.cdiv(M, META['BLOCK_M']) * triton.cdiv(N, META['BLOCK_N']),)
    kernel_fma[grid](outputs, act_inputs, x_, weight, bias if bias is not None else x, M, N, K, outputs.stride(0), x_.stride(0), weight.stride(0), ACTIVATION=activation, BIAS=bias is not None, GROUP_M=8, SAVE_ACT_INPUTS=save_act_inputs, is_fp16=x_.dtype == torch.float16)
    outputs = outputs if x.ndim == 2 else outputs.reshape(x.shape[0], -1, N)
    return outputs, act_inputs if save_act_inputs else None


def fused_matmul_backward(grad_out: 'torch.Tensor', inputs: 'torch.Tensor', act_in: 'Optional[torch.Tensor]', weight: 'torch.Tensor', trainable_weight: 'bool', trainable_bias: 'bool', activation_grad: 'int'=0):
    """
    Compute grad_in = activation^-1(grad_out) @ weight.transpose()

    .. note: The weight buffer is transposed on the fly
    .. note: Activation gradient needs to be a Triton kernel
    """
    if not grad_out.is_contiguous():
        grad_out = grad_out.contiguous()
    grad_out_ = grad_out if grad_out.ndim == 2 else grad_out.flatten(0, 1)
    inputs_ = inputs if inputs.ndim == 2 else inputs.flatten(0, 1)
    assert grad_out_.shape[1] == weight.shape[0], 'Incompatible dimensions in between grad_out and weight'
    M, N = grad_out_.shape
    N, _ = weight.shape
    if activation_grad > 0:
        grad_act = torch.empty_like(grad_out_)
        if act_in is None:
            act_in = grad_out_
        grid = lambda META: (M, triton.cdiv(N, META['BLOCK_N']))
        kernel_bw[grid](grad_act, grad_out_, act_in, N, grad_act.stride(0), act_in.stride(0), ACTIVATION_GRAD=activation_grad)
        grad_out_ = grad_act
    grad_in = triton.ops.matmul(grad_out_, weight)
    grad_weight = grad_out_.transpose(1, 0) @ inputs_ if trainable_weight else None
    grad_bias = torch.sum(grad_out_, dim=0) if trainable_bias else None
    return grad_in.reshape_as(inputs), grad_weight, grad_bias


class _fused_linear_triton(torch.autograd.Function):

    @staticmethod
    @custom_fwd(cast_inputs=torch.float16)
    def forward(ctx, x, weight, bias, activation, trainable_weight, trainable_bias, save_activation_inputs):
        y, activation_inputs = fused_matmul(x, weight, bias, activation, save_activation_inputs)
        ctx.activation = activation
        ctx.trainable_weight = trainable_weight
        ctx.trainable_bias = trainable_bias
        if x.requires_grad or ctx.trainable_weight or ctx.trainable_bias:
            ctx.save_for_backward(weight, activation_inputs, x)
        return y

    @staticmethod
    @custom_bwd
    def backward(ctx: 'Any', grad_out: 'torch.Tensor') ->Any:
        """
        Compute the derivative with respect to x, other tensors were not trainable inputs.
        """
        weight, activation_inputs, x = ctx.saved_tensors
        grad_input, grad_weight, grad_bias = fused_matmul_backward(grad_out=grad_out, inputs=x, act_in=activation_inputs, weight=weight, trainable_weight=ctx.trainable_weight, trainable_bias=ctx.trainable_bias, activation_grad=ctx.activation)
        return grad_input, grad_weight, grad_bias, None, None, None, None, None, None


class FusedLinear(nn.Module):
    """
    Handle a linear transform, like torch.nn.Linear_, and a given activation, in a single kernel.
    The whole transform: is :math:`y = activation(xA^T + b)`.

    This is typically significantly faster than PyTorch while using fp16 and non-sigmoid activations,
    as of September 2021.

    .. _torch.nn.Linear: https://pytorch.org/docs/stable/generated/torch.nn.Linear.html
    """

    def __init__(self, in_features: 'int', out_features: 'int', bias: 'bool'=False, activation: 'Optional[Activation]'=None, **_):
        super().__init__()
        self.weight = nn.Parameter(torch.empty(out_features, in_features), requires_grad=True)
        self.bias = nn.Parameter(torch.empty(out_features), requires_grad=True) if bias else None
        self._activation_index = get_triton_activation_index(activation)
        self.reset_parameters()

    def reset_parameters(self) ->None:
        torch.nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = torch.nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in) if fan_in > 0 else 0
            torch.nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, x):
        return _fused_linear_triton.apply(x, self.weight, self.bias, self._activation_index, self.weight.requires_grad, self.bias.requires_grad if self.bias is not None else False, self.training and x.requires_grad and self._activation_index > 0)


import torch
from torch.nn import MSELoss, ReLU
from _paritybench_helpers import _mock_config, _mock_layer, _paritybench_base, _fails_compile


TESTCASES = [
    # (nn.Module, init_args, forward_args, jit_compiles)
    (AdaGroupNorm,
     lambda: ([], {'embedding_dim': 4, 'out_dim': 4, 'num_groups': 1}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     False),
    (ApproximateGELU,
     lambda: ([], {'dim_in': 4, 'dim_out': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (AttentionPooling,
     lambda: ([], {'num_heads': 4, 'embed_dim': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     False),
    (AvgPool,
     lambda: ([], {'n': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (BEVControlNetConditioningEmbedding,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 25, 64, 64])], {}),
     True),
    (BEVControlNetConditioningEmbeddingPlus,
     lambda: ([], {'conditioning_embedding_size': 4}),
     lambda: ([torch.rand([4, 25, 64, 64])], {}),
     True),
    (ConditionalReshape,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (ControlNetConditioningEmbedding,
     lambda: ([], {'conditioning_embedding_channels': 4}),
     lambda: ([torch.rand([4, 3, 64, 64])], {}),
     True),
    (Conv2DFeedforward,
     lambda: ([], {'dim_model': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     True),
    (Deterministic,
     lambda: ([], {'net': _mock_layer()}),
     lambda: ([], {'input': torch.rand([4, 4])}),
     False),
    (DownBlock1D,
     lambda: ([], {'out_channels': 4, 'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     False),
    (Downsample1D,
     lambda: ([], {'channels': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     True),
    (Downsample1d,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     True),
    (Downsample2D,
     lambda: ([], {'channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (FIDInceptionA,
     lambda: ([], {'in_channels': 4, 'pool_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FIDInceptionC,
     lambda: ([], {'in_channels': 4, 'channels_7x7': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FIDInceptionE_1,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FIDInceptionE_2,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FeedForward,
     lambda: ([], {'dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (FirDownsample2D,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (FirUpsample2D,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (FourierMix,
     lambda: ([], {'dropout': 0.5}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (GEGLU,
     lambda: ([], {'dim_in': 4, 'dim_out': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (GELU,
     lambda: ([], {'dim_in': 4, 'dim_out': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (GatedConnector,
     lambda: ([], {'dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (GaussianSmoothing,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 1, 64, 64])], {}),
     True),
    (KDownsample2D,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (KUpsample2D,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (LDMBertAttention,
     lambda: ([], {'embed_dim': 4, 'num_heads': 4, 'head_dim': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     False),
    (LKA,
     lambda: ([], {'dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (LambdaLayer,
     lambda: ([], {'dropout': 0.5, 'seq_len': 4, 'dim_head': 4}),
     lambda: ([torch.rand([4, 4, 4]), torch.rand([4, 4, 4]), torch.rand([4, 4, 4])], {}),
     False),
    (LearnablePositionalEmbedding,
     lambda: ([], {'seq_len': 4, 'dim_model': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (LinearMultiDim,
     lambda: ([], {'in_features': 4}),
     lambda: ([torch.rand([16])], {}),
     False),
    (LoRALinearLayer,
     lambda: ([], {'in_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (NewGELUActivation,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (OutValueFunctionBlock,
     lambda: ([], {'fc_dim': 4, 'embed_dim': 4}),
     lambda: ([torch.rand([4, 4]), torch.rand([4, 4])], {}),
     True),
    (PaintByExampleMapper,
     lambda: ([], {'config': _mock_config(num_hidden_layers=1, hidden_size=4)}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Passthrough,
     lambda: ([], {}),
     lambda: ([], {}),
     False),
    (PatchToSequence,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Pooling,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     False),
    (ResConvBlock,
     lambda: ([], {'in_channels': 4, 'mid_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4])], {}),
     False),
    (RotaryEmbedding,
     lambda: ([], {'dim_model': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (SMHyperbolic,
     lambda: ([], {'dim_features': 4, 'iter_before_redraw': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (SMOrf,
     lambda: ([], {'dim_features': 4, 'iter_before_redraw': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (SMReg,
     lambda: ([], {'dim_features': 4, 'iter_before_redraw': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (SelfAttention1d,
     lambda: ([], {'in_channels': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     True),
    (SinePositionalEmbedding,
     lambda: ([], {'dim_model': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SkipBlock,
     lambda: ([], {'dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (SmeLU,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (SquaredReLU,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (StarReLU,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (T5DenseGatedActDense,
     lambda: ([], {'d_model': 4, 'd_ff': 4, 'dropout_rate': 0.5}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (T5FiLMLayer,
     lambda: ([], {'in_features': 4, 'out_features': 4}),
     lambda: ([torch.rand([4, 4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     True),
    (T5LayerFFCond,
     lambda: ([], {'d_model': 4, 'd_ff': 4, 'dropout_rate': 0.5, 'layer_norm_epsilon': 1}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (T5LayerNorm,
     lambda: ([], {'hidden_size': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (TimestepEmbedding,
     lambda: ([], {'in_channels': 4, 'time_embed_dim': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (UpBlock1D,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (UpBlock1DNoSkip,
     lambda: ([], {'in_channels': 4, 'out_channels': 4}),
     lambda: ([torch.rand([4, 4, 4]), torch.rand([4, 4, 4, 4])], {}),
     False),
    (Upsample1D,
     lambda: ([], {'channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (Upsample1d,
     lambda: ([], {}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     True),
    (Upsample2D,
     lambda: ([], {'channels': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     False),
    (VectorQuantizer,
     lambda: ([], {'n_e': 4, 'vq_embed_dim': 4, 'beta': 4}),
     lambda: ([torch.rand([4, 4, 4, 4])], {}),
     True),
    (Visual,
     lambda: ([], {'dim_model': 4}),
     lambda: ([torch.rand([4, 4, 4])], {}),
     False),
]

class Test_cure_lab_MagicDrive(_paritybench_base):
    def test_000(self):
        self._check(*TESTCASES[0])

    def test_001(self):
        self._check(*TESTCASES[1])

    def test_002(self):
        self._check(*TESTCASES[2])

    def test_003(self):
        self._check(*TESTCASES[3])

    def test_004(self):
        self._check(*TESTCASES[4])

    def test_005(self):
        self._check(*TESTCASES[5])

    def test_006(self):
        self._check(*TESTCASES[6])

    def test_007(self):
        self._check(*TESTCASES[7])

    def test_008(self):
        self._check(*TESTCASES[8])

    def test_009(self):
        self._check(*TESTCASES[9])

    def test_010(self):
        self._check(*TESTCASES[10])

    def test_011(self):
        self._check(*TESTCASES[11])

    def test_012(self):
        self._check(*TESTCASES[12])

    def test_013(self):
        self._check(*TESTCASES[13])

    def test_014(self):
        self._check(*TESTCASES[14])

    def test_015(self):
        self._check(*TESTCASES[15])

    def test_016(self):
        self._check(*TESTCASES[16])

    def test_017(self):
        self._check(*TESTCASES[17])

    def test_018(self):
        self._check(*TESTCASES[18])

    def test_019(self):
        self._check(*TESTCASES[19])

    def test_020(self):
        self._check(*TESTCASES[20])

    def test_021(self):
        self._check(*TESTCASES[21])

    def test_022(self):
        self._check(*TESTCASES[22])

    def test_023(self):
        self._check(*TESTCASES[23])

    def test_024(self):
        self._check(*TESTCASES[24])

    def test_025(self):
        self._check(*TESTCASES[25])

    def test_026(self):
        self._check(*TESTCASES[26])

    def test_027(self):
        self._check(*TESTCASES[27])

    def test_028(self):
        self._check(*TESTCASES[28])

    def test_029(self):
        self._check(*TESTCASES[29])

    def test_030(self):
        self._check(*TESTCASES[30])

    def test_031(self):
        self._check(*TESTCASES[31])

    def test_032(self):
        self._check(*TESTCASES[32])

    def test_033(self):
        self._check(*TESTCASES[33])

    def test_034(self):
        self._check(*TESTCASES[34])

    def test_035(self):
        self._check(*TESTCASES[35])

    def test_036(self):
        self._check(*TESTCASES[36])

    def test_037(self):
        self._check(*TESTCASES[37])

    def test_038(self):
        self._check(*TESTCASES[38])

    def test_039(self):
        self._check(*TESTCASES[39])

    def test_040(self):
        self._check(*TESTCASES[40])

    def test_041(self):
        self._check(*TESTCASES[41])

    def test_042(self):
        self._check(*TESTCASES[42])

    def test_043(self):
        self._check(*TESTCASES[43])

    def test_044(self):
        self._check(*TESTCASES[44])

    def test_045(self):
        self._check(*TESTCASES[45])

    def test_046(self):
        self._check(*TESTCASES[46])

    def test_047(self):
        self._check(*TESTCASES[47])

    def test_048(self):
        self._check(*TESTCASES[48])

    def test_049(self):
        self._check(*TESTCASES[49])

    def test_050(self):
        self._check(*TESTCASES[50])

    def test_051(self):
        self._check(*TESTCASES[51])

    def test_052(self):
        self._check(*TESTCASES[52])

    def test_053(self):
        self._check(*TESTCASES[53])

    def test_054(self):
        self._check(*TESTCASES[54])

    def test_055(self):
        self._check(*TESTCASES[55])

    def test_056(self):
        self._check(*TESTCASES[56])

    def test_057(self):
        self._check(*TESTCASES[57])

    def test_058(self):
        self._check(*TESTCASES[58])

    def test_059(self):
        self._check(*TESTCASES[59])

    def test_060(self):
        self._check(*TESTCASES[60])

    def test_061(self):
        self._check(*TESTCASES[61])

    def test_062(self):
        self._check(*TESTCASES[62])

