import sys
_module = sys.modules[__name__]
del sys
cogdl = _module
configs = _module
data = _module
batch = _module
data = _module
dataloader = _module
dataset = _module
sampler = _module
datasets = _module
customized_data = _module
gatne = _module
gcc_data = _module
geom_data = _module
grb_data = _module
gtn_data = _module
han_data = _module
kg_data = _module
matlab_matrix = _module
oagbert_data = _module
ogb = _module
planetoid_data = _module
rd2cd_data = _module
rec_data = _module
saint_data = _module
stgat_data = _module
stgcn_data = _module
tu_data = _module
experiments = _module
layers = _module
actgcn_layer = _module
actgcnii_layer = _module
actlinear_layer = _module
actmlp_layer = _module
actsage_layer = _module
base_layer = _module
deepergcn_layer = _module
disengcn_layer = _module
gat_layer = _module
gat_layerii = _module
gcn_layer = _module
gcn_layerii = _module
gcnii_layer = _module
gin_layer = _module
gine_layer = _module
han_layer = _module
jittor = _module
mixhop_layer = _module
mlp_layer = _module
pprgo_layer = _module
reversible_layer = _module
rgcn_layer = _module
sage_layer = _module
saint_layer = _module
se_layer = _module
set2set = _module
sgc_layer = _module
stgat_layer = _module
stgcn_layer = _module
loggers = _module
base_logger = _module
tensorboard_logger = _module
wandb_logger = _module
models = _module
base_model = _module
emb = _module
complex = _module
deepwalk = _module
dgk = _module
distmult = _module
dngr = _module
gatne = _module
graph2vec = _module
grarep = _module
hin2vec = _module
hope = _module
knowledge_base = _module
line = _module
metapath2vec = _module
netmf = _module
netsmf = _module
node2vec = _module
prone = _module
pronepp = _module
pte = _module
rotate = _module
sdne = _module
spectral = _module
transe = _module
nn = _module
actgcn = _module
agc = _module
autognn = _module
compgcn = _module
correct_smooth = _module
daegc = _module
deepergcn = _module
dgi = _module
diffpool = _module
disengcn = _module
drgat = _module
drgcn = _module
dropedge_gcn = _module
gae = _module
gat = _module
gcc_model = _module
gcn = _module
gcnii = _module
gcnmix = _module
gdc_gcn = _module
gin = _module
grace = _module
grand = _module
graph_unet = _module
graphsage = _module
graphsaint = _module
gtn = _module
han = _module
infograph = _module
m3s = _module
mixhop = _module
mlp = _module
moe_gcn = _module
mvgrl = _module
patchy_san = _module
ppnp = _module
pprgo = _module
revgcn = _module
rgcn = _module
sagn = _module
sgc = _module
sign = _module
sortpool = _module
srgcn = _module
stgat = _module
stgcn = _module
oag = _module
bert_model = _module
dual_position_bert_model = _module
oagbert = _module
oagbert_metainfo = _module
utils = _module
operators = _module
edge_softmax = _module
fused_gat = _module
jt_spmm = _module
linear = _module
mhspmm = _module
ops = _module
sample = _module
scatter_max = _module
spmm = _module
options = _module
pipelines = _module
trainer = _module
controller = _module
data_controller = _module
training_controller = _module
embed_trainer = _module
trainer = _module
trainer_utils = _module
evaluator = _module
graph_utils = _module
grb_utils = _module
index = _module
link_prediction_utils = _module
optimizer = _module
ppr_utils = _module
prone_utils = _module
rwalk = _module
sampling = _module
spmm_utils = _module
srgcn_utils = _module
transform = _module
utils = _module
wrappers = _module
data_wrapper = _module
base_data_wrapper = _module
graph_classification = _module
graph_classification_dw = _module
graph_embedding_dw = _module
infograph_dw = _module
patchy_san_dw = _module
heterogeneous = _module
heterogeneous_embedding_dw = _module
heterogeneous_gnn_dw = _module
multiplex_embedding_dw = _module
link_prediction = _module
embedding_link_prediction_dw = _module
gnn_kg_link_prediction_dw = _module
gnn_link_prediction_dw = _module
triple_link_prediction_dw = _module
node_classification = _module
cluster_dw = _module
graphsage_dw = _module
m3s_dw = _module
network_embedding_dw = _module
node_classification_dw = _module
pprgo_dw = _module
sagn_dw = _module
unsup_graphsage_dw = _module
pretraining = _module
gcc_dw = _module
traffic_prediction = _module
stgat_dw = _module
stgcn_dw = _module
default_match = _module
model_wrapper = _module
base_model_wrapper = _module
clustering = _module
agc_mw = _module
daegc_mw = _module
gae_mw = _module
graph_classification_mw = _module
graph_embedding_mw = _module
infograph_mw = _module
heterogeneous_embedding_mw = _module
heterogeneous_gnn_mw = _module
multiplex_embedding_mw = _module
embedding_link_prediction_mw = _module
gnn_kg_link_prediction_mw = _module
gnn_link_prediction_mw = _module
triple_link_prediction_mw = _module
correct_smooth_mw = _module
dgi_mw = _module
gcnmix_mw = _module
grace_mw = _module
grand_mw = _module
graphsage_mw = _module
m3s_mw = _module
mvgrl_mw = _module
network_embedding_mw = _module
node_classification_mw = _module
pprgo_mw = _module
sagn_mw = _module
self_auxiliary_mw = _module
unsup_graphsage_mw = _module
gcc_mw = _module
stgat_mw = _module
stgcn_mw = _module
tools = _module
memory_moco = _module
wrapper_utils = _module
conf = _module
GRB = _module
attack = _module
base = _module
injection = _module
fgsm = _module
pgd = _module
rand = _module
speit = _module
tdgia = _module
modification = _module
dice = _module
fga = _module
flip = _module
nea = _module
pgd = _module
prbcd = _module
stack = _module
defense = _module
gcnsvd = _module
gnnguard = _module
robustgcn = _module
test_adv = _module
test_attack_defense = _module
test_defense = _module
test_injection = _module
test_modification = _module
VRGCN = _module
dataloder = _module
main = _module
data = _module
models = _module
train = _module
utils = _module
gnn = _module
logger = _module
dgi = _module
gat = _module
gcn = _module
gin = _module
grand = _module
graphsage = _module
mixhop = _module
mlp = _module
sgc = _module
sign = _module
dgraphfin = _module
evaluator = _module
utils = _module
run_gcc = _module
graphmae = _module
data_util = _module
evaluation = _module
edcoder = _module
gat = _module
gcn = _module
gin = _module
loss_func = _module
utils = _module
main_graph = _module
main_inductive = _module
main_transductive = _module
data_proc = _module
lc_sampler = _module
localclustering = _module
main_full_batch = _module
main_large = _module
edcoder = _module
finetune = _module
gat = _module
gcn = _module
loss_func = _module
utils = _module
custom_dataset = _module
custom_gcn = _module
custom_triple_dataset = _module
cv_search = _module
generate_emb = _module
pipeline = _module
quick_start = _module
recommendation = _module
calculate_paper_similarity = _module
generate_title = _module
oagbert = _module
oagbert_encode_paper = _module
oagbert_metainfo_zh = _module
oagbert_metainfo_zh_similarity = _module
gnn = _module
gnn = _module
chebnet = _module
dgcnn = _module
gat = _module
gcn = _module
unet = _module
conv = _module
run = _module
simple_trafficPre = _module
example = _module
display_data = _module
download = _module
setup = _module
test_customized_data = _module
test_data = _module
test_gcc_data = _module
test_geom_data = _module
test_kg_data = _module
test_matlab_data = _module
test_oagbert_data = _module
test_ogb = _module
test_planetoid = _module
test_rd2cd_data = _module
test_rec_data = _module
test_saint_data = _module
test_deepwalk = _module
test_contrastive_models = _module
test_generative_models = _module
test_attributed_graph_clustering = _module
test_encode_paper = _module
test_graph_classification = _module
test_heterogeneous_node_classification = _module
test_link_prediction = _module
test_multiplex_link_prediction = _module
test_node_classification = _module
test_triple_link_prediction = _module
test_unsupervised_graph_classification = _module
test_unsupervised_node_classification = _module
test_args = _module
test_experiments = _module
test_layers = _module
test_oag = _module
test_ops = _module
test_options = _module
test_pipelines = _module
test_utils = _module

from _paritybench_helpers import _mock_config, patch_functional
from unittest.mock import mock_open, MagicMock
from torch.autograd import Function
from torch.nn import Module
import abc, collections, copy, enum, functools, inspect, itertools, logging, math, matplotlib, numbers, numpy, pandas, queue, random, re, scipy, sklearn, string, tensorflow, time, torch, torchaudio, torchvision, types, typing, uuid, warnings
import operator as op
from dataclasses import dataclass
import numpy as np
from torch import Tensor
patch_functional()
open = mock_open()
yaml = logging = sys = argparse = MagicMock()
ArgumentParser = argparse.ArgumentParser
_global_config = args = argv = cfg = config = params = _mock_config()
argparse.ArgumentParser.return_value.parse_args.return_value = _global_config
yaml.load.return_value = _global_config
sys.argv = _global_config
__version__ = '1.0.0'
xrange = range
wraps = functools.wraps


import re


import torch


import copy


import scipy.sparse as sp


import numpy as np


from abc import ABCMeta


import torch.utils.data


from torch.utils.data.dataloader import default_collate


import collections


from itertools import repeat


from typing import List


import random


import inspect


from sklearn.preprocessing import StandardScaler


from collections import defaultdict


import scipy.io as sio


from itertools import product


import scipy.io


import time


from torch import Tensor


import pandas as pd


import warnings


import torch.nn.functional as F


import itertools


from collections import namedtuple


import torch.nn as nn


import torch.multiprocessing as mp


import math


from typing import Optional


from torch.utils.checkpoint import checkpoint


from torch.utils.checkpoint import get_device_states


from torch.utils.checkpoint import set_device_states


from torch import nn


from typing import Type


from typing import Any


from sklearn import preprocessing


from torch.nn.parameter import Parameter


from sklearn.cluster import SpectralClustering


from functools import partial


from scipy.linalg import block_diag


from torch.nn.modules.module import Module


from scipy.linalg import expm


from typing import Tuple


import functools


import logging


from torch.utils import checkpoint


from torch.nn import Module


import torch.nn.init as init


from torch.nn import CrossEntropyLoss


from torch.utils.cpp_extension import load


import matplotlib.cm as cm


import matplotlib.pyplot as plt


import torch.distributed as dist


from torch.nn.parallel import DistributedDataParallel


from torch.cuda.amp import GradScaler


from torch.cuda.amp import autocast


from typing import Dict


from typing import Union


from typing import Callable


from sklearn.metrics import f1_score


import scipy


from functools import reduce


from scipy.special import iv


from torch.utils.data import DataLoader


import scipy.sparse.linalg as slinalg


from scipy.sparse import linalg


from sklearn.model_selection import StratifiedKFold


import scipy.sparse as sparse


import sklearn.preprocessing as preprocessing


from torch.utils.data import Sampler


from torch.utils.data import BatchSampler


from torch.utils.data import TensorDataset


from abc import abstractmethod


from sklearn.cluster import KMeans


from sklearn.linear_model import LogisticRegression


from sklearn.metrics import auc


from sklearn.metrics import precision_recall_curve


from sklearn.metrics import roc_auc_score


from torch.nn import functional as F


from sklearn.multiclass import OneVsRestClassifier


from sklearn.metrics import accuracy_score


from sklearn.utils import shuffle as skshuffle


from sklearn.model_selection import GridSearchCV


from sklearn.model_selection import KFold


from sklearn.svm import SVC


from sklearn.metrics.cluster import normalized_mutual_info_score


from scipy.optimize import linear_sum_assignment


import types


from scipy.sparse import lil_matrix


from sklearn.metrics.pairwise import cosine_similarity


from sklearn.preprocessing import normalize


from itertools import chain


from torch import optim


from sklearn import metrics


from sklearn.model_selection import ShuffleSplit


from sklearn.model_selection import train_test_split


from sklearn.preprocessing import OneHotEncoder


from collections import Counter


from torch import optim as optim


from typing import Counter


from sklearn.utils.extmath import randomized_svd


import torch.multiprocessing


from scipy.sparse import csr_matrix


from sklearn.decomposition import PCA


from sklearn.manifold import TSNE


class linear(torch.autograd.Function):

    @staticmethod
    def forward(ctx, input, weight, bias=None, scheme=None, rp_ratio=2):
        if rp_ratio > 1:
            D = input.shape[1]
            rmat = (torch.bernoulli(torch.ones((D, D // rp_ratio)) * 0.5) * 2.0 - 1) * math.sqrt(1.0 / (D // rp_ratio))
            input_rp = torch.mm(input, rmat)
            quantized = quantize_activation(input_rp, scheme)
        else:
            quantized = quantize_activation(input, scheme)
        empty_cache(config.empty_cache_threshold)
        ctx.scheme = scheme
        if rp_ratio > 1:
            ctx.saved = quantized, weight, bias, rmat
            ctx.other_args = input_rp.shape
        else:
            ctx.saved = quantized, weight, bias
            ctx.other_args = input.shape
        return F.linear(input, weight, bias)

    @staticmethod
    def backward(ctx, grad_output):
        if ctx.scheme:
            ctx.scheme.set_scale(grad_output)
        q_input_shape = ctx.other_args
        if len(ctx.saved) == 4:
            quantized, weight, bias, rmat = ctx.saved
            input_rp = dequantize_activation(quantized, q_input_shape)
            input = torch.mm(input_rp, rmat.t())
            del quantized, ctx.saved, input_rp
        else:
            quantized, weight, bias = ctx.saved
            input = dequantize_activation(quantized, q_input_shape)
            del quantized, ctx.saved
        empty_cache(config.empty_cache_threshold)
        C_in = input.shape[-1]
        C_out = grad_output.shape[-1]
        grad_output_flatten = grad_output.view(-1, C_out)
        input_flatten = input.view(-1, C_in)
        grad_input = grad_output_flatten.mm(weight)
        grad_weight = grad_output_flatten.t().mm(input_flatten)
        if bias is not None:
            grad_bias = grad_output_flatten.sum(0)
        else:
            grad_bias = None
        if ctx.scheme:
            ctx.scheme.if_allocate_perlayer()
        return grad_input, grad_weight, grad_bias, None, None


class QLinear(nn.Linear):
    num_layers = 0

    def __init__(self, input_features, output_features, bias=True, group=0, rp_ratio=2):
        super(QLinear, self).__init__(input_features, output_features, bias)
        if config.adaptive_conv_scheme:
            self.scheme = QScheme(self, group=group)
        else:
            self.scheme = None
        self.rp_ratio = rp_ratio

    def forward(self, input):
        if config.training:
            return linear.apply(input, self.weight, self.bias, self.scheme, self.rp_ratio)
        else:
            return super(QLinear, self).forward(input)


CONFIGS = {'fast_spmm': None, 'csrmhspmm': None, 'csr_edge_softmax': None, 'fused_gat_func': None, 'fast_spmm_cpu': None, 'spmm_flag': False, 'mh_spmm_flag': False, 'fused_gat_flag': False, 'spmm_cpu_flag': False}


def initialize_spmm():
    if CONFIGS['spmm_flag']:
        return
    CONFIGS['spmm_flag'] = True
    if torch.cuda.is_available():
        CONFIGS['fast_spmm'] = csrspmm

